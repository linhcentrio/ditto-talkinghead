#!/usr/bin/env python3
"""Streamlit UI t·ªëi ∆∞u cho Google Colab - AI Video Creator
Bao g·ªìm t·∫•t c·∫£ t√≠nh nƒÉng n√¢ng cao tr·ª´ subtitle
"""

import streamlit as st
import subprocess
import tempfile
import shutil
import os
import queue
import threading
import time
import re
import traceback
import asyncio
import sys
import numpy as np
import pickle
import json
import librosa
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Union, Any
import cv2
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
from contextlib import contextmanager

# T·∫Øt ch·∫ø ƒë·ªô theo d√µi file c·ªßa Streamlit
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'

# Import modules
try:
    from video_editor import VideoEditor
except ImportError:
    st.error("‚ùå Kh√¥ng th·ªÉ import VideoEditor. Vui l√≤ng ki·ªÉm tra file video_editor.py")
    st.stop()

# OpenAI client - s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o sau khi c√≥ API key
try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    st.warning("‚ö†Ô∏è OpenAI client kh√¥ng kh·∫£ d·ª•ng")
    OPENAI_AVAILABLE = False
    
openai_client = None

# === Kh·ªüi t·∫°o OpenAI Client ===
def initialize_openai_client():
    """Kh·ªüi t·∫°o OpenAI client v·ªõi API key t·ª´ session state"""
    global openai_client
    if OPENAI_AVAILABLE and st.session_state.openai_api_key.strip():
        try:
            openai_client = AsyncOpenAI(api_key=st.session_state.openai_api_key.strip())
            return True
        except Exception as e:
            st.error(f"L·ªói kh·ªüi t·∫°o OpenAI client: {str(e)}")
            openai_client = None
            return False
    else:
        openai_client = None
        return False

# === Ki·ªÉm tra API Key ===
async def test_openai_api_key(api_key: str) -> Tuple[bool, str]:
    """Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa OpenAI API key"""
    try:
        test_client = AsyncOpenAI(api_key=api_key.strip())
        # Test v·ªõi m·ªôt request ƒë∆°n gi·∫£n
        response = await test_client.models.list()
        return True, "API key h·ª£p l·ªá"
    except Exception as e:
        error_msg = str(e)
        if "invalid_api_key" in error_msg:
            return False, "API key kh√¥ng h·ª£p l·ªá"
        elif "rate_limit" in error_msg:
            return False, "ƒê√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n rate limit"
        elif "insufficient_quota" in error_msg:
            return False, "T√†i kho·∫£n kh√¥ng ƒë·ªß quota"
        else:
            return False, f"L·ªói k·∫øt n·ªëi: {error_msg}"

# === C·∫•u h√¨nh Google Colab ===
def get_colab_config():
    """L·∫•y c·∫•u h√¨nh t·ª´ environment variables cho Google Colab"""
    config = {
        'data_root': os.environ.get('DITTO_DATA_ROOT', './checkpoints/ditto_trt'),
        'gpu_arch': os.environ.get('DITTO_GPU_ARCH', 'pre_ampere'),
        'cfg_pkl': './checkpoints/ditto_cfg/v0.4_hubert_cfg_trt.pkl'
    }
    
    # Ki·ªÉm tra files t·ªìn t·∫°i
    if not os.path.exists(config['data_root']):
        st.error(f"‚ùå Th∆∞ m·ª•c model kh√¥ng t√¨m th·∫•y: {config['data_root']}")
        st.info("üí° Vui l√≤ng ch·∫°y l·∫°i cell setup models")
        st.stop()
    
    if not os.path.exists(config['cfg_pkl']):
        st.error(f"‚ùå File config kh√¥ng t√¨m th·∫•y: {config['cfg_pkl']}")
        st.info("üí° Vui l√≤ng ch·∫°y l·∫°i cell t·∫£i config")
        st.stop()
    
    return config

# === ƒê·ªãnh nghƒ©a c√°c b∆∞·ªõc quy tr√¨nh ===
WORKFLOW_STEPS = {
    "prepare_files": "Chu·∫©n b·ªã files",
    "tts_generation": "T·∫°o √¢m thanh t·ª´ vƒÉn b·∫£n",
    "talking_head_generation": "T·∫°o video khu√¥n m·∫∑t n√≥i",
    "video_overlay": "Gh√©p video MC v√† n·ªÅn",
}

# === Th√¥ng tin m√¥ t·∫£ gi·ªçng n√≥i ===
VOICE_DESCRIPTIONS = {
    "Ash": "Gi·ªçng nam tr∆∞·ªüng th√†nh, h∆°i tr·∫ßm, ph√π h·ª£p cho phim t√†i li·ªáu",
    "Ballad": "Gi·ªçng n·ªØ m·ªÅm m·∫°i, ·∫•m √°p, ph√π h·ª£p cho n·ªôi dung t∆∞ v·∫•n",
    "Coral": "Gi·ªçng n·ªØ tr·∫ª, r√µ r√†ng, t·ª± tin, ph√π h·ª£p cho n·ªôi dung gi√°o d·ª•c",
    "Echo": "Gi·ªçng nam tr·∫ª, nƒÉng ƒë·ªông, ph√π h·ª£p cho qu·∫£ng c√°o",
    "Fable": "Gi·ªçng nam uy t√≠n, ph√π h·ª£p cho th√¥ng b√°o ch√≠nh th·ª©c",
    "Onyx": "Gi·ªçng nam tr·∫ßm, sang tr·ªçng, ph√π h·ª£p cho thuy·∫øt tr√¨nh",
    "Nova": "Gi·ªçng n·ªØ chuy√™n nghi·ªáp, ph√π h·ª£p cho tin t·ª©c",
    "Sage": "Gi·ªçng n·ªØ t·ª´ng tr·∫£i, ·∫•m √°p, ph√π h·ª£p cho podcast",
    "Shimmer": "Gi·ªçng n·ªØ t∆∞∆°i s√°ng, nƒÉng ƒë·ªông, ph√π h·ª£p cho gi·∫£i tr√≠",
    "Verse": "Gi·ªçng nam t·ª± nhi√™n, c√¢n b·∫±ng, ph√π h·ª£p cho ƒëa d·∫°ng n·ªôi dung"
}

# === Kh·ªüi t·∫°o session state ===
def init_session_state():
    """Kh·ªüi t·∫°o to√†n b·ªô session state c·∫ßn thi·∫øt"""
    # L·∫•y API key t·ª´ environment variable tr∆∞·ªõc (t·ª´ notebook setup)
    env_openai_key = os.environ.get('OPENAI_API_KEY', '').strip()
    
    defaults = {
        'processing': False,
        'complete': False,
        'output_file': None,
        'history': [],
        'process_start_time': None,
        'auto_scale': True,
        'logs': [],
        'workflow_steps': {k: True for k in WORKFLOW_STEPS},
        'cancel_event': None,
        'msg_queue': None,
        'tts_instructions_preset': "Tone: T·ª± nhi√™n, tr√¥i ch·∫£y, chuy√™n nghi·ªáp\nEmotion: Nhi·ªát t√¨nh, t·ª± tin\nDelivery: R√µ r√†ng, nh·ªãp ƒë·ªô v·ª´a ph·∫£i, nh·∫•n m·∫°nh t·ª´ kh√≥a quan tr·ªçng",
        'openai_api_key': env_openai_key,  # ∆Øu ti√™n environment variable
        'openai_api_status': 'valid' if env_openai_key else 'not_tested',  # Assume valid if from env
        'api_key_source': 'environment' if env_openai_key else 'manual',  # Track source
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value
    
    # N·∫øu c√≥ API key t·ª´ environment v√† ch∆∞a kh·ªüi t·∫°o client
    if env_openai_key and not st.session_state.openai_api_key:
        st.session_state.openai_api_key = env_openai_key
        st.session_state.openai_api_status = 'valid'
        st.session_state.api_key_source = 'environment'

# === H√†m x√°c th·ª±c tham s·ªë kh·∫©u h√¨nh ===
def validate_mouth_params(vad_alpha=1.0, exp_components=None, exp_scale=1.0, pose_scale=1.0, delta_exp_enabled=False, delta_exp_value=0.0):
    """X√°c th·ª±c c√°c tham s·ªë kh·∫©u h√¨nh ƒë·ªÉ ƒë·∫£m b·∫£o ch√∫ng trong ph·∫°m vi an to√†n"""
    validated = {}
    
    # X√°c th·ª±c vad_alpha (gi·ªØ trong kho·∫£ng 0.0-1.0)
    validated['vad_alpha'] = max(0.0, min(1.0, float(vad_alpha)))
    
    # X√°c th·ª±c exp_components (ƒë·∫£m b·∫£o l√† list h·ª£p l·ªá)
    if exp_components and isinstance(exp_components, list):
        validated['exp_components'] = [str(comp) for comp in exp_components if comp in ["exp", "pitch", "yaw", "roll", "t"]]
    else:
        validated['exp_components'] = None
    
    # X√°c th·ª±c exp_scale v√† pose_scale (gi·ªØ trong kho·∫£ng 0.5-1.5)
    validated['exp_scale'] = max(0.5, min(1.5, float(exp_scale)))
    validated['pose_scale'] = max(0.5, min(1.5, float(pose_scale)))
    
    # X√°c th·ª±c delta_exp_enabled v√† delta_exp_value
    validated['delta_exp_enabled'] = bool(delta_exp_enabled)
    validated['delta_exp_value'] = max(-0.2, min(0.2, float(delta_exp_value)))
    
    return validated

# === H√†m ti·ªán √≠ch ===
@lru_cache(maxsize=32)
def get_video_resolution(video_path: str) -> Tuple[int, int]:
    """L·∫•y ƒë·ªô ph√¢n gi·∫£i c·ªßa video v·ªõi cache"""
    try:
        cap = cv2.VideoCapture(str(video_path))
        if cap.isOpened():
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 1920
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 1080
            cap.release()
            return width, height
        return 1920, 1080
    except Exception:
        return 1920, 1080

def calculate_auto_scale(mc_path: Union[str, Any], bg_width: int, bg_height: int) -> float:
    """T√≠nh to√°n t·ªâ l·ªá scale ph√π h·ª£p cho MC"""
    try:
        mc_width, mc_height = 0, 0
        
        # X·ª≠ l√Ω c√°c lo·∫°i ƒë·∫ßu v√†o kh√°c nhau
        if hasattr(mc_path, 'getbuffer'):  # UploadedFile
            suffix = Path(mc_path.name).suffix.lower()
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp:
                temp.write(mc_path.getbuffer())
                temp_path = temp.name
            
            try:
                if suffix in ['.jpg', '.jpeg', '.png']:
                    img = cv2.imread(temp_path)
                    if img is not None:
                        mc_width, mc_height = img.shape[1], img.shape[0]
                else:  # Video
                    cap = cv2.VideoCapture(temp_path)
                    if cap.isOpened():
                        mc_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                        mc_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        cap.release()
            finally:
                os.unlink(temp_path)
        else:  # Path string
            mc_path_str = str(mc_path)
            suffix = Path(mc_path_str).suffix.lower()
            
            if suffix in ['.jpg', '.jpeg', '.png']:
                img = cv2.imread(mc_path_str)
                if img is not None:
                    mc_width, mc_height = img.shape[1], img.shape[0]
            else:  # Video
                cap = cv2.VideoCapture(mc_path_str)
                if cap.isOpened():
                    mc_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                    mc_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                    cap.release()
        
        if not mc_width or not mc_height:
            return 0.25
        
        # T√≠nh to√°n t·ªâ l·ªá t·ªëi ∆∞u
        width_scale = bg_width / mc_width / 3
        height_scale = bg_height / mc_height / 1.5
        
        return min(round(min(width_scale, height_scale), 2), 0.5)
    except Exception:
        return 0.25

def update_history_from_folder():
    """C·∫≠p nh·∫≠t l·ªãch s·ª≠ t·ª´ th∆∞ m·ª•c output"""
    if not (output_folder := Path("./output")).exists():
        return
    
    # L·∫•y danh s√°ch ƒë∆∞·ªùng d·∫´n hi·ªán c√≥
    existing_paths = {item.get('path', '') for item in st.session_state.history}
    
    # T√¨m c√°c file m·ªõi
    new_files = [
        {
            'path': str(file),
            'created': datetime.fromtimestamp(file.stat().st_mtime),
            'size': file.stat().st_size / (1024*1024)
        }
        for file in output_folder.glob("final_mc_*.mp4")
        if file.exists() and str(file) not in existing_paths
    ]
    
    # Th√™m v√†o l·ªãch s·ª≠ n·∫øu c√≥ file m·ªõi
    if new_files:
        st.session_state.history.extend(new_files)

# === H√†m t·∫°o audio b·∫±ng GPT-4o-mini-TTS ===
async def generate_gpt4o_tts(text: str, output_path: str, instructions: str, voice: str = "shimmer") -> bool:
    """T·∫°o audio t·ª´ vƒÉn b·∫£n b·∫±ng GPT-4o-mini-TTS v·ªõi h∆∞·ªõng d·∫´n v·ªÅ gi·ªçng ƒëi·ªáu"""
    try:
        # Ki·ªÉm tra openai_client c√≥ s·∫µn
        if not openai_client:
            raise Exception("OpenAI client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Vui l√≤ng ki·ªÉm tra API key trong tab C√†i ƒë·∫∑t.")
        
        # T·∫°o file PCM t·∫°m
        temp_pcm = output_path + ".pcm"
        
        # T·∫°o audio v·ªõi streaming
        async with openai_client.audio.speech.with_streaming_response.create(
            model="gpt-4o-mini-tts",
            voice=voice.lower(),
            input=text,
            response_format="pcm",
        ) as response:
            # L∆∞u n·ªôi dung PCM v√†o file
            with open(temp_pcm, 'wb') as f:
                async for chunk in response.iter_bytes():
                    f.write(chunk)
        
        # Chuy·ªÉn ƒë·ªïi PCM sang MP3 b·∫±ng ffmpeg
        result = subprocess.run([
            "ffmpeg", "-y", "-f", "s16le", "-ar", "24000", "-ac", "1", "-i", temp_pcm,
            "-acodec", "libmp3lame", "-b:a", "192k", output_path
        ], capture_output=True)
        
        # X√≥a file t·∫°m
        if os.path.exists(temp_pcm):
            os.remove(temp_pcm)
        
        return result.returncode == 0
        
    except Exception as e:
        print(f"L·ªói t·∫°o GPT-4o TTS: {str(e)}")
        return False

# === H√†m nghe th·ª≠ gi·ªçng n√≥i ===
async def preview_audio_tts(text, instructions, voice, message_placeholder=None):
    """T·∫°o v√† ph√°t m·∫´u gi·ªçng n√≥i t·ª´ GPT-4o-mini-TTS"""
    try:
        # Ki·ªÉm tra openai_client c√≥ s·∫µn
        if not openai_client:
            raise Exception("OpenAI client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Vui l√≤ng ki·ªÉm tra API key trong tab C√†i ƒë·∫∑t.")
        
        if message_placeholder:
            message_placeholder.write("‚è≥ ƒêang t·∫°o m·∫´u gi·ªçng n√≥i...")
        
        # T·∫°o t·ªáp t·∫°m th·ªùi
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp:
            temp_path = temp.name
        
        # T·∫°o audio v·ªõi streaming
        async with openai_client.audio.speech.with_streaming_response.create(
            model="gpt-4o-mini-tts",
            voice=voice.lower(),
            input=text,
            response_format="pcm",
        ) as response:
            # L∆∞u n·ªôi dung PCM v√†o file
            with open(temp_path + ".pcm", 'wb') as f:
                async for chunk in response.iter_bytes():
                    f.write(chunk)
        
        # Chuy·ªÉn ƒë·ªïi PCM sang MP3 b·∫±ng ffmpeg
        result = subprocess.run([
            "ffmpeg", "-y", "-f", "s16le", "-ar", "24000", "-ac", "1", "-i", temp_path + ".pcm",
            "-acodec", "libmp3lame", "-b:a", "192k", temp_path
        ], capture_output=True)
        
        if result.returncode != 0:
            if message_placeholder:
                message_placeholder.error("Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi √¢m thanh. Vui l√≤ng th·ª≠ l·∫°i.")
            return None
        
        # ƒê·ªçc file MP3 ƒë·ªÉ hi·ªÉn th·ªã
        with open(temp_path, "rb") as f:
            audio_bytes = f.read()
        
        # X√≥a files t·∫°m
        try:
            os.unlink(temp_path)
            os.unlink(temp_path + ".pcm")
        except:
            pass
        
        return audio_bytes
        
    except Exception as e:
        if message_placeholder:
            message_placeholder.error(f"L·ªói: {str(e)}")
        return None

# === Handler cho c√°c messages t·ª´ processing thread ===
def handle_message(msg_type: str, content: Any, containers: Dict[str, Any], show_logs: bool = True):
    """X·ª≠ l√Ω messages t·ª´ queue d·ª±a tr√™n lo·∫°i"""
    if msg_type == 'status':
        containers['status'].write(content)
    elif msg_type == 'progress':
        containers['progress'].progress(content)
    elif msg_type == 'log':
        st.session_state.logs.append(content)
        if show_logs and 'log_content' in containers and containers['log_content']:
            containers['log_content'].code("\n".join(st.session_state.logs[-20:]))
    elif msg_type == 'metrics':
        with containers['metrics']:
            cols = st.columns(len(content))
            for i, (key, value) in enumerate(content.items()):
                cols[i].metric(key, value)
    elif msg_type == 'error':
        st.error(content)
        st.session_state.processing = False
    elif msg_type == 'complete':
        st.session_state.processing = False
        st.session_state.complete = True
        st.session_state.output_file = content['output_file']
        
        # Th√™m v√†o l·ªãch s·ª≠
        st.session_state.history.append({
            'path': content['output_file'],
            'created': datetime.now(),
            'size': content.get('file_size', 0)
        })

# === H√†m x·ª≠ l√Ω video ch√≠nh ===
def process_video(workflow_dict, mc_path_final, bg_path_final, audio_path_final, text_prompt, temp_dir, msg_queue, cancel_event, editor, timestamp, tts_service_val, tts_voice_val, tts_speed_val, tts_instructions_val="", position_val="G√≥c d∆∞·ªõi ph·∫£i", scale_val=0.25, quality_val="medium", ai_model_val="M√¥ h√¨nh m·∫∑c ƒë·ªãnh", vad_alpha=1.0, exp_components=None, exp_scale=1.0, pose_scale=1.0, delta_exp_enabled=False, delta_exp_value=0.0):
    """X·ª≠ l√Ω video trong thread ri√™ng bi·ªát v·ªõi ƒë·∫ßy ƒë·ªß t√≠nh nƒÉng"""
    try:
        # X√°c th·ª±c tham s·ªë kh·∫©u h√¨nh
        mouth_params = validate_mouth_params(
            vad_alpha, exp_components, exp_scale, pose_scale, delta_exp_enabled, delta_exp_value
        )
        
        # Chu·∫©n b·ªã files
        if workflow_dict.get("prepare_files", True):
            msg_queue.put(('status', "‚è≥ ƒêang chu·∫©n b·ªã files..."))
            msg_queue.put(('progress', 5))
            msg_queue.put(('log', "B·∫Øt ƒë·∫ßu chu·∫©n b·ªã files..."))
            
            # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n files t·∫°m v√† ƒë·∫ßu ra
            mc_suffix = Path(mc_path_final.name).suffix if hasattr(mc_path_final, 'name') else Path(str(mc_path_final)).suffix
            bg_suffix = Path(bg_path_final.name).suffix if hasattr(bg_path_final, 'name') and bg_path_final else ".mp4"
            
            mc_temp_path = temp_dir / f"mc{mc_suffix}"
            bg_temp_path = temp_dir / f"bg{bg_suffix}" if bg_path_final else None
            audio_temp_path = temp_dir / "audio.mp3"
            talking_path = temp_dir / "talking.mp4"
            output_file = editor.output_dir / f"video_mc_{timestamp}.mp4"
            final_output = editor.output_dir / f"final_mc_{timestamp}.mp4"
            
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
            os.makedirs(editor.output_dir, exist_ok=True)
            
            # L∆∞u files t·∫°m
            if hasattr(mc_path_final, 'getbuffer'):  # UploadedFile
                with open(mc_temp_path, "wb") as f:
                    f.write(mc_path_final.getbuffer())
                actual_mc_path = mc_temp_path
            else:
                actual_mc_path = mc_path_final
            
            if bg_path_final:
                if hasattr(bg_path_final, 'getbuffer'):  # UploadedFile
                    with open(bg_temp_path, "wb") as f:
                        f.write(bg_path_final.getbuffer())
                    actual_bg_path = bg_temp_path
                else:
                    actual_bg_path = bg_path_final
            else:
                actual_bg_path = None
            
            msg_queue.put(('progress', 10))
        else:
            msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc chu·∫©n b·ªã files"))
            # Thi·∫øt l·∫≠p c√°c bi·∫øn c·∫ßn thi·∫øt
            mc_temp_path = temp_dir / f"mc{Path(mc_path_final.name).suffix if hasattr(mc_path_final, 'name') else Path(str(mc_path_final)).suffix}"
            bg_temp_path = temp_dir / f"bg{Path(bg_path_final.name).suffix if hasattr(bg_path_final, 'name') else Path(str(bg_path_final)).suffix}" if bg_path_final else None
            audio_temp_path = temp_dir / "audio.mp3"
            talking_path = temp_dir / "talking.mp4"
            output_file = editor.output_dir / f"video_mc_{timestamp}.mp4"
            final_output = editor.output_dir / f"final_mc_{timestamp}.mp4"
            
            os.makedirs(editor.output_dir, exist_ok=True)
            
            # Copy files
            if hasattr(mc_path_final, 'getbuffer'):
                with open(mc_temp_path, "wb") as f:
                    f.write(mc_path_final.getbuffer())
                actual_mc_path = mc_temp_path
            else:
                actual_mc_path = mc_path_final
            
            if bg_path_final:
                if hasattr(bg_path_final, 'getbuffer'):
                    with open(bg_temp_path, "wb") as f:
                        f.write(bg_path_final.getbuffer())
                    actual_bg_path = bg_temp_path
                else:
                    actual_bg_path = bg_path_final
            else:
                actual_bg_path = None
        
        # X·ª≠ l√Ω audio
        if audio_path_final:  # Upload file
            if hasattr(audio_path_final, 'getbuffer'):
                with open(audio_temp_path, "wb") as f:
                    f.write(audio_path_final.getbuffer())
                actual_audio_path = audio_temp_path
            else:
                actual_audio_path = audio_path_final
        else:  # T·∫°o t·ª´ vƒÉn b·∫£n
            if workflow_dict.get("tts_generation", True):
                msg_queue.put(('status', "üéôÔ∏è ƒêang t·∫°o audio t·ª´ vƒÉn b·∫£n..."))
                msg_queue.put(('log', "B·∫Øt ƒë·∫ßu t·∫°o audio t·ª´ vƒÉn b·∫£n..."))
                
                # X·ª≠ l√Ω TTS d·ª±a tr√™n service
                if tts_service_val == "GPT-4o-mini-TTS":
                    msg_queue.put(('log', f"S·ª≠ d·ª•ng GPT-4o-mini-TTS v·ªõi gi·ªçng {tts_voice_val}"))
                    
                    # S·ª≠ d·ª•ng asyncio ƒë·ªÉ ch·∫°y function async
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    success = loop.run_until_complete(
                        generate_gpt4o_tts(
                            text_prompt,
                            str(audio_temp_path),
                            tts_instructions_val,
                            tts_voice_val
                        )
                    )
                    loop.close()
                    
                    if not success:
                        msg_queue.put(('error', f"L·ªói t·∫°o audio v·ªõi GPT-4o-mini-TTS"))
                        return
                else:
                    # S·ª≠ d·ª•ng c√°c service kh√°c
                    tts_service = "edge" if tts_service_val == "Edge TTS" else "openai"
                    success, error = editor.generate_audio_from_text(
                        text_prompt,
                        audio_temp_path,
                        service=tts_service,
                        voice=tts_voice_val,
                        speed=tts_speed_val
                    )
                    
                    if not success:
                        msg_queue.put(('error', f"L·ªói t·∫°o audio: {error}"))
                        return
                
                actual_audio_path = audio_temp_path
            else:
                msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc t·∫°o audio"))
                # T·∫°o audio m·∫´u
                actual_audio_path = str(audio_temp_path)
                subprocess.run([
                    "ffmpeg", "-y", "-f", "lavfi", "-i", "anullsrc=r=44100:cl=mono",
                    "-t", "5", "-q:a", "0", "-map", "0", str(audio_temp_path)
                ], capture_output=True)
        
        msg_queue.put(('progress', 30))
        
        # Ki·ªÉm tra n·∫øu ƒë√£ h·ªßy qu√° tr√¨nh
        if cancel_event.is_set():
            msg_queue.put(('error', "Qu√° tr√¨nh ƒë√£ b·ªã h·ªßy b·ªüi ng∆∞·ªùi d√πng"))
            return
        
        # T·∫°o video khu√¥n m·∫∑t n√≥i
        if workflow_dict.get("talking_head_generation", True):
            msg_queue.put(('status', "üé≠ ƒêang t·∫°o video khu√¥n m·∫∑t n√≥i..."))
            msg_queue.put(('log', "B·∫Øt ƒë·∫ßu t·∫°o video khu√¥n m·∫∑t..."))
            
            # L·∫•y c·∫•u h√¨nh
            config = get_colab_config()
            
            # Ch·ªçn model path d·ª±a tr√™n l·ª±a ch·ªçn
            if ai_model_val == "M√¥ h√¨nh t·ªëi ∆∞u h√≥a":
                model_path = config['data_root'].replace('ditto_trt', 'ditto_trt_custom')
                msg_queue.put(('log', "S·ª≠ d·ª•ng m√¥ h√¨nh t·ªëi ∆∞u h√≥a"))
            else:
                model_path = config['data_root']
                msg_queue.put(('log', "S·ª≠ d·ª•ng m√¥ h√¨nh m·∫∑c ƒë·ªãnh"))
            
            # Chu·∫©n b·ªã tham s·ªë kh·∫©u h√¨nh
            # 1. T·∫°o use_d_keys v·ªõi t·ª∑ l·ªá ph√π h·ª£p
            use_d_keys_dict = {}
            if mouth_params['exp_components']:
                if "exp" in mouth_params['exp_components']:
                    use_d_keys_dict["exp"] = mouth_params['exp_scale']
                for k in ["pitch", "yaw", "roll"]:
                    if k in mouth_params['exp_components']:
                        use_d_keys_dict[k] = mouth_params['pose_scale']
                if "t" in mouth_params['exp_components']:
                    use_d_keys_dict["t"] = 1.0
            
            # 2. T·∫°o ctrl_info
            ctrl_info = {}
            if mouth_params['vad_alpha'] < 1.0:
                msg_queue.put(('log', f"√Åp d·ª•ng m·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i: {mouth_params['vad_alpha']}"))
                for i in range(10000):
                    ctrl_info[i] = {"vad_alpha": mouth_params['vad_alpha']}
            
            # 3. Th√™m delta_exp n·∫øu ƒë∆∞·ª£c k√≠ch ho·∫°t
            if mouth_params['delta_exp_enabled'] and mouth_params['delta_exp_value'] != 0.0:
                msg_queue.put(('log', f"√Åp d·ª•ng offset bi·ªÉu c·∫£m m√¥i: {mouth_params['delta_exp_value']}"))
                for i in range(10000):
                    if i in ctrl_info:
                        ctrl_info[i]["delta_exp"] = mouth_params['delta_exp_value']
                    else:
                        ctrl_info[i] = {"delta_exp": mouth_params['delta_exp_value']}
            
            # 4. T·∫°o more_kwargs
            more_kwargs = {
                "setup_kwargs": {},
                "run_kwargs": {
                    "fade_in": 1,
                    "fade_out": 1,
                }
            }
            
            # Th√™m use_d_keys n·∫øu c√≥
            if use_d_keys_dict:
                msg_queue.put(('log', f"√Åp d·ª•ng th√†nh ph·∫ßn bi·ªÉu c·∫£m t√πy ch·ªânh: {use_d_keys_dict}"))
                more_kwargs["setup_kwargs"]["use_d_keys"] = use_d_keys_dict
            
            # Th√™m ctrl_info n·∫øu c√≥
            if ctrl_info:
                more_kwargs["run_kwargs"]["ctrl_info"] = ctrl_info
            
            # L∆∞u more_kwargs v√†o file pickle
            more_kwargs_path = temp_dir / "more_kwargs.pkl"
            with open(more_kwargs_path, 'wb') as f:
                pickle.dump(more_kwargs, f)
            
            # ∆Ø·ªõc t√≠nh s·ªë frame
            audio, sr = librosa.core.load(str(actual_audio_path), sr=16000)
            num_frames = int(len(audio) / 16000 * 25)
            msg_queue.put(('log', f"∆Ø·ªõc t√≠nh video s·∫Ω c√≥ kho·∫£ng {num_frames} frames"))
            
            # S·ª≠ d·ª•ng subprocess ƒë·ªÉ g·ªçi inference.py
            cmd = [
                "python", "inference.py",
                "--data_root", model_path,
                "--cfg_pkl", config['cfg_pkl'],
                "--audio_path", str(actual_audio_path),
                "--source_path", str(actual_mc_path),
                "--output_path", str(talking_path),
                "--more_kwargs", str(more_kwargs_path)
            ]
            
            msg_queue.put(('log', f"Ch·∫°y l·ªánh: {' '.join(cmd)}"))
            
            # Kh·ªüi ch·∫°y ti·∫øn tr√¨nh inference v·ªõi theo d√µi output
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )
            
            # X·ª≠ l√Ω output
            frame_count, fps = 0, 0
            
            # Thi·∫øt l·∫≠p timeout
            start_time = time.time()
            max_wait_time = 3600  # 1 gi·ªù
            
            while process.poll() is None:
                # Ki·ªÉm tra timeout v√† h·ªßy
                if time.time() - start_time > max_wait_time or cancel_event.is_set():
                    process.terminate()
                    msg_queue.put(('error', "Qu√° th·ªùi gian x·ª≠ l√Ω" if time.time() - start_time > max_wait_time else "Qu√° tr√¨nh ƒë√£ b·ªã h·ªßy"))
                    return
                
                # ƒê·ªçc m·ªôt d√≤ng t·ª´ stdout
                if line := process.stdout.readline():
                    # L·ªçc ANSI escape sequences
                    clean = re.sub(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])', '', line.strip())
                    if not clean or "aligned" in clean:
                        continue
                    
                    # X·ª≠ l√Ω th√¥ng tin audio processing
                    if "dit:" in clean and (m := re.search(r'dit: (\d+)it.*?(\d+\.\d+)?it/s', clean)):
                        step, speed = int(m.group(1)), float(m.group(2) or 0)
                        progress_value = min(30 + step/10*10, 40)
                        msg_queue.put(('progress', int(progress_value)))
                        msg_queue.put(('status', f"ƒêang x·ª≠ l√Ω √¢m thanh ‚Üí chuy·ªÉn ƒë·ªông ({step}/10)"))
                        msg_queue.put(('metrics', {'Ti·∫øn ƒë·ªô √¢m thanh': f"{step*10}%", 'T·ªëc ƒë·ªô': f"{speed:.1f}it/s"}))
                        if step % 2 == 0:
                            msg_queue.put(('log', f"‚û°Ô∏è Audio processing: {step}/10 ({speed:.1f}it/s)"))
                    
                    # X·ª≠ l√Ω th√¥ng tin frame video
                    elif "writer:" in clean and (m := re.search(r'writer: (\d+)it.*?(\d+\.\d+)?it/s', clean)):
                        frame, speed = int(m.group(1)), float(m.group(2) or 0)
                        frame_count, fps = frame, speed
                        progress_value = min(40 + frame/400*20, 60)
                        msg_queue.put(('progress', int(progress_value)))
                        msg_queue.put(('status', f"ƒêang t·∫°o video (frame {frame})"))
                        msg_queue.put(('metrics', {'Frames': frame, 'FPS': f"{speed:.1f}"}))
                        if frame % 50 == 0 or frame <= 5:
                            msg_queue.put(('log', f"üé¨ Video: frame {frame} ({speed:.1f} fps)"))
                else:
                    time.sleep(0.1)
            
            # ƒê·ªçc stderr output ƒë·ªÉ ghi log
            stderr_output = process.stderr.read()
            
            # Ki·ªÉm tra file ƒë·∫ßu ra
            if process.returncode != 0 or not os.path.exists(talking_path):
                # Fallback: T·∫°o video tr·ª±c ti·∫øp b·∫±ng ffmpeg
                msg_queue.put(('log', f"L·ªói khi t·∫°o video khu√¥n m·∫∑t n√≥i: {stderr_output}"))
                msg_queue.put(('log', "D√πng ffmpeg tr·ª±c ti·∫øp ƒë·ªÉ t·∫°o video khu√¥n m·∫∑t n√≥i"))
                
                fallback_cmd = [
                    "ffmpeg", "-y",
                    "-loop", "1" if Path(str(actual_mc_path)).suffix.lower() in ['.jpg', '.jpeg', '.png'] else "-i",
                    str(actual_mc_path),
                    "-i", str(actual_audio_path),
                    "-c:v", "libx264",
                    "-c:a", "aac",
                    "-shortest", str(talking_path)
                ]
                # L·ªçc b·ªè c√°c tham s·ªë None
                fallback_cmd = [cmd for cmd in fallback_cmd if cmd is not None]
                
                result = subprocess.run(fallback_cmd, capture_output=True, text=True)
                
                if result.returncode != 0 or not os.path.exists(talking_path):
                    msg_queue.put(('error', f"L·ªói khi t·∫°o video v·ªõi ph∆∞∆°ng √°n d·ª± ph√≤ng: {result.stderr}"))
                    return
                
                msg_queue.put(('log', "ƒê√£ t·∫°o video b·∫±ng ph∆∞∆°ng √°n d·ª± ph√≤ng"))
            
            msg_queue.put(('log', f"‚úÖ ƒê√£ t·∫°o th√†nh c√¥ng video khu√¥n m·∫∑t n√≥i: {talking_path}"))
            msg_queue.put(('progress', 60))
        else:
            msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc t·∫°o video khu√¥n m·∫∑t n√≥i"))
            # N·∫øu b·ªè qua, s·ª≠ d·ª•ng MC g·ªëc l√†m video khu√¥n m·∫∑t
            if Path(str(actual_mc_path)).suffix.lower() in ['.mp4']:
                shutil.copy(str(actual_mc_path), str(talking_path))
            else:
                # N·∫øu l√† ·∫£nh, t·∫°o video tƒ©nh t·ª´ ·∫£nh
                ffmpeg_cmd = [
                    "ffmpeg", "-y", "-loop", "1", "-i", str(actual_mc_path),
                    "-i", str(actual_audio_path), "-c:v", "libx264", "-tune", "stillimage",
                    "-c:a", "aac", "-shortest", str(talking_path)
                ]
                subprocess.run(ffmpeg_cmd, capture_output=True)
            
            msg_queue.put(('progress', 60))
        
        # N·∫øu kh√¥ng c√≥ background, s·ª≠ d·ª•ng talking head l√†m k·∫øt qu·∫£ cu·ªëi
        if actual_bg_path is None or not workflow_dict.get("video_overlay", True):
            msg_queue.put(('log', "D√πng video khu√¥n m·∫∑t n√≥i l√†m k·∫øt qu·∫£ cu·ªëi c√πng"))
            shutil.copy(str(talking_path), str(final_output))
            
            msg_queue.put(('progress', 100))
            msg_queue.put(('status', "‚úÖ Ho√†n th√†nh!"))
            msg_queue.put(('log', "X·ª≠ l√Ω video ho√†n t·∫•t!"))
            
            msg_queue.put(('complete', {
                'output_file': str(final_output),
                'file_size': os.path.getsize(final_output) / (1024*1024),
                'frame_count': frame_count,
                'fps': fps
            }))
            return
        
        # Ki·ªÉm tra n·∫øu qu√° tr√¨nh ti·∫øp theo c√≥ ƒë∆∞·ª£c th·ª±c hi·ªán hay kh√¥ng
        if cancel_event.is_set():
            msg_queue.put(('error', "Qu√° tr√¨nh ƒë√£ b·ªã h·ªßy b·ªüi ng∆∞·ªùi d√πng"))
            return
        
        # Gh√©p video MC v√† n·ªÅn
        if workflow_dict.get("video_overlay", True):
            msg_queue.put(('status', "üé¨ ƒêang gh√©p video..."))
            msg_queue.put(('log', "B·∫Øt ƒë·∫ßu gh√©p video..."))
            
            # Truy·ªÅn tr·ª±c ti·∫øp t√™n ti·∫øng Vi·ªát
            overlay_cmd = [
                "python", "video_overlay.py",
                "-m", str(talking_path),
                "-b", str(actual_bg_path),
                "-o", str(output_file),
                "-p", position_val,
                "-s", str(scale_val),
                "-q", quality_val
            ]
            
            msg_queue.put(('log', f"Ch·∫°y l·ªánh gh√©p video: {' '.join(overlay_cmd)}"))
            
            try:
                # Ch·∫°y l·ªánh v·ªõi timeout
                result = subprocess.run(
                    overlay_cmd,
                    capture_output=True,
                    text=True,
                    timeout=1800  # 30 ph√∫t
                )
                
                # Log output ƒë·∫ßy ƒë·ªß ƒë·ªÉ debug
                if result.stdout.strip():
                    msg_queue.put(('log', f"Output: {result.stdout}"))
                if result.stderr.strip():
                    msg_queue.put(('log', f"Error: {result.stderr}"))
                
                if result.returncode != 0:
                    # Fallback: S·ª≠ d·ª•ng ffmpeg tr·ª±c ti·∫øp
                    msg_queue.put(('log', "D√πng ffmpeg tr·ª±c ti·∫øp ƒë·ªÉ gh√©p video"))
                    
                    # √Ånh x·∫° v·ªã tr√≠ cho ffmpeg
                    positions = {
                        "G√≥c tr√™n tr√°i": "10:10",
                        "G√≥c tr√™n ph·∫£i": "main_w-overlay_w-10:10",
                        "G√≥c d∆∞·ªõi tr√°i": "10:main_h-overlay_h-10",
                        "G√≥c d∆∞·ªõi ph·∫£i": "main_w-overlay_w-10:main_h-overlay_h-10",
                        "Ch√≠nh gi·ªØa": "(main_w-overlay_w)/2:(main_h-overlay_h)/2"
                    }
                    pos = positions.get(position_val, positions["G√≥c d∆∞·ªõi ph·∫£i"])
                    
                    fallback_cmd = [
                        "ffmpeg", "-y",
                        "-i", str(actual_bg_path),
                        "-i", str(talking_path),
                        "-filter_complex", f"[1:v]scale=iw*{scale_val}:ih*{scale_val}[overlay];[0:v][overlay]overlay={pos}",
                        "-c:v", "libx264",
                        "-preset", {"low": "ultrafast", "medium": "medium", "high": "slow"}.get(quality_val, "medium"),
                        "-crf", "23",
                        str(output_file)
                    ]
                    
                    # Th√™m audio mapping
                    try:
                        # Ki·ªÉm tra stream audio
                        audio_check = subprocess.run(
                            ["ffprobe", "-v", "error", "-show_entries", "stream=codec_type", "-of", "json", str(talking_path)],
                            capture_output=True, text=True
                        )
                        
                        if "audio" in audio_check.stdout:
                            fallback_cmd.extend(["-map", "1:a", "-c:a", "aac", "-b:a", "192k"])
                        else:
                            audio_check_bg = subprocess.run(
                                ["ffprobe", "-v", "error", "-show_entries", "stream=codec_type", "-of", "json", str(actual_bg_path)],
                                capture_output=True, text=True
                            )
                            if "audio" in audio_check_bg.stdout:
                                fallback_cmd.extend(["-map", "0:a", "-c:a", "aac", "-b:a", "192k"])
                            else:
                                fallback_cmd.append("-an")
                    except Exception:
                        fallback_cmd.extend(["-map", "0:a?", "-c:a", "aac"])
                    
                    msg_queue.put(('log', f"L·ªánh fallback: {' '.join(fallback_cmd)}"))
                    fallback_result = subprocess.run(fallback_cmd, capture_output=True, text=True, timeout=1800)
                    
                    if fallback_result.returncode != 0:
                        msg_queue.put(('error', f"L·ªói khi gh√©p video v·ªõi ph∆∞∆°ng √°n d·ª± ph√≤ng: {fallback_result.stderr}"))
                        return
                    
                    msg_queue.put(('log', "ƒê√£ gh√©p video b·∫±ng ph∆∞∆°ng √°n d·ª± ph√≤ng"))
            except subprocess.TimeoutExpired:
                msg_queue.put(('error', "Qu√° th·ªùi gian x·ª≠ l√Ω khi gh√©p video (30 ph√∫t)"))
                return
            except Exception as e:
                error_details = traceback.format_exc()
                msg_queue.put(('error', f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi gh√©p video: {str(e)}\n{error_details}"))
                return
            
            # Ki·ªÉm tra file ƒë·∫ßu ra
            if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:
                msg_queue.put(('error', f"File ƒë·∫ßu ra kh√¥ng h·ª£p l·ªá: {output_file}"))
                return
            
            msg_queue.put(('log', f"‚úÖ ƒê√£ gh√©p video th√†nh c√¥ng: {output_file}"))
            msg_queue.put(('progress', 80))
        else:
            msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc gh√©p video"))
            # N·∫øu b·ªè qua, s·ª≠ d·ª•ng video khu√¥n m·∫∑t l√†m k·∫øt qu·∫£
            shutil.copy(str(talking_path), str(output_file))
            msg_queue.put(('progress', 80))
        
        # Ki·ªÉm tra h·ªßy
        if cancel_event.is_set():
            msg_queue.put(('error', "Qu√° tr√¨nh ƒë√£ b·ªã h·ªßy b·ªüi ng∆∞·ªùi d√πng"))
            return
        
        # S·ª≠ d·ª•ng output_file l√†m k·∫øt qu·∫£ cu·ªëi c√πng (kh√¥ng c√≥ subtitle)
        shutil.copy(str(output_file), str(final_output))
        
        # Ho√†n t·∫•t
        msg_queue.put(('progress', 100))
        msg_queue.put(('status', "‚úÖ Ho√†n th√†nh!"))
        msg_queue.put(('log', "X·ª≠ l√Ω video ho√†n t·∫•t!"))
        
        # Ki·ªÉm tra file ƒë·∫ßu ra cu·ªëi c√πng
        if not os.path.exists(final_output) or os.path.getsize(final_output) == 0:
            msg_queue.put(('error', f"File ƒë·∫ßu ra cu·ªëi c√πng kh√¥ng h·ª£p l·ªá: {final_output}"))
            # Th·ª≠ copy file output n·∫øu c√≥
            if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
                shutil.copy(output_file, final_output)
                msg_queue.put(('log', f"ƒê√£ sao ch√©p video l√†m k·∫øt qu·∫£ cu·ªëi c√πng"))
            else:
                return
        
        # Th√™m v√†o l·ªãch s·ª≠ v√† ho√†n th√†nh
        msg_queue.put(('complete', {
            'output_file': str(final_output),
            'file_size': os.path.getsize(final_output) / (1024*1024),
            'frame_count': frame_count if 'frame_count' in locals() else 0,
            'fps': fps if 'fps' in locals() else 0
        }))
        
    except Exception as e:
        error_details = traceback.format_exc()
        msg_queue.put(('error', f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}\n{error_details}"))
    finally:
        # D·ªçn d·∫πp
        try:
            shutil.rmtree(temp_dir, ignore_errors=True)
        except Exception as e:
            msg_queue.put(('log', f"L·ªói khi d·ªçn d·∫πp: {str(e)}"))

# === Main App ===
def main():
    # C·∫•u h√¨nh trang
    st.set_page_config(
        page_title="üé≠ AI Video Creator",
        page_icon="üé≠",
        layout="wide"
    )
    
    # Kh·ªüi t·∫°o session state
    init_session_state()
    
    # L·∫•y c·∫•u h√¨nh
    config = get_colab_config()
    
    # Header
    st.title("üé≠ AI Video Creator")
    st.caption("T·∫°o video khu√¥n m·∫∑t n√≥i v·ªõi AI - Phi√™n b·∫£n Google Colab")
    
    # Hi·ªÉn th·ªã th√¥ng tin c·∫•u h√¨nh
    with st.expander("‚öôÔ∏è Th√¥ng tin h·ªá th·ªëng", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"üìÅ **Models**: {config['data_root']}")
            st.info(f"üéÆ **GPU**: {config['gpu_arch']}")
        with col2:
            st.info(f"üìä **Config**: {config['cfg_pkl']}")
            st.info(f"üîß **Tr·∫°ng th√°i**: {'‚úÖ S·∫µn s√†ng' if os.path.exists(config['data_root']) else '‚ùå Ch∆∞a s·∫µn s√†ng'}")
    
    # Kh·ªüi t·∫°o editor
    editor = VideoEditor(output_dir="./output")
    
    # === Sidebar cho c√†i ƒë·∫∑t ===
    with st.sidebar:
        st.title("‚öôÔ∏è C√†i ƒë·∫∑t chung")
        
        # Workflow configuration
        st.subheader("üîÑ Quy tr√¨nh x·ª≠ l√Ω")
        workflow_steps = {}
        for step_id, step_name in WORKFLOW_STEPS.items():
            workflow_steps[step_id] = st.checkbox(
                step_name,
                value=st.session_state.workflow_steps.get(step_id, True),
                key=f"workflow_{step_id}"
            )
        st.session_state.workflow_steps = workflow_steps
        
        # Quality settings
        st.subheader("üìä Ch·∫•t l∆∞·ª£ng")
        quality = st.select_slider(
            "Ch·∫•t l∆∞·ª£ng video",
            options=["low", "medium", "high"],
            value="medium",
            format_func=lambda x: {"low": "Th·∫•p", "medium": "Trung b√¨nh", "high": "Cao"}[x]
        )
        
        # Show logs
        show_logs = st.checkbox("Hi·ªÉn th·ªã logs chi ti·∫øt", value=False)
        
        st.divider()
        
        with st.expander("üí° M·∫πo s·ª≠ d·ª•ng", expanded=False):
            st.markdown("""
            **M·∫πo t·ªëi ∆∞u:**
            - MC n√™n c√≥ n·ªÅn ƒë·ªìng m√†u ho·∫∑c trong su·ªët
            - Video n·ªÅn n√™n c√≥ ƒë·ªãnh d·∫°ng 16:9
            - Audio n√™n r√µ r√†ng, kh√¥ng nhi·ªÖu
            
            **ƒê·ªãnh d·∫°ng h·ªó tr·ª£:**
            - MC: JPG, PNG, MP4
            - N·ªÅn: MP4
            - Audio: WAV, MP3
            """)
    
    # === Tabs ch√≠nh ===
    tabs = st.tabs(["üé¨ T·∫°o Video MC", "üé≠ Video Khu√¥n M·∫∑t AI", "üéôÔ∏è Text-to-Speech", "üìã L·ªãch S·ª≠", "‚öôÔ∏è C√†i ƒê·∫∑t", "‚ùì H∆∞·ªõng D·∫´n"])
    
    # === Tab 0: T·∫°o Video MC ===
    with tabs[0]:
        col1, col2 = st.columns([3, 2])
        
        with col1:
            st.subheader("üìÅ T·∫£i l√™n files")
            
            # MC uploader
            mc_file = st.file_uploader(
                "T·∫£i l√™n ·∫¢nh/Video MC",
                type=["png", "jpg", "jpeg", "mp4"],
                help="·∫¢nh ho·∫∑c video c·ªßa ng∆∞·ªùi MC"
            )
            if mc_file:
                if Path(mc_file.name).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    st.image(mc_file, use_container_width=True, caption="Xem tr∆∞·ªõc MC")
                else:
                    st.video(mc_file)
            else:
                mc_path = st.selectbox(
                    "Ho·∫∑c ch·ªçn file m·∫´u:",
                    options=[""] + [str(p) for p in Path("./example").glob("*.[jp][pn]g")] + [str(p) for p in Path("./example").glob("*mc*.mp4")],
                    format_func=lambda x: Path(x).name if x else "Ch·ªçn file m·∫´u...",
                    key="mc_sample"
                )
                if mc_path:
                    if Path(mc_path).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                        st.image(mc_path, use_container_width=True, caption="Xem tr∆∞·ªõc MC")
                    else:
                        st.video(mc_path)
            
            # BG uploader
            bg_file = st.file_uploader(
                "T·∫£i l√™n Video N·ªÅn",
                type=["mp4"],
                help="Video n·ªÅn ƒë·ªÉ gh√©p v·ªõi MC"
            )
            if bg_file:
                st.video(bg_file)
            else:
                bg_path = st.selectbox(
                    "Ho·∫∑c ch·ªçn video n·ªÅn m·∫´u:",
                    options=[""] + [str(p) for p in Path("./example").glob("*bg*.mp4")],
                    format_func=lambda x: Path(x).name if x else "Ch·ªçn video n·ªÅn m·∫´u...",
                    key="bg_sample"
                )
                if bg_path:
                    st.video(bg_path)
            
            # Audio source
            audio_source = st.radio(
                "Ngu·ªìn audio:",
                ["Upload file", "T·∫°o t·ª´ vƒÉn b·∫£n"],
                horizontal=True
            )
            
            if audio_source == "Upload file":
                audio_file = st.file_uploader(
                    "T·∫£i l√™n Audio tho·∫°i",
                    type=["wav", "mp3"]
                )
                if audio_file:
                    st.audio(audio_file)
                else:
                    audio_path = st.selectbox(
                        "Ho·∫∑c ch·ªçn audio m·∫´u:",
                        options=[""] + [str(p) for p in Path("./example").glob("*.wav")] + [str(p) for p in Path("./example").glob("*.mp3")],
                        format_func=lambda x: Path(x).name if x else "Ch·ªçn audio m·∫´u...",
                        key="audio_sample"
                    )
                    if audio_path:
                        st.audio(audio_path)
                text_prompt = None
            else:
                audio_file = None
                audio_path = None
                text_prompt = st.text_area(
                    "Nh·∫≠p vƒÉn b·∫£n tho·∫°i:",
                    height=150,
                    placeholder="Nh·∫≠p n·ªôi dung b·∫°n mu·ªën MC n√≥i..."
                )
                
                # TTS settings ng·∫Øn g·ªçn
                if text_prompt:
                    with st.expander("üéôÔ∏è C√†i ƒë·∫∑t TTS nhanh", expanded=True):
                        tts_service = st.selectbox(
                            "D·ªãch v·ª• TTS:",
                            options=["Edge TTS", "OpenAI TTS", "GPT-4o-mini-TTS"],
                            index=0,
                            key="tts_service_main"
                        )
                        
                        if tts_service == "Edge TTS":
                            tts_voice = st.selectbox(
                                "Gi·ªçng ƒë·ªçc:",
                                options=["vi-VN-NamMinhNeural", "vi-VN-HoaiMyNeural"],
                                key="tts_voice_main"
                            )
                            tts_speed = st.slider("T·ªëc ƒë·ªô:", 0.8, 1.5, 1.2, 0.1, key="tts_speed_main")
                            tts_instructions = ""
                        elif tts_service == "OpenAI TTS":
                            tts_voice = st.selectbox(
                                "Gi·ªçng ƒë·ªçc:",
                                options=["alloy", "echo", "fable", "onyx", "nova", "shimmer"],
                                key="tts_voice_main"
                            )
                            tts_speed = st.slider("T·ªëc ƒë·ªô:", 0.8, 1.5, 1.2, 0.1, key="tts_speed_main")
                            tts_instructions = ""
                        else:  # GPT-4o-mini-TTS
                            tts_voice = st.selectbox(
                                "Gi·ªçng ƒë·ªçc:",
                                options=["Ash", "Ballad", "Coral", "Echo", "Fable", "Onyx", "Nova", "Sage", "Shimmer", "Verse"],
                                index=8,
                                key="tts_voice_main"
                            )
                            tts_speed = 1.2
                            tts_instructions = st.text_area(
                                "H∆∞·ªõng d·∫´n gi·ªçng ƒëi·ªáu:",
                                value="Tone: T·ª± nhi√™n, tr√¥i ch·∫£y\nEmotion: Nhi·ªát t√¨nh, t·ª± tin\nDelivery: R√µ r√†ng, nh·ªãp ƒë·ªô v·ª´a ph·∫£i",
                                height=80,
                                key="tts_instructions_main"
                            )
            
            # MC Settings
            st.subheader("üéõÔ∏è C√†i ƒë·∫∑t MC")
            
            # Position and scale
            col_p, col_s = st.columns(2)
            with col_p:
                position = st.selectbox(
                    "V·ªã tr√≠ MC",
                    ["G√≥c tr√™n tr√°i", "G√≥c tr√™n ph·∫£i", "G√≥c d∆∞·ªõi tr√°i", "G√≥c d∆∞·ªõi ph·∫£i", "Ch√≠nh gi·ªØa"],
                    index=3
                )
            
            with col_s:
                auto_scale = st.checkbox(
                    "T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc",
                    value=st.session_state.auto_scale
                )
                st.session_state.auto_scale = auto_scale
            
            # Scale calculation
            scale = 0.25
            if auto_scale and bg_file:
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=Path(bg_file.name).suffix) as temp:
                        temp.write(bg_file.getbuffer())
                        bg_temp_path = temp.name
                    
                    width, height = get_video_resolution(bg_temp_path)
                    os.unlink(bg_temp_path)
                    
                    scale = calculate_auto_scale(mc_file if mc_file else mc_path, width, height)
                    st.write(f"K√≠ch th∆∞·ªõc t·ª± ƒë·ªông: {scale:.2f}")
                except Exception:
                    scale = 0.25
            elif auto_scale and 'bg_path' in locals() and bg_path:
                try:
                    width, height = get_video_resolution(bg_path)
                    scale = calculate_auto_scale(mc_file if mc_file else mc_path, width, height)
                    st.write(f"K√≠ch th∆∞·ªõc t·ª± ƒë·ªông: {scale:.2f}")
                except Exception:
                    scale = 0.25
            else:
                scale = st.slider("K√≠ch th∆∞·ªõc", 0.1, 0.5, 0.25, 0.05)
            
            # Advanced mouth controls
            with st.expander("üó£Ô∏è ƒêi·ªÅu khi·ªÉn kh·∫©u h√¨nh n√¢ng cao", expanded=False):
                vad_alpha = st.slider(
                    "M·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i:",
                    min_value=0.0, max_value=1.0, value=1.0, step=0.05,
                    help="Gi√° tr·ªã th·∫•p h∆°n s·∫Ω l√†m gi·∫£m chuy·ªÉn ƒë·ªông m√¥i"
                )
                
                mouth_advanced = st.checkbox("T√πy ch·ªçn n√¢ng cao", value=False)
                if mouth_advanced:
                    exp_components = st.multiselect(
                        "Th√†nh ph·∫ßn bi·ªÉu c·∫£m:",
                        options=["exp", "pitch", "yaw", "roll", "t"],
                        default=["exp", "pitch", "yaw", "roll", "t"],
                        help="Ch·ªçn c√°c th√†nh ph·∫ßn bi·ªÉu c·∫£m ƒë·ªÉ s·ª≠ d·ª•ng"
                    )
                    
                    exp_scale = st.slider(
                        "T·ª∑ l·ªá bi·ªÉu c·∫£m mi·ªáng:",
                        min_value=0.5, max_value=1.5, value=1.0, step=0.1
                    )
                    
                    pose_scale = st.slider(
                        "T·ª∑ l·ªá chuy·ªÉn ƒë·ªông ƒë·∫ßu:",
                        min_value=0.5, max_value=1.5, value=1.0, step=0.1
                    )
                    
                    delta_exp_enabled = st.checkbox("Th√™m offset bi·ªÉu c·∫£m m√¥i", value=False)
                    delta_exp_value = 0.0
                    if delta_exp_enabled:
                        delta_exp_value = st.slider(
                            "Gi√° tr·ªã offset:",
                            min_value=-0.2, max_value=0.2, value=0.0, step=0.01
                        )
                else:
                    exp_components = None
                    exp_scale = 1.0
                    pose_scale = 1.0
                    delta_exp_enabled = False
                    delta_exp_value = 0.0
            
            # AI Model selection
            ai_model = st.selectbox(
                "M√¥ h√¨nh AI:",
                options=["M√¥ h√¨nh m·∫∑c ƒë·ªãnh", "M√¥ h√¨nh t·ªëi ∆∞u h√≥a"],
                help="Ch·ªçn m√¥ h√¨nh AI ƒë·ªÉ t·∫°o video",
                index=0
            )
            
            # Submit button
            submitted = st.button(
                "üöÄ T·∫°o Video MC",
                use_container_width=True,
                type="primary",
                disabled=st.session_state.processing
            )
        
        with col2:
            # T·∫°o c√°c placeholder cho UI tr·∫°ng th√°i
            elapsed_time_container = st.empty()
            status_container = st.empty()
            progress_container = st.empty()
            metrics_container = st.container()
            cancel_container = st.empty()
            log_container = st.container()
            
            if st.session_state.processing:
                status_container.subheader("‚è≥ ƒêang x·ª≠ l√Ω...")
                
                # Hi·ªÉn th·ªã th·ªùi gian x·ª≠ l√Ω
                if st.session_state.process_start_time:
                    elapsed = time.time() - st.session_state.process_start_time
                    elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
                
                progress = progress_container.progress(0)
                
                # N√∫t h·ªßy x·ª≠ l√Ω
                cancel_button = cancel_container.button("üõë H·ªßy x·ª≠ l√Ω", use_container_width=True)
                
                # Hi·ªÉn th·ªã logs
                if show_logs:
                    log_container.markdown("**Logs:**")
                    log_content = log_container.code("\n".join(st.session_state.logs[-20:]))
                
            elif st.session_state.complete and st.session_state.output_file:
                status_container.subheader("‚úÖ ƒê√£ ho√†n th√†nh!")
                output_file = st.session_state.output_file
                
                if Path(output_file).exists():
                    metrics_container.video(output_file)
                    
                    file_stats = Path(output_file).stat()
                    
                    # Hi·ªÉn th·ªã th√¥ng tin video
                    cols = metrics_container.columns(2)
                    cols[0].metric("K√≠ch th∆∞·ªõc", f"{file_stats.st_size / (1024*1024):.1f} MB")
                    cols[1].metric("Th·ªùi gian t·∫°o", datetime.fromtimestamp(file_stats.st_mtime).strftime("%H:%M:%S"))
                    
                    with open(output_file, "rb") as file:
                        cancel_container.download_button(
                            "üíæ T·∫£i xu·ªëng video",
                            file,
                            file_name=Path(output_file).name,
                            mime="video/mp4",
                            use_container_width=True
                        )
            else:
                status_container.subheader("Tr·∫°ng th√°i")
                metrics_container.info("Nh·∫•n n√∫t 'T·∫°o Video MC' ƒë·ªÉ b·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
                
                # Preview ch·ªâ hi·ªÉn th·ªã n·∫øu kh√¥ng ƒëang trong qu√° tr√¨nh x·ª≠ l√Ω ho·∫∑c ho√†n th√†nh
                if mc_file or ('mc_path' in locals() and mc_path):
                    preview_container = log_container.container()
                    preview_container.subheader("Xem tr∆∞·ªõc MC")
                    if mc_file:
                        if Path(mc_file.name).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                            preview_container.image(mc_file, use_container_width=True)
                        else:
                            preview_container.video(mc_file)
                    elif 'mc_path' in locals() and mc_path:
                        if Path(mc_path).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                            preview_container.image(mc_path, use_container_width=True)
                        else:
                            preview_container.video(mc_path)
        
        # X·ª≠ l√Ω khi submit
        if submitted and not st.session_state.processing:
            # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c√°c files
            mc_path_final = mc_file if mc_file else (mc_path if 'mc_path' in locals() and mc_path else None)
            bg_path_final = bg_file if bg_file else (bg_path if 'bg_path' in locals() and bg_path else None)
            audio_path_final = audio_file if audio_file else (audio_path if 'audio_path' in locals() and audio_path else None)
            
            if ((mc_path_final and bg_path_final) and (audio_path_final or text_prompt)):
                st.session_state.processing = True
                st.session_state.process_start_time = time.time()
                st.session_state.logs = ["B·∫Øt ƒë·∫ßu qu√° tr√¨nh x·ª≠ l√Ω video MC..."]
                
                # C·∫≠p nh·∫≠t UI tr·∫°ng th√°i ban ƒë·∫ßu
                status_container.subheader("‚è≥ ƒêang x·ª≠ l√Ω...")
                elapsed = time.time() - st.session_state.process_start_time
                elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
                progress = progress_container.progress(0)
                cancel_button = cancel_container.button("üõë H·ªßy x·ª≠ l√Ω", key="cancel_processing", use_container_width=True)
                if show_logs:
                    log_container.markdown("**Logs:**")
                    log_content = log_container.code("ƒêang b·∫Øt ƒë·∫ßu...")
                
                # T·∫°o tempdir v√† ƒë∆∞·ªùng d·∫´n
                temp_dir = Path(tempfile.mkdtemp())
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                
                # Chu·∫©n b·ªã h√†ng ƒë·ª£i giao ti·∫øp
                msg_queue = queue.Queue()
                cancel_event = threading.Event()
                
                # Chu·∫©n b·ªã containers cho handler
                ui_containers = {
                    'status': status_container,
                    'progress': progress,
                    'log_content': log_content if show_logs else None,
                    'metrics': metrics_container
                }
                
                # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ truy·ªÅn cho thread
                tts_service_val = tts_service if audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" else "Edge TTS"
                tts_voice_val = tts_voice if audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" else "vi-VN-NamMinhNeural"
                tts_speed_val = tts_speed if audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" and tts_service != "GPT-4o-mini-TTS" else 1.2
                tts_instructions_val = tts_instructions if audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" and tts_service == "GPT-4o-mini-TTS" else ""
                
                # Kh·ªüi ch·∫°y thread x·ª≠ l√Ω
                thread = threading.Thread(
                    target=process_video,
                    args=(
                        st.session_state.workflow_steps,
                        mc_path_final, bg_path_final, audio_path_final, text_prompt,
                        temp_dir, msg_queue, cancel_event, editor, timestamp,
                        tts_service_val, tts_voice_val, tts_speed_val,
                    ),
                    kwargs={
                        'tts_instructions_val': tts_instructions_val,
                        'position_val': position,
                        'scale_val': scale,
                        'quality_val': quality,
                        'ai_model_val': ai_model,
                        'vad_alpha': vad_alpha,
                        'exp_components': exp_components,
                        'exp_scale': exp_scale,
                        'pose_scale': pose_scale,
                        'delta_exp_enabled': delta_exp_enabled,
                        'delta_exp_value': delta_exp_value,
                    }
                )
                thread.daemon = True
                thread.start()
                
                # UI theo d√µi ti·∫øn tr√¨nh
                try:
                    while thread.is_alive() or not msg_queue.empty():
                        # C·∫≠p nh·∫≠t th·ªùi gian x·ª≠ l√Ω
                        if st.session_state.process_start_time:
                            elapsed = time.time() - st.session_state.process_start_time
                            elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
                        
                        # Ki·ªÉm tra n·∫øu ƒë√£ nh·∫•n n√∫t h·ªßy
                        if cancel_button:
                            cancel_event.set()
                            st.session_state.processing = False
                            st.warning("ƒê√£ h·ªßy qu√° tr√¨nh x·ª≠ l√Ω")
                            break
                        
                        # X·ª≠ l√Ω th√¥ng ƒëi·ªáp
                        try:
                            msg_type, content = msg_queue.get(timeout=0.1)
                            handle_message(msg_type, content, ui_containers, show_logs)
                            msg_queue.task_done()
                            
                            if msg_type == 'complete':
                                time.sleep(0.5)
                                st.rerun()
                        except queue.Empty:
                            time.sleep(0.1)
                        
                        time.sleep(0.05)
                except Exception as e:
                    st.error(f"L·ªói UI: {str(e)}")
                    st.session_state.processing = False
            else:
                st.error("Vui l√≤ng ch·ªçn ƒë·∫ßy ƒë·ªß: MC, video n·ªÅn, v√† audio (ho·∫∑c nh·∫≠p vƒÉn b·∫£n)")
    
    # === Tab 1: T·∫°o Video Khu√¥n M·∫∑t AI ===
    with tabs[1]:
        st.subheader("üé≠ T·∫°o Video Khu√¥n M·∫∑t N√≥i v·ªõi AI")
        st.write("Chuy·ªÉn ƒë·ªïi ·∫£nh ho·∫∑c video MC tƒ©nh th√†nh video v·ªõi kh·∫£ nƒÉng n√≥i theo audio")
        
        ai_col1, ai_col2 = st.columns([3, 2])
        
        with ai_col1:
            st.subheader("üìÅ T·∫£i l√™n files ƒë·∫ßu v√†o")
            
            # MC uploader
            ai_mc_file = st.file_uploader(
                "T·∫£i l√™n ·∫¢nh/Video MC",
                type=["png", "jpg", "jpeg", "mp4"],
                key="ai_mc_file"
            )
            if ai_mc_file:
                if Path(ai_mc_file.name).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    st.image(ai_mc_file, use_container_width=True, caption="MC Preview")
                else:
                    st.video(ai_mc_file)
            else:
                ai_mc_path = st.selectbox(
                    "Ho·∫∑c ch·ªçn file m·∫´u:",
                    options=[""] + [str(p) for p in Path("./example").glob("*.[jp][pn]g")] + [str(p) for p in Path("./example").glob("*mc*.mp4")],
                    format_func=lambda x: Path(x).name if x else "Ch·ªçn file m·∫´u...",
                    key="ai_mc_sample"
                )
                if ai_mc_path:
                    if Path(ai_mc_path).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                        st.image(ai_mc_path, use_container_width=True, caption="MC Preview")
                    else:
                        st.video(ai_mc_path)
            
            # Audio source
            ai_audio_source = st.radio(
                "Ngu·ªìn audio:",
                ["Upload file", "T·∫°o t·ª´ vƒÉn b·∫£n"],
                horizontal=True,
                key="ai_audio_source"
            )
            
            if ai_audio_source == "Upload file":
                ai_audio_file = st.file_uploader(
                    "T·∫£i l√™n Audio tho·∫°i",
                    type=["wav", "mp3"],
                    key="ai_audio_file"
                )
                if ai_audio_file:
                    st.audio(ai_audio_file)
                else:
                    ai_audio_path = st.selectbox(
                        "Ho·∫∑c ch·ªçn audio m·∫´u:",
                        options=[""] + [str(p) for p in Path("./example").glob("*.wav")] + [str(p) for p in Path("./example").glob("*.mp3")],
                        format_func=lambda x: Path(x).name if x else "Ch·ªçn audio m·∫´u...",
                        key="ai_audio_sample"
                    )
                    if ai_audio_path:
                        st.audio(ai_audio_path)
                ai_text_prompt = None
            else:
                ai_audio_file = None
                ai_audio_path = None
                ai_text_prompt = st.text_area(
                    "Nh·∫≠p vƒÉn b·∫£n tho·∫°i:",
                    height=150,
                    key="ai_text_prompt",
                    placeholder="Nh·∫≠p n·ªôi dung b·∫°n mu·ªën MC n√≥i..."
                )
                
                # TTS settings cho AI mode
                if ai_text_prompt:
                    with st.expander("üéôÔ∏è C√†i ƒë·∫∑t TTS", expanded=True):
                        ai_tts_service = st.selectbox(
                            "D·ªãch v·ª• TTS:",
                            options=["Edge TTS", "OpenAI TTS", "GPT-4o-mini-TTS"],
                            index=2,
                            key="ai_tts_service"
                        )
                        
                        if ai_tts_service == "Edge TTS":
                            ai_tts_voice = st.selectbox(
                                "Gi·ªçng ƒë·ªçc:",
                                options=["vi-VN-NamMinhNeural", "vi-VN-HoaiMyNeural"],
                                key="ai_tts_voice"
                            )
                            ai_tts_speed = st.slider("T·ªëc ƒë·ªô:", 0.8, 1.5, 1.2, 0.1, key="ai_tts_speed")
                            ai_tts_instructions = ""
                        elif ai_tts_service == "OpenAI TTS":
                            ai_tts_voice = st.selectbox(
                                "Gi·ªçng ƒë·ªçc:",
                                options=["alloy", "echo", "fable", "onyx", "nova", "shimmer"],
                                key="ai_tts_voice"
                            )
                            ai_tts_speed = st.slider("T·ªëc ƒë·ªô:", 0.8, 1.5, 1.2, 0.1, key="ai_tts_speed")
                            ai_tts_instructions = ""
                        else:
                            ai_tts_voice = st.selectbox(
                                "Gi·ªçng ƒë·ªçc:",
                                options=["Ash", "Ballad", "Coral", "Echo", "Fable", "Onyx", "Nova", "Sage", "Shimmer", "Verse"],
                                index=8,
                                key="ai_tts_voice"
                            )
                            ai_tts_speed = 1.2
                            ai_tts_instructions = st.text_area(
                                "H∆∞·ªõng d·∫´n gi·ªçng ƒëi·ªáu:",
                                value="Tone: T·ª± nhi√™n, tr√¥i ch·∫£y\nEmotion: Nhi·ªát t√¨nh, t·ª± tin\nDelivery: R√µ r√†ng, nh·ªãp ƒë·ªô v·ª´a ph·∫£i",
                                height=80,
                                key="ai_tts_instructions"
                            )
            
            # AI Model settings
            with st.expander("ü§ñ C√†i ƒë·∫∑t m√¥ h√¨nh AI", expanded=False):
                ai_model = st.selectbox(
                    "M√¥ h√¨nh AI:",
                    options=["M√¥ h√¨nh m·∫∑c ƒë·ªãnh", "M√¥ h√¨nh t·ªëi ∆∞u h√≥a"],
                    help="Ch·ªçn m√¥ h√¨nh AI ƒë·ªÉ t·∫°o video khu√¥n m·∫∑t n√≥i",
                    index=0,
                    key="ai_model"
                )
                
                ai_quality = st.select_slider(
                    "Ch·∫•t l∆∞·ª£ng video AI:",
                    options=["low", "medium", "high"],
                    value="medium",
                    key="ai_quality",
                    format_func=lambda x: {"low": "Th·∫•p", "medium": "Trung b√¨nh", "high": "Cao"}[x]
                )
            
            # T√πy ch·ªânh kh·∫©u h√¨nh AI
            with st.expander("üó£Ô∏è T√πy ch·ªânh kh·∫©u h√¨nh", expanded=False):
                ai_vad_alpha = st.slider(
                    "M·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i:",
                    min_value=0.0, max_value=1.0, value=1.0, step=0.05,
                    key="ai_vad_alpha",
                    help="Gi√° tr·ªã th·∫•p h∆°n s·∫Ω l√†m gi·∫£m chuy·ªÉn ƒë·ªông m√¥i"
                )
                
                ai_mouth_advanced = st.checkbox("T√πy ch·ªçn n√¢ng cao", value=False, key="ai_mouth_advanced")
                if ai_mouth_advanced:
                    ai_exp_components = st.multiselect(
                        "Th√†nh ph·∫ßn bi·ªÉu c·∫£m:",
                        options=["exp", "pitch", "yaw", "roll", "t"],
                        default=["exp", "pitch", "yaw", "roll", "t"],
                        key="ai_exp_components"
                    )
                    
                    ai_exp_scale = st.slider(
                        "T·ª∑ l·ªá bi·ªÉu c·∫£m mi·ªáng:",
                        min_value=0.5, max_value=1.5, value=1.0, step=0.1,
                        key="ai_exp_scale"
                    )
                    
                    ai_pose_scale = st.slider(
                        "T·ª∑ l·ªá chuy·ªÉn ƒë·ªông ƒë·∫ßu:",
                        min_value=0.5, max_value=1.5, value=1.0, step=0.1,
                        key="ai_pose_scale"
                    )
                    
                    ai_delta_exp_enabled = st.checkbox("Th√™m offset bi·ªÉu c·∫£m m√¥i", value=False, key="ai_delta_exp_enabled")
                    ai_delta_exp_value = 0.0
                    if ai_delta_exp_enabled:
                        ai_delta_exp_value = st.slider(
                            "Gi√° tr·ªã offset:",
                            min_value=-0.2, max_value=0.2, value=0.0, step=0.01,
                            key="ai_delta_exp_value"
                        )
                else:
                    ai_exp_components = None
                    ai_exp_scale = 1.0
                    ai_pose_scale = 1.0
                    ai_delta_exp_enabled = False
                    ai_delta_exp_value = 0.0
            
            # Submit button
            ai_submitted = st.button(
                "üöÄ T·∫°o Video Khu√¥n M·∫∑t N√≥i",
                use_container_width=True,
                type="primary",
                key="ai_create_button"
            )
        
        with ai_col2:
            # C·ªôt hi·ªÉn th·ªã ti·∫øn tr√¨nh v√† k·∫øt qu·∫£
            ai_elapsed_time_container = st.empty()
            ai_status_container = st.empty()
            ai_progress_container = st.empty()
            ai_metrics_container = st.container()
            ai_cancel_container = st.empty()
            ai_result_container = st.container()
            
            if st.session_state.processing:
                ai_status_container.subheader("‚è≥ ƒêang x·ª≠ l√Ω...")
                
                if st.session_state.process_start_time:
                    elapsed = time.time() - st.session_state.process_start_time
                    ai_elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
                
                progress = ai_progress_container.progress(0)
                cancel_button = ai_cancel_container.button("üõë H·ªßy x·ª≠ l√Ω", key="ai_cancel_processing", use_container_width=True)
                
            elif st.session_state.complete and st.session_state.output_file:
                ai_status_container.subheader("‚úÖ ƒê√£ ho√†n th√†nh!")
                
                output_file = st.session_state.output_file
                if Path(output_file).exists():
                    ai_metrics_container.video(output_file)
                    
                    file_stats = Path(output_file).stat()
                    cols = ai_metrics_container.columns(2)
                    cols[0].metric("K√≠ch th∆∞·ªõc", f"{file_stats.st_size / (1024*1024):.1f} MB")
                    cols[1].metric("Th·ªùi gian t·∫°o", datetime.fromtimestamp(file_stats.st_mtime).strftime("%H:%M:%S"))
                    
                    with open(output_file, "rb") as file:
                        ai_cancel_container.download_button(
                            "üíæ T·∫£i xu·ªëng video",
                            file,
                            file_name=Path(output_file).name,
                            mime="video/mp4",
                            use_container_width=True,
                            key="ai_download_button"
                        )
            else:
                ai_status_container.subheader("Tr·∫°ng th√°i")
                ai_metrics_container.info("Nh·∫•n n√∫t 'T·∫°o Video Khu√¥n M·∫∑t N√≥i' ƒë·ªÉ b·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
        
        # X·ª≠ l√Ω khi nh·∫•n n√∫t t·∫°o video AI
        if ai_submitted and not st.session_state.processing:
            # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n files
            mc_path_final = ai_mc_file if ai_mc_file else (ai_mc_path if 'ai_mc_path' in locals() and ai_mc_path else None)
            audio_path_final = ai_audio_file if ai_audio_file else (ai_audio_path if 'ai_audio_path' in locals() and ai_audio_path else None)
            
            if (mc_path_final and (audio_path_final or ai_text_prompt)):
                # Chu·∫©n b·ªã workflow ch·ªâ cho talking head generation
                ai_workflow_steps = {k: False for k in WORKFLOW_STEPS}
                ai_workflow_steps["prepare_files"] = True
                ai_workflow_steps["tts_generation"] = ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n"
                ai_workflow_steps["talking_head_generation"] = True
                
                # C√†i ƒë·∫∑t session state v√† UI
                st.session_state.processing = True
                st.session_state.process_start_time = time.time()
                st.session_state.logs = ["B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o video khu√¥n m·∫∑t n√≥i..."]
                
                # C·∫≠p nh·∫≠t UI tr·∫°ng th√°i ban ƒë·∫ßu
                ai_status_container.subheader("‚è≥ ƒêang x·ª≠ l√Ω...")
                elapsed = time.time() - st.session_state.process_start_time
                ai_elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
                progress = ai_progress_container.progress(0)
                cancel_button = ai_cancel_container.button("üõë H·ªßy x·ª≠ l√Ω", key="ai_cancel_processing", use_container_width=True)
                
                # Chu·∫©n b·ªã tempdir v√† ƒë∆∞·ªùng d·∫´n
                temp_dir = Path(tempfile.mkdtemp())
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                
                # Chu·∫©n b·ªã h√†ng ƒë·ª£i giao ti·∫øp
                msg_queue = queue.Queue()
                cancel_event = threading.Event()
                
                # Chu·∫©n b·ªã containers cho handler
                ui_containers = {
                    'status': ai_status_container,
                    'progress': progress,
                    'log_content': None,  # Kh√¥ng hi·ªÉn th·ªã log tr√™n tab n√†y
                    'metrics': ai_metrics_container
                }
                
                # Chu·∫©n b·ªã c√°c tham s·ªë TTS
                tts_service_val = ai_tts_service if ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" else "Edge TTS"
                tts_voice_val = ai_tts_voice if ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" else "vi-VN-NamMinhNeural"
                tts_speed_val = ai_tts_speed if ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" and ai_tts_service != "GPT-4o-mini-TTS" else 1.2
                tts_instructions_val = ai_tts_instructions if ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" and ai_tts_service == "GPT-4o-mini-TTS" else ""
                
                # Kh·ªüi ch·∫°y thread x·ª≠ l√Ω v·ªõi workflow ch·ªâ d√†nh cho AI talking head
                thread = threading.Thread(
                    target=process_video,
                    args=(
                        ai_workflow_steps,
                        mc_path_final, None, audio_path_final, ai_text_prompt,
                        temp_dir, msg_queue, cancel_event, editor, timestamp,
                        tts_service_val, tts_voice_val, tts_speed_val,
                    ),
                    kwargs={
                        'tts_instructions_val': tts_instructions_val,
                        'ai_model_val': ai_model,
                        'quality_val': ai_quality,
                        'vad_alpha': ai_vad_alpha,
                        'exp_components': ai_exp_components,
                        'exp_scale': ai_exp_scale,
                        'pose_scale': ai_pose_scale,
                        'delta_exp_enabled': ai_delta_exp_enabled,
                        'delta_exp_value': ai_delta_exp_value,
                    }
                )
                thread.daemon = True
                thread.start()
                
                # UI theo d√µi ti·∫øn tr√¨nh
                try:
                    while thread.is_alive() or not msg_queue.empty():
                        # C·∫≠p nh·∫≠t th·ªùi gian x·ª≠ l√Ω
                        if st.session_state.process_start_time:
                            elapsed = time.time() - st.session_state.process_start_time
                            ai_elapsed_time_container.caption(
                                f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}"
                            )
                        
                        # Ki·ªÉm tra n·∫øu ƒë√£ nh·∫•n n√∫t h·ªßy
                        if cancel_button:
                            cancel_event.set()
                            st.session_state.processing = False
                            st.warning("ƒê√£ h·ªßy qu√° tr√¨nh x·ª≠ l√Ω")
                            break
                        
                        # X·ª≠ l√Ω th√¥ng ƒëi·ªáp
                        try:
                            msg_type, content = msg_queue.get(timeout=0.1)
                            handle_message(msg_type, content, ui_containers, False)  # Kh√¥ng hi·ªÉn th·ªã logs
                            msg_queue.task_done()
                            
                            if msg_type == 'complete':
                                time.sleep(0.5)  # ƒê·ª£i UI c·∫≠p nh·∫≠t
                                st.rerun()
                        except queue.Empty:
                            time.sleep(0.1)
                        
                        time.sleep(0.05)
                except Exception as e:
                    st.error(f"L·ªói UI: {str(e)}")
                    st.session_state.processing = False
            else:
                st.error("Vui l√≤ng t·∫£i l√™n c·∫£ MC v√† audio (ho·∫∑c nh·∫≠p vƒÉn b·∫£n)")
    
    # === Tab 2: Text-to-Speech ===
    with tabs[2]:
        st.subheader("üéôÔ∏è Text-to-Speech - T·∫°o gi·ªçng n√≥i t·ª´ vƒÉn b·∫£n")
        st.write("T·∫°o v√† nghe th·ª≠ c√°c gi·ªçng n√≥i kh√°c nhau t·ª´ vƒÉn b·∫£n")
        
        tts_col1, tts_col2 = st.columns([2, 1])
        
        with tts_col1:
            # Text input
            tts_text = st.text_area(
                "üìù Nh·∫≠p vƒÉn b·∫£n:",
                height=150,
                placeholder="Nh·∫≠p n·ªôi dung b·∫°n mu·ªën chuy·ªÉn th√†nh gi·ªçng n√≥i...",
                key="tts_standalone_text"
            )
            
            # Service selection
            tts_service_standalone = st.selectbox(
                "üîß D·ªãch v·ª• TTS:",
                options=["Edge TTS", "OpenAI TTS", "GPT-4o-mini-TTS"],
                index=2,
                key="tts_service_standalone"
            )
            
            # Voice and settings based on service
            if tts_service_standalone == "Edge TTS":
                st.markdown("### üé§ C√†i ƒë·∫∑t Edge TTS")
                tts_voice_standalone = st.selectbox(
                    "Gi·ªçng ƒë·ªçc:",
                    options=["vi-VN-NamMinhNeural", "vi-VN-HoaiMyNeural"],
                    key="tts_voice_standalone"
                )
                tts_speed_standalone = st.slider(
                    "T·ªëc ƒë·ªô ƒë·ªçc:",
                    min_value=0.8, max_value=1.5, value=1.2, step=0.1,
                    key="tts_speed_standalone"
                )
                tts_instructions_standalone = ""
                
                # Voice descriptions
                voice_info = {
                    "vi-VN-NamMinhNeural": "Gi·ªçng nam tr·∫ª, r√µ r√†ng, ph√π h·ª£p cho n·ªôi dung ch√≠nh th·ª©c",
                    "vi-VN-HoaiMyNeural": "Gi·ªçng n·ªØ ·∫•m √°p, th√¢n thi·ªán, ph√π h·ª£p cho n·ªôi dung gi√°o d·ª•c"
                }
                st.caption(f"‚ÑπÔ∏è {voice_info[tts_voice_standalone]}")
                
            elif tts_service_standalone == "OpenAI TTS":
                st.markdown("### üé§ C√†i ƒë·∫∑t OpenAI TTS")
                tts_voice_standalone = st.selectbox(
                    "Gi·ªçng ƒë·ªçc:",
                    options=["alloy", "echo", "fable", "onyx", "nova", "shimmer"],
                    key="tts_voice_standalone"
                )
                tts_speed_standalone = st.slider(
                    "T·ªëc ƒë·ªô ƒë·ªçc:",
                    min_value=0.8, max_value=1.5, value=1.2, step=0.1,
                    key="tts_speed_standalone"
                )
                tts_instructions_standalone = ""
                
                # Voice descriptions for OpenAI
                openai_voice_info = {
                    "alloy": "Gi·ªçng trung t√≠nh, c√¢n b·∫±ng",
                    "echo": "Gi·ªçng nam tr·∫ª",
                    "fable": "Gi·ªçng nam tr∆∞·ªüng th√†nh",
                    "onyx": "Gi·ªçng nam tr·∫ßm",
                    "nova": "Gi·ªçng n·ªØ tr·∫ª",
                    "shimmer": "Gi·ªçng n·ªØ t∆∞∆°i s√°ng"
                }
                st.caption(f"‚ÑπÔ∏è {openai_voice_info[tts_voice_standalone]}")
                
            else:  # GPT-4o-mini-TTS
                st.markdown("### üé§ C√†i ƒë·∫∑t GPT-4o-mini-TTS")
                tts_voice_standalone = st.selectbox(
                    "Gi·ªçng ƒë·ªçc:",
                    options=["Ash", "Ballad", "Coral", "Echo", "Fable", "Onyx", "Nova", "Sage", "Shimmer", "Verse"],
                    index=8,  # Shimmer
                    key="tts_voice_standalone"
                )
                
                # Hi·ªÉn th·ªã m√¥ t·∫£ gi·ªçng n√≥i
                st.caption(f"‚ÑπÔ∏è **{tts_voice_standalone}**: {VOICE_DESCRIPTIONS.get(tts_voice_standalone, '')}")
                
                tts_speed_standalone = 1.2  # Fixed for GPT-4o
                
                # Kh·ªüi t·∫°o gi√° tr·ªã preset n·∫øu ch∆∞a c√≥
                if 'tts_instructions_preset' not in st.session_state:
                    st.session_state.tts_instructions_preset = "Tone: T·ª± nhi√™n, tr√¥i ch·∫£y, chuy√™n nghi·ªáp\nEmotion: Nhi·ªát t√¨nh, t·ª± tin\nDelivery: R√µ r√†ng, nh·ªãp ƒë·ªô v·ª´a ph·∫£i, nh·∫•n m·∫°nh t·ª´ kh√≥a quan tr·ªçng"
                
                tts_instructions_standalone = st.text_area(
                    "üé≠ H∆∞·ªõng d·∫´n v·ªÅ gi·ªçng ƒëi·ªáu:",
                    value=st.session_state.tts_instructions_preset,
                    height=120,
                    key="tts_instructions_standalone",
                    help="M√¥ t·∫£ t√¥ng gi·ªçng, c·∫£m x√∫c v√† c√°ch truy·ªÅn ƒë·∫°t mong mu·ªën"
                )
                
                # C·∫≠p nh·∫≠t preset khi ng∆∞·ªùi d√πng thay ƒë·ªïi
                if tts_instructions_standalone != st.session_state.tts_instructions_preset:
                    st.session_state.tts_instructions_preset = tts_instructions_standalone
                
                # Instruction templates
                with st.expander("üìã M·∫´u h∆∞·ªõng d·∫´n gi·ªçng ƒëi·ªáu", expanded=False):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**üé§ Gi·ªçng di·ªÖn thuy·∫øt:**")
                        if st.button("S·ª≠ d·ª•ng", key="preset_speech"):
                            st.session_state.tts_instructions_preset = "Tone: ƒêƒ©nh ƒë·∫°c, trang tr·ªçng, ƒë·∫ßy t·ª± tin\nEmotion: Nhi·ªát huy·∫øt, quy·∫øt ƒëo√°n\nDelivery: Nh·ªãp ƒë·ªô v·ª´a ph·∫£i v·ªõi c√°c ng·∫Øt qu√£ng, nh·∫•n m·∫°nh t·ª´ kh√≥a quan tr·ªçng"
                            st.rerun()
                        
                        st.markdown("**üíº Gi·ªçng thuy·∫øt tr√¨nh:**")
                        if st.button("S·ª≠ d·ª•ng", key="preset_presentation"):
                            st.session_state.tts_instructions_preset = "Tone: Chuy√™n nghi·ªáp, r√µ r√†ng, t·ª± tin\nEmotion: T·∫≠p trung, nghi√™m t√∫c\nDelivery: Nh·ªãp ƒë·ªô ƒë·ªÅu ƒë·∫∑n, ph√°t √¢m r√µ r√†ng t·ª´ng t·ª´"
                            st.rerun()
                    
                    with col2:
                        st.markdown("**ü§ù Gi·ªçng t∆∞ v·∫•n:**")
                        if st.button("S·ª≠ d·ª•ng", key="preset_consulting"):
                            st.session_state.tts_instructions_preset = "Tone: ·∫§m √°p, th√¢n thi·ªán, ƒë√°ng tin c·∫≠y\nEmotion: Th·∫•u hi·ªÉu, quan t√¢m\nDelivery: Nh·∫π nh√†ng, r√µ r√†ng, t·∫°o c·∫£m gi√°c an t√¢m"
                            st.rerun()
                        
                        st.markdown("**üì∫ Gi·ªçng qu·∫£ng c√°o:**")
                        if st.button("S·ª≠ d·ª•ng", key="preset_ads"):
                            st.session_state.tts_instructions_preset = "Tone: S√¥i n·ªïi, cu·ªën h√∫t, nƒÉng ƒë·ªông\nEmotion: Ph·∫•n kh√≠ch, h√†o h·ª©ng\nDelivery: Nhanh, ƒë·∫ßy nƒÉng l∆∞·ª£ng, v·ªõi c∆∞·ªùng ƒë·ªô tƒÉng d·∫ßn"
                            st.rerun()
            
            # Action buttons
            col_preview, col_download = st.columns(2)
            
            with col_preview:
                preview_button = st.button(
                    "üîä Nghe th·ª≠ gi·ªçng n√≥i",
                    use_container_width=True,
                    disabled=not tts_text.strip()
                )
            
            with col_download:
                generate_button = st.button(
                    "üíæ T·∫°o v√† t·∫£i xu·ªëng",
                    use_container_width=True,
                    type="primary",
                    disabled=not tts_text.strip()
                )
        
        with tts_col2:
            st.subheader("üéµ K·∫øt qu·∫£")
            
            # Preview area
            preview_message = st.empty()
            preview_audio = st.empty()
            
            # Handle preview button
            if preview_button and tts_text.strip():
                preview_message.info("‚è≥ ƒêang t·∫°o m·∫´u gi·ªçng n√≥i...")
                
                if tts_service_standalone == "GPT-4o-mini-TTS" and openai_client:
                    try:
                        # Use asyncio for GPT-4o preview
                        audio_bytes = asyncio.run(preview_audio_tts(
                            tts_text[:200],  # Limit preview length
                            tts_instructions_standalone,
                            tts_voice_standalone,
                            preview_message
                        ))
                        
                        if audio_bytes:
                            preview_message.success("‚úÖ T·∫°o m·∫´u gi·ªçng n√≥i th√†nh c√¥ng!")
                            preview_audio.audio(audio_bytes, format="audio/mp3")
                    except Exception as e:
                        preview_message.error(f"L·ªói: {str(e)}")
                        
                else:
                    try:
                        # Use VideoEditor for other services
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp:
                            temp_path = temp.name
                        
                        tts_service = "edge" if tts_service_standalone == "Edge TTS" else "openai"
                        success, error = editor.generate_audio_from_text(
                            tts_text[:200],  # Limit preview length
                            temp_path,
                            service=tts_service,
                            voice=tts_voice_standalone,
                            speed=tts_speed_standalone
                        )
                        
                        if success and os.path.exists(temp_path):
                            preview_message.success("‚úÖ T·∫°o m·∫´u gi·ªçng n√≥i th√†nh c√¥ng!")
                            with open(temp_path, "rb") as f:
                                audio_bytes = f.read()
                            preview_audio.audio(audio_bytes, format="audio/mp3")
                            os.unlink(temp_path)
                        else:
                            preview_message.error(f"L·ªói: {error}")
                            
                    except Exception as e:
                        preview_message.error(f"L·ªói: {str(e)}")
            
            # Handle generate and download
            if generate_button and tts_text.strip():
                with st.spinner("‚è≥ ƒêang t·∫°o file audio..."):
                    try:
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        
                        if tts_service_standalone == "GPT-4o-mini-TTS" and openai_client:
                            # Generate with GPT-4o-mini-TTS
                            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp:
                                temp_path = temp.name
                            
                            success = asyncio.run(generate_gpt4o_tts(
                                tts_text,
                                temp_path,
                                tts_instructions_standalone,
                                tts_voice_standalone
                            ))
                            
                            if success and os.path.exists(temp_path):
                                with open(temp_path, "rb") as f:
                                    audio_bytes = f.read()
                                
                                st.download_button(
                                    "üíæ T·∫£i xu·ªëng audio",
                                    audio_bytes,
                                    file_name=f"tts_gpt4o_{timestamp}.mp3",
                                    mime="audio/mp3",
                                    use_container_width=True
                                )
                                
                                st.audio(audio_bytes, format="audio/mp3")
                                os.unlink(temp_path)
                            else:
                                st.error("L·ªói khi t·∫°o audio v·ªõi GPT-4o-mini-TTS")
                                
                        else:
                            # Generate with other services
                            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp:
                                temp_path = temp.name
                            
                            tts_service = "edge" if tts_service_standalone == "Edge TTS" else "openai"
                            success, error = editor.generate_audio_from_text(
                                tts_text,
                                temp_path,
                                service=tts_service,
                                voice=tts_voice_standalone,
                                speed=tts_speed_standalone
                            )
                            
                            if success and os.path.exists(temp_path):
                                with open(temp_path, "rb") as f:
                                    audio_bytes = f.read()
                                
                                st.download_button(
                                    "üíæ T·∫£i xu·ªëng audio",
                                    audio_bytes,
                                    file_name=f"tts_{tts_service}_{timestamp}.mp3",
                                    mime="audio/mp3",
                                    use_container_width=True
                                )
                                
                                st.audio(audio_bytes, format="audio/mp3")
                                os.unlink(temp_path)
                            else:
                                st.error(f"L·ªói: {error}")
                                
                    except Exception as e:
                        st.error(f"L·ªói khi t·∫°o audio: {str(e)}")
            
            # TTS Info
            st.markdown("---")
            st.markdown("### üìä Th√¥ng tin TTS")
            if tts_service_standalone == "GPT-4o-mini-TTS":
                st.info("üé≠ **GPT-4o-mini-TTS**: Gi·ªçng n√≥i bi·ªÉu c·∫£m v·ªõi AI")
                st.markdown("""
                **T√≠nh nƒÉng:**
                - ‚úÖ 10 gi·ªçng n√≥i ƒëa d·∫°ng
                - ‚úÖ T√πy ch·ªânh gi·ªçng ƒëi·ªáu
                - ‚úÖ Bi·ªÉu c·∫£m t·ª± nhi√™n
                - ‚úÖ Ch·∫•t l∆∞·ª£ng cao
                """)
            elif tts_service_standalone == "Edge TTS":
                st.info("üé§ **Edge TTS**: Mi·ªÖn ph√≠, ch·∫•t l∆∞·ª£ng t·ªët")
                st.markdown("""
                **T√≠nh nƒÉng:**
                - ‚úÖ Mi·ªÖn ph√≠ ho√†n to√†n
                - ‚úÖ H·ªó tr·ª£ ti·∫øng Vi·ªát
                - ‚úÖ ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô
                - ‚úÖ Ch·∫•t l∆∞·ª£ng ·ªïn ƒë·ªãnh
                """)
            else:
                st.info("ü§ñ **OpenAI TTS**: Ch·∫•t l∆∞·ª£ng cao, ƒëa ng√¥n ng·ªØ")
                st.markdown("""
                **T√≠nh nƒÉng:**
                - ‚úÖ Ch·∫•t l∆∞·ª£ng premium
                - ‚úÖ 6 gi·ªçng n√≥i kh√°c nhau
                - ‚úÖ H·ªó tr·ª£ nhi·ªÅu ng√¥n ng·ªØ
                - ‚úÖ ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô
                """)
    
    # === Tab 3: L·ªãch s·ª≠ ===
    with tabs[3]:
        st.subheader("üìã L·ªãch s·ª≠ video ƒë√£ t·∫°o")
        
        # C·∫≠p nh·∫≠t l·ªãch s·ª≠ t·ª´ th∆∞ m·ª•c
        update_history_from_folder()
        
        if not st.session_state.history:
            st.info("üìù Ch∆∞a c√≥ video n√†o ƒë∆∞·ª£c t·∫°o.")
        else:
            # T√πy ch·ªçn s·∫Øp x·∫øp v√† t√¨m ki·∫øm
            col1, col2 = st.columns(2)
            with col1:
                sort_option = st.selectbox(
                    "S·∫Øp x·∫øp theo:",
                    ["Th·ªùi gian t·∫°o (m·ªõi nh·∫•t)", "Th·ªùi gian t·∫°o (c≈© nh·∫•t)", "K√≠ch th∆∞·ªõc (l·ªõn nh·∫•t)", "K√≠ch th∆∞·ªõc (nh·ªè nh·∫•t)"],
                    index=0
                )
            
            with col2:
                search_term = st.text_input("T√¨m ki·∫øm:", placeholder="Nh·∫≠p t√™n file...")
            
            # S·∫Øp x·∫øp v√† l·ªçc l·ªãch s·ª≠
            history = sorted(
                st.session_state.history,
                key=lambda x: x.get('created', datetime.now()) if sort_option.startswith("Th·ªùi gian") else x.get('size', 0),
                reverse=sort_option in ["Th·ªùi gian t·∫°o (m·ªõi nh·∫•t)", "K√≠ch th∆∞·ªõc (l·ªõn nh·∫•t)"]
            )
            
            # L·ªçc theo t·ª´ kh√≥a
            if search_term:
                history = [item for item in history if search_term.lower() in Path(item.get('path', '')).name.lower()]
            
            # Hi·ªÉn th·ªã danh s√°ch
            for i, item in enumerate(history):
                file_path = Path(item['path'])
                if not file_path.exists():
                    continue
                
                with st.expander(f"{file_path.name} ({item['created'].strftime('%Y-%m-%d %H:%M:%S')})", expanded=i==0):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.video(str(file_path))
                    
                    with col2:
                        st.write(f"**K√≠ch th∆∞·ªõc:** {item['size']:.1f} MB")
                        st.write(f"**Th·ªùi gian t·∫°o:** {item['created'].strftime('%Y-%m-%d %H:%M:%S')}")
                        
                        buttons_col1, buttons_col2 = st.columns(2)
                        with buttons_col1:
                            with open(file_path, "rb") as f:
                                st.download_button(
                                    "üíæ T·∫£i xu·ªëng",
                                    f,
                                    file_name=file_path.name,
                                    mime="video/mp4",
                                    key=f"download_{file_path.name}",
                                    use_container_width=True
                                )
                        
                        with buttons_col2:
                            if st.button("üóëÔ∏è X√≥a video", key=f"delete_{file_path.name}", use_container_width=True):
                                try:
                                    file_path.unlink()
                                    st.session_state.history = [h for h in st.session_state.history if h['path'] != str(file_path)]
                                    st.success(f"ƒê√£ x√≥a {file_path.name}")
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"Kh√¥ng th·ªÉ x√≥a file: {str(e)}")
    
    # === Tab 4: C√†i ƒë·∫∑t ===
    with tabs[4]:
        st.subheader("‚öôÔ∏è C√†i ƒë·∫∑t API Keys")
        st.write("C·∫•u h√¨nh c√°c API keys c·∫ßn thi·∫øt cho c√°c t√≠nh nƒÉng n√¢ng cao")
        
        # Kh·ªüi t·∫°o OpenAI client n·∫øu ch∆∞a c√≥
        initialize_openai_client()
        
        # OpenAI API Key Section
        with st.expander("ü§ñ OpenAI API Key", expanded=True):
            st.markdown("""
            **OpenAI API Key** ƒë∆∞·ª£c s·ª≠ d·ª•ng cho:
            - üé≠ GPT-4o-mini-TTS (Text-to-Speech ch·∫•t l∆∞·ª£ng cao)
            - üé§ OpenAI TTS (Text-to-Speech chuy√™n nghi·ªáp)
            
            üí° **C√°ch l·∫•y API Key:**
            1. Truy c·∫≠p [OpenAI Platform](https://platform.openai.com/account/api-keys)
            2. ƒêƒÉng nh·∫≠p v√† t·∫°o API key m·ªõi
            3. Copy v√† paste v√†o √¥ b√™n d∆∞·ªõi
            """)
            
            # Hi·ªÉn th·ªã th√¥ng tin ngu·ªìn API key
            if st.session_state.get('api_key_source') == 'environment':
                st.info("‚ÑπÔ∏è **API Key ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫£i t·ª´ c·∫•u h√¨nh Notebook** - Kh√¥ng c·∫ßn nh·∫≠p l·∫°i!")
            
            # API Key input - disable n·∫øu ƒë√£ c√≥ t·ª´ environment
            api_key_input = st.text_input(
                "OpenAI API Key:",
                value=st.session_state.openai_api_key if st.session_state.get('api_key_source') != 'environment' else "‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢",
                type="password",
                placeholder="sk-proj-..." if st.session_state.get('api_key_source') != 'environment' else "ƒê√£ c·∫•u h√¨nh t·ª´ Notebook",
                help="API key ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫£i t·ª´ cell c·∫•u h√¨nh trong Notebook" if st.session_state.get('api_key_source') == 'environment' else "Nh·∫≠p OpenAI API key ƒë·ªÉ s·ª≠ d·ª•ng GPT-4o-mini-TTS v√† OpenAI TTS",
                disabled=st.session_state.get('api_key_source') == 'environment'
            )
            
            # Buttons row - ch·ªâ hi·ªÉn th·ªã khi kh√¥ng ph·∫£i t·ª´ environment
            if st.session_state.get('api_key_source') != 'environment':
                col1, col2, col3 = st.columns([1, 1, 2])
                
                with col1:
                    save_button = st.button("üíæ L∆∞u", use_container_width=True)
                
                with col2:
                    test_button = st.button("üß™ Ki·ªÉm tra", use_container_width=True, disabled=not (api_key_input and api_key_input.strip()))
                
                with col3:
                    clear_button = st.button("üóëÔ∏è X√≥a", use_container_width=True)
                
                # Handle buttons
                if save_button and api_key_input and api_key_input.strip():
                    st.session_state.openai_api_key = api_key_input.strip()
                    st.session_state.openai_api_status = 'not_tested'
                    st.session_state.api_key_source = 'manual'
                    initialize_openai_client()
                    st.success("‚úÖ ƒê√£ l∆∞u OpenAI API key!")
                    st.rerun()
                
                if test_button and api_key_input and api_key_input.strip():
                    st.session_state.openai_api_status = 'testing'
                    with st.spinner("üß™ ƒêang ki·ªÉm tra API key..."):
                        try:
                            is_valid, message = asyncio.run(test_openai_api_key(api_key_input.strip()))
                            if is_valid:
                                st.session_state.openai_api_status = 'valid'
                                st.success(f"‚úÖ {message}")
                                st.session_state.openai_api_key = api_key_input.strip()
                                st.session_state.api_key_source = 'manual'
                                initialize_openai_client()
                            else:
                                st.session_state.openai_api_status = 'invalid'
                                st.error(f"‚ùå {message}")
                        except Exception as e:
                            st.session_state.openai_api_status = 'invalid'
                            st.error(f"‚ùå L·ªói ki·ªÉm tra: {str(e)}")
                    st.rerun()
                
                if clear_button:
                    st.session_state.openai_api_key = ''
                    st.session_state.openai_api_status = 'not_tested'
                    st.session_state.api_key_source = 'manual'
                    initialize_openai_client()
                    st.info("üóëÔ∏è ƒê√£ x√≥a API key")
                    st.rerun()
            else:
                # Ch·ªâ hi·ªÉn th·ªã n√∫t test connection cho API key t·ª´ environment
                if st.button("üîó Test k·∫øt n·ªëi v·ªõi API key t·ª´ Notebook", use_container_width=True):
                    with st.spinner("üß™ ƒêang ki·ªÉm tra API key t·ª´ Notebook..."):
                        try:
                            is_valid, message = asyncio.run(test_openai_api_key(st.session_state.openai_api_key))
                            if is_valid:
                                st.session_state.openai_api_status = 'valid'
                                st.success(f"‚úÖ {message}")
                                initialize_openai_client()
                            else:
                                st.session_state.openai_api_status = 'invalid'
                                st.error(f"‚ùå {message}")
                        except Exception as e:
                            st.session_state.openai_api_status = 'invalid'
                            st.error(f"‚ùå L·ªói ki·ªÉm tra: {str(e)}")
                    st.rerun()
            
            # Status indicator
            if st.session_state.openai_api_key:
                status = st.session_state.openai_api_status
                api_source = st.session_state.get('api_key_source', 'manual')
                
                if status == 'valid':
                    source_text = "t·ª´ Notebook" if api_source == 'environment' else "th·ªß c√¥ng"
                    st.success(f"‚úÖ API key ƒë√£ ƒë∆∞·ª£c x√°c th·ª±c v√† ƒëang ho·∫°t ƒë·ªông (ngu·ªìn: {source_text})")
                elif status == 'invalid':
                    st.error("‚ùå API key kh√¥ng h·ª£p l·ªá ho·∫∑c c√≥ l·ªói")
                elif status == 'testing':
                    st.info("üß™ ƒêang ki·ªÉm tra API key...")
                else:
                    if api_source == 'environment':
                        st.info("‚ÑπÔ∏è API key t·ª´ Notebook - Nh·∫•n 'Test k·∫øt n·ªëi' ƒë·ªÉ x√°c th·ª±c.")
                    else:
                        st.warning("‚ö†Ô∏è API key ch∆∞a ƒë∆∞·ª£c ki·ªÉm tra. Nh·∫•n 'Ki·ªÉm tra' ƒë·ªÉ x√°c th·ª±c.")
            else:
                st.info("‚ÑπÔ∏è Ch∆∞a c√≥ API key. M·ªôt s·ªë t√≠nh nƒÉng s·∫Ω kh√¥ng kh·∫£ d·ª•ng.")
        
        # Th√¥ng tin API Usage
        if openai_client:
            with st.expander("üìä Th√¥ng tin s·ª≠ d·ª•ng API", expanded=False):
                st.markdown("""
                **L∆∞u √Ω v·ªÅ chi ph√≠:**
                - GPT-4o-mini-TTS: ~$0.150 / 1M k√Ω t·ª±
                - OpenAI TTS: ~$15.00 / 1M k√Ω t·ª±
                - M·ªôt ƒëo·∫°n vƒÉn 1000 t·ª´ ‚âà 5000 k√Ω t·ª±
                
                **Khuy·∫øn ngh·ªã:**
                - S·ª≠ d·ª•ng Edge TTS (mi·ªÖn ph√≠) cho m·ª•c ƒë√≠ch th·ª≠ nghi·ªám
                - GPT-4o-mini-TTS cho ch·∫•t l∆∞·ª£ng cao v·ªõi chi ph√≠ h·ª£p l√Ω
                - OpenAI TTS cho ch·∫•t l∆∞·ª£ng premium
                """)
                
                # Test connection button
                if st.button("üîó Test k·∫øt n·ªëi", key="test_connection"):
                    with st.spinner("ƒêang test k·∫øt n·ªëi..."):
                        try:
                            is_valid, message = asyncio.run(test_openai_api_key(st.session_state.openai_api_key))
                            if is_valid:
                                st.success(f"‚úÖ {message}")
                            else:
                                st.error(f"‚ùå {message}")
                        except Exception as e:
                            st.error(f"‚ùå L·ªói: {str(e)}")
        
        # API Keys Security
        with st.expander("üîí B·∫£o m·∫≠t API Keys", expanded=False):
            st.markdown("""
            **‚ö†Ô∏è L∆∞u √Ω b·∫£o m·∫≠t quan tr·ªçng:**
            
            1. **Kh√¥ng chia s·∫ª API keys** v·ªõi ng∆∞·ªùi kh√°c
            2. **X√≥a API keys** khi kh√¥ng s·ª≠ d·ª•ng n·ªØa
            3. **Gi√°m s√°t usage** th∆∞·ªùng xuy√™n tr√™n OpenAI Platform
            4. **ƒê·∫∑t limits** cho API usage ƒë·ªÉ tr√°nh chi ph√≠ b·∫•t ng·ªù
            
            **üõ°Ô∏è API keys ƒë∆∞·ª£c l∆∞u trong:**
            - Session memory c·ªßa Streamlit (t·∫°m th·ªùi)
            - Kh√¥ng ƒë∆∞·ª£c l∆∞u v√†o file ho·∫∑c database
            - S·∫Ω b·ªã x√≥a khi t·∫Øt ·ª©ng d·ª•ng
            """)
    
    # === Tab 5: H∆∞·ªõng d·∫´n ===
    with tabs[5]:
        st.subheader("‚ùì H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng")
        
        with st.expander("üöÄ B·∫Øt ƒë·∫ßu nhanh", expanded=True):
            st.markdown("""
            ### üé¨ T·∫°o Video MC:
            1. **üìÅ T·∫£i l√™n file MC**: ·∫¢nh ho·∫∑c video c√≥ nh√¢n v·∫≠t n√≥i
            2. **üéûÔ∏è T·∫£i l√™n video n·ªÅn**: Video n·ªÅn cho MC
            3. **üéµ Ch·ªçn ngu·ªìn audio**: T·∫£i l√™n file audio ho·∫∑c t·∫°o t·ª´ vƒÉn b·∫£n
            4. **‚öôÔ∏è ƒêi·ªÅu ch·ªânh c√†i ƒë·∫∑t**: V·ªã tr√≠ v√† k√≠ch th∆∞·ªõc MC
            5. **üöÄ Nh·∫•n "T·∫°o Video MC"**: Ch·ªù x·ª≠ l√Ω v√† t·∫£i xu·ªëng k·∫øt qu·∫£
            
            ### üé≠ T·∫°o Video Khu√¥n M·∫∑t AI:
            1. **üìÅ T·∫£i l√™n file MC**: ·∫¢nh ho·∫∑c video khu√¥n m·∫∑t
            2. **üéµ Ch·ªçn ngu·ªìn audio**: Audio ho·∫∑c vƒÉn b·∫£n
            3. **ü§ñ Ch·ªçn m√¥ h√¨nh AI**: M·∫∑c ƒë·ªãnh ho·∫∑c t·ªëi ∆∞u h√≥a
            4. **üó£Ô∏è T√πy ch·ªânh kh·∫©u h√¨nh**: ƒêi·ªÅu ch·ªânh chuy·ªÉn ƒë·ªông m√¥i
            5. **üöÄ Nh·∫•n "T·∫°o Video Khu√¥n M·∫∑t N√≥i"**: Ch·ªù k·∫øt qu·∫£
            """)
        
        with st.expander("üéôÔ∏è Text-to-Speech", expanded=False):
            st.markdown("""
            ### D·ªãch v·ª• TTS c√≥ s·∫µn:
            
            **ü§ñ GPT-4o-mini-TTS** (Khuy·∫øn ngh·ªã):
            - 10 gi·ªçng n√≥i ƒëa d·∫°ng
            - T√πy ch·ªânh gi·ªçng ƒëi·ªáu chi ti·∫øt
            - Bi·ªÉu c·∫£m t·ª± nhi√™n
            
            **üé§ Edge TTS** (Mi·ªÖn ph√≠):
            - H·ªó tr·ª£ ti·∫øng Vi·ªát t·ªët
            - Kh√¥ng c·∫ßn API key
            - Ch·∫•t l∆∞·ª£ng ·ªïn ƒë·ªãnh
            
            **üîä OpenAI TTS** (Premium):
            - Ch·∫•t l∆∞·ª£ng cao
            - 6 gi·ªçng n√≥i kh√°c nhau
            - C·∫ßn API key
            """)
        
        with st.expander("‚öôÔ∏è T√≠nh nƒÉng n√¢ng cao", expanded=False):
            st.markdown("""
            ### üó£Ô∏è ƒêi·ªÅu khi·ªÉn kh·∫©u h√¨nh:
            - **M·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i**: ƒêi·ªÅu ch·ªânh ƒë·ªô m·∫°nh c·ªßa chuy·ªÉn ƒë·ªông m√¥i
            - **Th√†nh ph·∫ßn bi·ªÉu c·∫£m**: Ch·ªçn c√°c ph·∫ßn c·ªßa khu√¥n m·∫∑t ƒë·ªÉ animate
            - **T·ª∑ l·ªá bi·ªÉu c·∫£m**: ƒêi·ªÅu ch·ªânh c∆∞·ªùng ƒë·ªô bi·ªÉu c·∫£m
            - **Offset bi·ªÉu c·∫£m**: Thay ƒë·ªïi h√¨nh d√°ng m√¥i m·∫∑c ƒë·ªãnh
            
            ### üéõÔ∏è C√†i ƒë·∫∑t video:
            - **V·ªã tr√≠ MC**: 5 v·ªã tr√≠ kh√°c nhau tr√™n video
            - **T·ª± ƒë·ªông scale**: T√≠nh to√°n k√≠ch th∆∞·ªõc ph√π h·ª£p
            - **Ch·∫•t l∆∞·ª£ng**: Low/Medium/High
            - **M√¥ h√¨nh AI**: M·∫∑c ƒë·ªãnh ho·∫∑c t·ªëi ∆∞u h√≥a
            
            ### üîÑ Quy tr√¨nh x·ª≠ l√Ω:
            - B·∫≠t/t·∫Øt t·ª´ng b∆∞·ªõc x·ª≠ l√Ω
            - T√πy ch·ªânh workflow theo nhu c·∫ßu
            - Theo d√µi ti·∫øn tr√¨nh real-time
            """)
        
        with st.expander("üîß X·ª≠ l√Ω s·ª± c·ªë", expanded=False):
            st.markdown("""
            ### ‚ùå V·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p:
            
            **üêå X·ª≠ l√Ω ch·∫≠m:**
            - Gi·∫£m ch·∫•t l∆∞·ª£ng xu·ªëng "Low"
            - S·ª≠ d·ª•ng file input nh·ªè h∆°n
            - Ki·ªÉm tra GPU memory
            
            **‚ùå L·ªói khi t·∫°o video:**
            - Ki·ªÉm tra ƒë·ªãnh d·∫°ng file ƒë√∫ng
            - ƒê·∫£m b·∫£o files kh√¥ng b·ªã l·ªói
            - Th·ª≠ restart runtime
            
            **‚ùå L·ªói TTS:**
            - Ki·ªÉm tra API keys (n·∫øu d√πng OpenAI)
            - Th·ª≠ d·ªãch v·ª• Edge TTS
            - Ki·ªÉm tra k·∫øt n·ªëi internet
            
            **üîÑ App b·ªã ƒë∆°:**
            - Nh·∫•n n√∫t "H·ªßy x·ª≠ l√Ω"
            - Restart Streamlit
            - Ki·ªÉm tra logs ƒë·ªÉ debug
            """)
        
        with st.expander("üí° M·∫πo t·ªëi ∆∞u", expanded=False):
            st.markdown("""
            ### üì∏ Chu·∫©n b·ªã file MC:
            - Khu√¥n m·∫∑t r√µ r√†ng, nh√¨n th·∫≥ng
            - N·ªÅn ƒë·ªìng m√†u ho·∫∑c trong su·ªët
            - √Ånh s√°ng ƒë·ªÅu, kh√¥ng b·ªã t·ªëi
            - ƒê·ªô ph√¢n gi·∫£i √≠t nh·∫•t 512x512
            
            ### üéûÔ∏è Video n·ªÅn:
            - ƒê·ªãnh d·∫°ng MP4, t·ª∑ l·ªá 16:9
            - ƒê·ªô ph√¢n gi·∫£i HD (1280x720) tr·ªü l√™n
            - Th·ªùi l∆∞·ª£ng ph√π h·ª£p v·ªõi audio
            - N·ªôi dung ph√π h·ª£p v·ªõi ch·ªß ƒë·ªÅ
            
            ### üéµ Audio ch·∫•t l∆∞·ª£ng:
            - File WAV ho·∫∑c MP3
            - Kh√¥ng c√≥ ti·∫øng ·ªìn n·ªÅn
            - Gi·ªçng n√≥i r√µ r√†ng
            - T·ªëc ƒë·ªô n√≥i v·ª´a ph·∫£i
            
            ### ‚ö° T·ªëi ∆∞u hi·ªáu su·∫•t:
            - S·ª≠ d·ª•ng GPU T4 ho·∫∑c cao h∆°n
            - ƒê√≥ng c√°c tab kh√¥ng c·∫ßn thi·∫øt
            - Ki·ªÉm tra RAM c√≤n tr·ªëng
            - S·ª≠ d·ª•ng ch·∫•t l∆∞·ª£ng Medium cho c√¢n b·∫±ng
            """)

if __name__ == "__main__":
    main()

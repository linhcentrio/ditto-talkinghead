#!/usr/bin/env python3
"""Streamlit UI t·ªëi ∆∞u cho vi·ªác t·∫°o video MC v·ªõi n·ªÅn v√† audio tho·∫°i - Phi√™n b·∫£n t·ªëi ∆∞u"""

import streamlit as st
import subprocess
import tempfile
import shutil
import os
import queue
import threading
import time
import re
import traceback
import asyncio
import sys
import numpy as np
import pickle
import json
import librosa
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Union, Any
import cv2
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
from contextlib import contextmanager
from video_editor import VideoEditor

# Nh·∫≠n API keys t·ª´ m√¥i tr∆∞·ªùng
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PEXELS_API_KEY = os.getenv("PEXELS_API_KEY")

if OPENAI_API_KEY is None or PEXELS_API_KEY is None:
    raise ValueError("OPENAI_API_KEY v√† PEXELS_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")

# T·∫Øt ch·∫ø ƒë·ªô theo d√µi file c·ªßa Streamlit ƒë·ªÉ tr√°nh l·ªói segmentation fault
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'

# Th√™m th∆∞ vi·ªán OpenAI m·ªõi
from openai import AsyncOpenAI
from openai.helpers import LocalAudioPlayer

# === ƒê·ªãnh nghƒ©a c√°c b∆∞·ªõc quy tr√¨nh ===
WORKFLOW_STEPS = {
    "prepare_files": "Chu·∫©n b·ªã files",
    "tts_generation": "T·∫°o √¢m thanh t·ª´ vƒÉn b·∫£n",
    "subtitle_generation": "T·∫°o ph·ª• ƒë·ªÅ t·ª´ audio",
    "talking_head_generation": "T·∫°o video khu√¥n m·∫∑t n√≥i",
    "video_overlay": "Gh√©p video MC v√† n·ªÅn",
    "caption_application": "Th√™m ph·ª• ƒë·ªÅ v√†o video"
}

# === Kh·ªüi t·∫°o OpenAI API client ===
openai_client = AsyncOpenAI()

# === ƒê·ªãnh nghƒ©a th√¥ng tin m√¥ t·∫£ gi·ªçng n√≥i ===
VOICE_DESCRIPTIONS = {
    "Ash": "Gi·ªçng nam tr∆∞·ªüng th√†nh, h∆°i tr·∫ßm, ph√π h·ª£p cho phim t√†i li·ªáu",
    "Ballad": "Gi·ªçng n·ªØ m·ªÅm m·∫°i, ·∫•m √°p, ph√π h·ª£p cho n·ªôi dung t∆∞ v·∫•n",
    "Coral": "Gi·ªçng n·ªØ tr·∫ª, r√µ r√†ng, t·ª± tin, ph√π h·ª£p cho n·ªôi dung gi√°o d·ª•c",
    "Echo": "Gi·ªçng nam tr·∫ª, nƒÉng ƒë·ªông, ph√π h·ª£p cho qu·∫£ng c√°o",
    "Fable": "Gi·ªçng nam uy t√≠n, ph√π h·ª£p cho th√¥ng b√°o ch√≠nh th·ª©c",
    "Onyx": "Gi·ªçng nam tr·∫ßm, sang tr·ªçng, ph√π h·ª£p cho thuy·∫øt tr√¨nh",
    "Nova": "Gi·ªçng n·ªØ chuy√™n nghi·ªáp, ph√π h·ª£p cho tin t·ª©c",
    "Sage": "Gi·ªçng n·ªØ t·ª´ng tr·∫£i, ·∫•m √°p, ph√π h·ª£p cho podcast",
    "Shimmer": "Gi·ªçng n·ªØ t∆∞∆°i s√°ng, nƒÉng ƒë·ªông, ph√π h·ª£p cho gi·∫£i tr√≠",
    "Verse": "Gi·ªçng nam t·ª± nhi√™n, c√¢n b·∫±ng, ph√π h·ª£p cho ƒëa d·∫°ng n·ªôi dung"
}

# === H√†m x√°c th·ª±c tham s·ªë kh·∫©u h√¨nh ===
def validate_mouth_params(vad_alpha=1.0, exp_components=None, exp_scale=1.0, 
                          pose_scale=1.0, delta_exp_enabled=False, delta_exp_value=0.0):
    """X√°c th·ª±c c√°c tham s·ªë kh·∫©u h√¨nh ƒë·ªÉ ƒë·∫£m b·∫£o ch√∫ng trong ph·∫°m vi an to√†n"""
    validated = {}
    
    # X√°c th·ª±c vad_alpha (gi·ªØ trong kho·∫£ng 0.0-1.0)
    validated['vad_alpha'] = max(0.0, min(1.0, float(vad_alpha)))
    
    # X√°c th·ª±c exp_components (ƒë·∫£m b·∫£o l√† list h·ª£p l·ªá)
    if exp_components and isinstance(exp_components, list):
        validated['exp_components'] = [str(comp) for comp in exp_components if comp in ["exp", "pitch", "yaw", "roll", "t"]]
    else:
        validated['exp_components'] = None
    
    # X√°c th·ª±c exp_scale v√† pose_scale (gi·ªØ trong kho·∫£ng 0.5-1.5)
    validated['exp_scale'] = max(0.5, min(1.5, float(exp_scale)))
    validated['pose_scale'] = max(0.5, min(1.5, float(pose_scale)))
    
    # X√°c th·ª±c delta_exp_enabled v√† delta_exp_value
    validated['delta_exp_enabled'] = bool(delta_exp_enabled)
    validated['delta_exp_value'] = max(-0.2, min(0.2, float(delta_exp_value)))
    
    return validated

# === Kh·ªüi t·∫°o session state ===
def init_session_state():
    """Kh·ªüi t·∫°o to√†n b·ªô session state c·∫ßn thi·∫øt"""
    defaults = {
        'processing': False,
        'complete': False,
        'output_file': None,
        'history': [],
        'process_start_time': None,
        'auto_scale': True,
        'auto_fontsize': True,
        'logs': [],
    }

    # Kh·ªüi t·∫°o workflow_steps ri√™ng ƒë·ªÉ ƒë·∫£m b·∫£o n√≥ ƒë∆∞·ª£c t·∫°o ƒë√∫ng
    if 'workflow_steps' not in st.session_state:
        st.session_state['workflow_steps'] = {k: True for k in WORKFLOW_STEPS}

    # Kh·ªüi t·∫°o c√°c bi·∫øn kh√°c
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

# === C√°c h√†m ti·ªán √≠ch ===
@lru_cache(maxsize=32)
def get_video_resolution(video_path: str) -> Tuple[int, int]:
    """L·∫•y ƒë·ªô ph√¢n gi·∫£i c·ªßa video v·ªõi cache"""
    try:
        with contextmanager(lambda: cv2.VideoCapture(str(video_path)))() as cap:
            return (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 1920, int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 1080) if cap.isOpened() else (1920, 1080)
    except Exception:
        return 1920, 1080

def calculate_auto_fontsize(video_width: int, video_height: int) -> int:
    """T√≠nh to√°n font size ph√π h·ª£p d·ª±a tr√™n ƒë·ªô ph√¢n gi·∫£i video"""
    return max(24, min(min(video_width * 24 // 1280, video_height * 24 // 720), 72))

def calculate_auto_scale(mc_path: Union[str, Any], bg_width: int, bg_height: int) -> float:
    """T√≠nh to√°n t·ªâ l·ªá scale ph√π h·ª£p cho MC"""
    try:
        mc_width, mc_height = 0, 0

        # X·ª≠ l√Ω c√°c lo·∫°i ƒë·∫ßu v√†o kh√°c nhau v·ªõi walrus operator
        if hasattr(mc_path, 'getbuffer'):  # UploadedFile
            suffix = Path(mc_path.name).suffix.lower()
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp:
                temp.write(mc_path.getbuffer())
                temp_path = temp.name

            try:
                match suffix:
                    case ext if ext in ['.jpg', '.jpeg', '.png']:
                        if img := cv2.imread(temp_path):
                            mc_width, mc_height = img.shape[1], img.shape[0]
                    case _:  # X·ª≠ l√Ω video
                        if cap := cv2.VideoCapture(temp_path):
                            if cap.isOpened():
                                mc_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                                mc_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                            cap.release()
            finally:
                os.unlink(temp_path)
        else:  # Path string
            mc_path_str = str(mc_path)
            suffix = Path(mc_path_str).suffix.lower()

            match suffix:
                case ext if ext in ['.jpg', '.jpeg', '.png']:
                    if img := cv2.imread(mc_path_str):
                        mc_width, mc_height = img.shape[1], img.shape[0]
                case _:  # X·ª≠ l√Ω video
                    if cap := cv2.VideoCapture(mc_path_str):
                        if cap.isOpened():
                            mc_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            mc_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        cap.release()

        if not mc_width or not mc_height:
            return 0.25

        # T√≠nh to√°n t·ªâ l·ªá t·ªëi ∆∞u
        width_scale = bg_width / mc_width / 3
        height_scale = bg_height / mc_height / 1.5

        return min(round(min(width_scale, height_scale), 2), 0.5)
    except Exception:
        return 0.25

def update_history_from_folder():
    """C·∫≠p nh·∫≠t l·ªãch s·ª≠ t·ª´ th∆∞ m·ª•c output s·ª≠ d·ª•ng pathlib v√† comprehension"""
    if not (output_folder := Path("./output")).exists():
        return

    # L·∫•y danh s√°ch ƒë∆∞·ªùng d·∫´n hi·ªán c√≥
    existing_paths = {item.get('path', '') for item in st.session_state.history}

    # T√¨m c√°c file m·ªõi
    new_files = [
        {
            'path': str(file),
            'created': datetime.fromtimestamp(file.stat().st_mtime),
            'size': file.stat().st_size / (1024*1024)
        }
        for file in output_folder.glob("final_mc_*.mp4")
        if file.exists() and str(file) not in existing_paths
    ]

    # Th√™m v√†o l·ªãch s·ª≠ n·∫øu c√≥ file m·ªõi
    if new_files:
        st.session_state.history.extend(new_files)

def create_empty_srt(srt_path: str):
    """T·∫°o file SRT tr·ªëng khi b·ªè qua b∆∞·ªõc t·∫°o ph·ª• ƒë·ªÅ"""
    with open(srt_path, 'w', encoding='utf-8') as f:
        f.write("""1
00:00:00,000 --> 00:00:05,000
Video MC Creator

2
00:00:05,000 --> 00:00:10,000
T·∫°o video MC v·ªõi n·ªÅn v√† audio tho·∫°i """)

def use_sample_audio(audio_path: str) -> str:
    """S·ª≠ d·ª•ng audio m·∫´u khi b·ªè qua b∆∞·ªõc t·∫°o audio"""
    # T√¨m file audio m·∫´u
    if sample_paths := list(Path("./example").glob("*.wav")) + list(Path("./example").glob("*.mp3")):
        shutil.copy(str(sample_paths[0]), str(audio_path))
        return str(sample_paths[0])

    # T·∫°o audio im l·∫∑ng 5 gi√¢y n·∫øu kh√¥ng c√≥ m·∫´u
    subprocess.run([
        "ffmpeg", "-y", "-f", "lavfi", "-i", "anullsrc=r=44100:cl=mono",
        "-t", "5", "-q:a", "0", "-map", "0", str(audio_path)
    ], capture_output=True)
    return str(audio_path)

# === H√†m t·∫°o audio b·∫±ng GPT-4o-mini-TTS ===
async def generate_gpt4o_tts(text: str, output_path: str, instructions: str, voice: str = "shimmer") -> bool:
    """T·∫°o audio t·ª´ vƒÉn b·∫£n b·∫±ng GPT-4o-mini-TTS v·ªõi h∆∞·ªõng d·∫´n v·ªÅ gi·ªçng ƒëi·ªáu"""
    try:
        # T·∫°o file PCM t·∫°m
        temp_pcm = output_path + ".pcm"

        # T·∫°o audio v·ªõi streaming
        async with openai_client.audio.speech.with_streaming_response.create(
            model="gpt-4o-mini-tts",
            voice=voice.lower(),  # API y√™u c·∫ßu t√™n gi·ªçng vi·∫øt th∆∞·ªùng
            input=text,
            instructions=instructions,
            response_format="pcm",
        ) as response:
            # L∆∞u n·ªôi dung PCM v√†o file
            with open(temp_pcm, 'wb') as f:
                async for chunk in response.iter_bytes():
                    f.write(chunk)

        # Chuy·ªÉn ƒë·ªïi PCM sang MP3 b·∫±ng ffmpeg
        result = subprocess.run([
            "ffmpeg", "-y", "-f", "s16le", "-ar", "24000", "-ac", "1",
            "-i", temp_pcm, "-acodec", "libmp3lame", "-b:a", "192k", output_path
        ], capture_output=True)

        # X√≥a file t·∫°m
        if os.path.exists(temp_pcm):
            os.remove(temp_pcm)

        return result.returncode == 0

    except Exception as e:
        print(f"L·ªói t·∫°o GPT-4o TTS: {str(e)}")
        return False

# === H√†m nghe th·ª≠ gi·ªçng n√≥i ===
async def preview_audio_tts(text, instructions, voice, message_placeholder=None):
    """T·∫°o v√† ph√°t m·∫´u gi·ªçng n√≥i t·ª´ GPT-4o-mini-TTS"""
    try:
        if message_placeholder:
            message_placeholder.write("‚è≥ ƒêang t·∫°o m·∫´u gi·ªçng n√≥i...")

        # T·∫°o t·ªáp t·∫°m th·ªùi
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp:
            temp_path = temp.name

        # T·∫°o audio v·ªõi streaming
        async with openai_client.audio.speech.with_streaming_response.create(
            model="gpt-4o-mini-tts",
            voice=voice.lower(),  # API y√™u c·∫ßu t√™n gi·ªçng vi·∫øt th∆∞·ªùng
            input=text,
            instructions=instructions,
            response_format="pcm",
        ) as response:
            # L∆∞u n·ªôi dung PCM v√†o file
            with open(temp_path + ".pcm", 'wb') as f:
                async for chunk in response.iter_bytes():
                    f.write(chunk)

        # Chuy·ªÉn ƒë·ªïi PCM sang MP3 b·∫±ng ffmpeg
        result = subprocess.run([
            "ffmpeg", "-y", "-f", "s16le", "-ar", "24000", "-ac", "1",
            "-i", temp_path + ".pcm", "-acodec", "libmp3lame", "-b:a", "192k", temp_path
        ], capture_output=True)

        if result.returncode != 0:
            if message_placeholder:
                message_placeholder.error("Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi √¢m thanh. Vui l√≤ng th·ª≠ l·∫°i.")
            return None

        # ƒê·ªçc file MP3 ƒë·ªÉ hi·ªÉn th·ªã
        with open(temp_path, "rb") as f:
            audio_bytes = f.read()

        # X√≥a files t·∫°m
        try:
            os.unlink(temp_path)
            os.unlink(temp_path + ".pcm")
        except:
            pass

        return audio_bytes

    except Exception as e:
        if message_placeholder:
            message_placeholder.error(f"L·ªói: {str(e)}")
        return None

# === Handler cho c√°c messages t·ª´ processing thread ===
def handle_message(msg_type: str, content: Any, containers: Dict[str, Any], show_logs: bool = True):
    """X·ª≠ l√Ω messages t·ª´ queue d·ª±a tr√™n lo·∫°i"""
    match msg_type:
        case 'status':
            containers['status'].write(content)
        case 'progress':
            containers['progress'].progress(content)
        case 'log':
            st.session_state.logs.append(content)
            if show_logs and 'log_content' in containers and containers['log_content']:
                containers['log_content'].code("\n".join(st.session_state.logs[-20:]))
        case 'metrics':
            with containers['metrics']:
                cols = st.columns(len(content))
                for i, (key, value) in enumerate(content.items()):
                    cols[i].metric(key, value)
        case 'error':
            st.error(content)
            st.session_state.processing = False
        case 'complete':
            st.session_state.processing = False
            st.session_state.complete = True
            st.session_state.output_file = content['output_file']

            # Th√™m v√†o l·ªãch s·ª≠
            st.session_state.history.append({
                'path': content['output_file'],
                'created': datetime.now(),
                'size': content.get('file_size', 0)
            })

# === H√†m x·ª≠ l√Ω video chung ===
def process_video(workflow_dict, mc_path_final, bg_path_final, audio_path_final, text_prompt, temp_dir, msg_queue, cancel_event, editor, timestamp, tts_service_val, tts_voice_val, tts_speed_val, tts_instructions_val="", position_val="G√≥c d∆∞·ªõi ph·∫£i", scale_val=0.25, caption_style_val="Style 01 (t·ª´ng t·ª´)", fontsize_val=48, caption_position_val="center", caption_zoom_val=False, zoom_size_val=0.01, quality_val="medium", ai_model_val="M√¥ h√¨nh m·∫∑c ƒë·ªãnh", 
                  # Th√™m c√°c tham s·ªë kh·∫©u h√¨nh
                  vad_alpha=1.0, exp_components=None, exp_scale=1.0, pose_scale=1.0, 
                  delta_exp_enabled=False, delta_exp_value=0.0):
    """X·ª≠ l√Ω video trong thread ri√™ng bi·ªát, truy·ªÅn v√†o ƒë·∫ßy ƒë·ªß tham s·ªë"""
    try:
        # X√°c th·ª±c tham s·ªë kh·∫©u h√¨nh ƒë·ªÉ tr√°nh l·ªói
        mouth_params = validate_mouth_params(
            vad_alpha, exp_components, exp_scale, pose_scale, delta_exp_enabled, delta_exp_value
        )
        
        # Chu·∫©n b·ªã files
        if workflow_dict.get("prepare_files", True):
            msg_queue.put(('status', "‚è≥ ƒêang chu·∫©n b·ªã files..."))
            msg_queue.put(('progress', 5))
            msg_queue.put(('log', "B·∫Øt ƒë·∫ßu chu·∫©n b·ªã files..."))

            # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n files t·∫°m v√† ƒë·∫ßu ra
            mc_suffix = Path(mc_path_final.name).suffix if hasattr(mc_path_final, 'name') else Path(str(mc_path_final)).suffix
            bg_suffix = Path(bg_path_final.name).suffix if hasattr(bg_path_final, 'name') and bg_path_final else ".mp4"

            mc_temp_path = temp_dir / f"mc{mc_suffix}"
            bg_temp_path = temp_dir / f"bg{bg_suffix}" if bg_path_final else None
            audio_temp_path = temp_dir / "audio.mp3"
            srt_path = temp_dir / "subtitle.srt"
            talking_path = temp_dir / "talking.mp4"
            output_file = editor.output_dir / f"video_mc_{timestamp}.mp4"
            final_output = editor.output_dir / f"final_mc_{timestamp}.mp4"

            # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
            os.makedirs(editor.output_dir, exist_ok=True)

            # L∆∞u files t·∫°m
            if hasattr(mc_path_final, 'getbuffer'):  # UploadedFile
                with open(mc_temp_path, "wb") as f:
                    f.write(mc_path_final.getbuffer())
                actual_mc_path = mc_temp_path
            else:
                actual_mc_path = mc_path_final

            if bg_path_final:
                if hasattr(bg_path_final, 'getbuffer'):  # UploadedFile
                    with open(bg_temp_path, "wb") as f:
                        f.write(bg_path_final.getbuffer())
                    actual_bg_path = bg_temp_path
                else:
                    actual_bg_path = bg_path_final
            else:
                actual_bg_path = None

            msg_queue.put(('progress', 10))
        else:
            msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc chu·∫©n b·ªã files"))
            # ƒê·∫£m b·∫£o c√°c bi·∫øn c·∫ßn thi·∫øt ƒë∆∞·ª£c kh·ªüi t·∫°o
            mc_temp_path = temp_dir / f"mc{Path(mc_path_final.name).suffix if hasattr(mc_path_final, 'name') else Path(str(mc_path_final)).suffix}"
            bg_temp_path = temp_dir / f"bg{Path(bg_path_final.name).suffix if hasattr(bg_path_final, 'name') else Path(str(bg_path_final)).suffix}" if bg_path_final else None
            audio_temp_path = temp_dir / "audio.mp3"
            srt_path = temp_dir / "subtitle.srt"
            talking_path = temp_dir / "talking.mp4"
            output_file = editor.output_dir / f"video_mc_{timestamp}.mp4"
            final_output = editor.output_dir / f"final_mc_{timestamp}.mp4"

            # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
            os.makedirs(editor.output_dir, exist_ok=True)

            # Copy files
            if hasattr(mc_path_final, 'getbuffer'):
                with open(mc_temp_path, "wb") as f:
                    f.write(mc_path_final.getbuffer())
                actual_mc_path = mc_temp_path
            else:
                actual_mc_path = mc_path_final

            if bg_path_final:
                if hasattr(bg_path_final, 'getbuffer'):
                    with open(bg_temp_path, "wb") as f:
                        f.write(bg_path_final.getbuffer())
                    actual_bg_path = bg_temp_path
                else:
                    actual_bg_path = bg_path_final
            else:
                actual_bg_path = None

        # X·ª≠ l√Ω audio v√† ph·ª• ƒë·ªÅ
        if audio_path_final:  # Upload file
            if workflow_dict.get("subtitle_generation", True):
                msg_queue.put(('status', "üîä ƒêang x·ª≠ l√Ω audio..."))
                msg_queue.put(('log', "X·ª≠ l√Ω audio t·ª´ file..."))

                if hasattr(audio_path_final, 'getbuffer'):
                    with open(audio_temp_path, "wb") as f:
                        f.write(audio_path_final.getbuffer())
                    actual_audio_path = audio_temp_path
                else:
                    actual_audio_path = audio_path_final

                # T·∫°o ph·ª• ƒë·ªÅ t·ª´ audio
                msg_queue.put(('status', "üìù ƒêang t·∫°o ph·ª• ƒë·ªÅ t·ª´ audio..."))
                msg_queue.put(('log', "B·∫Øt ƒë·∫ßu t·∫°o ph·ª• ƒë·ªÅ..."))
                success, error = editor.generate_srt_from_audio(actual_audio_path, srt_path)

                if not success:
                    msg_queue.put(('error', f"L·ªói t·∫°o ph·ª• ƒë·ªÅ: {error}"))
                    return
            else:
                msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc t·∫°o ph·ª• ƒë·ªÅ t·ª´ audio"))

                # V·∫´n x·ª≠ l√Ω audio file
                if hasattr(audio_path_final, 'getbuffer'):
                    with open(audio_temp_path, "wb") as f:
                        f.write(audio_path_final.getbuffer())
                    actual_audio_path = audio_temp_path
                else:
                    actual_audio_path = audio_path_final

                # T·∫°o SRT tr·ªëng ho·∫∑c m·∫´u n·∫øu b·ªè qua b∆∞·ªõc t·∫°o ph·ª• ƒë·ªÅ
                if not workflow_dict.get("subtitle_generation", True):
                    create_empty_srt(srt_path)
        else:  # T·∫°o t·ª´ vƒÉn b·∫£n
            if workflow_dict.get("tts_generation", True):
                msg_queue.put(('status', "üéôÔ∏è ƒêang t·∫°o audio t·ª´ vƒÉn b·∫£n..."))
                msg_queue.put(('log', "B·∫Øt ƒë·∫ßu t·∫°o audio t·ª´ vƒÉn b·∫£n..."))

                # L·∫•y c√†i ƒë·∫∑t TTS t·ª´ c√°c tham s·ªë
                tts_service = "edge" if tts_service_val == "Edge TTS" else "openai"

                # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p GPT-4o-mini-TTS
                if tts_service_val == "GPT-4o-mini-TTS":
                    msg_queue.put(('log', f"S·ª≠ d·ª•ng GPT-4o-mini-TTS v·ªõi gi·ªçng {tts_voice_val} ƒë·ªÉ t·∫°o gi·ªçng n√≥i bi·ªÉu c·∫£m"))

                    # S·ª≠ d·ª•ng asyncio ƒë·ªÉ ch·∫°y function async trong thread ƒë·ªìng b·ªô
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    success = loop.run_until_complete(
                        generate_gpt4o_tts(
                            text_prompt,
                            str(audio_temp_path),
                            tts_instructions_val,
                            tts_voice_val
                        )
                    )
                    loop.close()

                    if not success:
                        msg_queue.put(('error', f"L·ªói t·∫°o audio v·ªõi GPT-4o-mini-TTS"))
                        return

                    # T·∫°o ph·ª• ƒë·ªÅ t·ª´ audio ƒë√£ t·∫°o
                    if workflow_dict.get("subtitle_generation", True):
                        msg_queue.put(('status', "üìù ƒêang t·∫°o ph·ª• ƒë·ªÅ t·ª´ audio..."))
                        success, error = editor.generate_srt_from_audio(str(audio_temp_path), srt_path)
                        if not success:
                            msg_queue.put(('error', f"L·ªói t·∫°o ph·ª• ƒë·ªÅ: {error}"))
                            create_empty_srt(srt_path)
                    else:
                        create_empty_srt(srt_path)
                else:
                    # S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c t·∫°o audio th√¥ng th∆∞·ªùng
                    success, error = editor.generate_audio_from_text(
                        text_prompt,
                        audio_temp_path,
                        srt_path,
                        service=tts_service,
                        voice=tts_voice_val,
                        speed=tts_speed_val
                    )

                    if not success:
                        msg_queue.put(('error', f"L·ªói t·∫°o audio: {error}"))
                        return

                actual_audio_path = audio_temp_path
            else:
                msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc t·∫°o audio t·ª´ vƒÉn b·∫£n"))
                actual_audio_path = use_sample_audio(audio_temp_path)

            if not workflow_dict.get("subtitle_generation", True):
                msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc t·∫°o ph·ª• ƒë·ªÅ"))
                create_empty_srt(srt_path)

        msg_queue.put(('progress', 30))

        # Ki·ªÉm tra n·∫øu ƒë√£ h·ªßy qu√° tr√¨nh
        if cancel_event.is_set():
            msg_queue.put(('error', "Qu√° tr√¨nh ƒë√£ b·ªã h·ªßy b·ªüi ng∆∞·ªùi d√πng"))
            return

        # T·∫°o video khu√¥n m·∫∑t n√≥i
        if workflow_dict.get("talking_head_generation", True):
            msg_queue.put(('status', "üé≠ ƒêang t·∫°o video khu√¥n m·∫∑t n√≥i..."))
            msg_queue.put(('log', "B·∫Øt ƒë·∫ßu t·∫°o video khu√¥n m·∫∑t..."))

            # Ch·ªçn ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh d·ª±a tr√™n l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi d√πng
            model_path = "./checkpoints/trt_Ampere_Plus"
            if ai_model_val == "M√¥ h√¨nh t·ªëi ∆∞u h√≥a":
                model_path = "./checkpoints/trt_custom"
                msg_queue.put(('log', "S·ª≠ d·ª•ng m√¥ h√¨nh t·ªëi ∆∞u h√≥a"))
            else:
                msg_queue.put(('log', "S·ª≠ d·ª•ng m√¥ h√¨nh m·∫∑c ƒë·ªãnh"))
            
            # Chu·∫©n b·ªã tham s·ªë kh·∫©u h√¨nh
            # 1. T·∫°o use_d_keys v·ªõi t·ª∑ l·ªá ph√π h·ª£p
            use_d_keys_dict = {}
            if mouth_params['exp_components']:
                if "exp" in mouth_params['exp_components']:
                    use_d_keys_dict["exp"] = mouth_params['exp_scale']
                for k in ["pitch", "yaw", "roll"]:
                    if k in mouth_params['exp_components']:
                        use_d_keys_dict[k] = mouth_params['pose_scale']
                if "t" in mouth_params['exp_components']:
                    use_d_keys_dict["t"] = 1.0
            
            # 2. T·∫°o ctrl_info
            ctrl_info = {}
            if mouth_params['vad_alpha'] < 1.0:
                msg_queue.put(('log', f"√Åp d·ª•ng m·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i: {mouth_params['vad_alpha']}"))
                for i in range(10000):  # ƒê·ªß cho h·∫ßu h·∫øt video
                    ctrl_info[i] = {"vad_alpha": mouth_params['vad_alpha']}
            
            # 3. Th√™m delta_exp n·∫øu ƒë∆∞·ª£c k√≠ch ho·∫°t
            if mouth_params['delta_exp_enabled'] and mouth_params['delta_exp_value'] != 0.0:
                msg_queue.put(('log', f"√Åp d·ª•ng offset bi·ªÉu c·∫£m m√¥i: {mouth_params['delta_exp_value']}"))
                for i in range(10000):
                    if i in ctrl_info:
                        ctrl_info[i]["delta_exp"] = mouth_params['delta_exp_value']
                    else:
                        ctrl_info[i] = {"delta_exp": mouth_params['delta_exp_value']}
            
            # 4. T·∫°o more_kwargs
            more_kwargs = {
                "setup_kwargs": {},
                "run_kwargs": {
                    "fade_in": 1,
                    "fade_out": 1,
                }
            }
            
            # Th√™m use_d_keys n·∫øu c√≥
            if use_d_keys_dict:
                msg_queue.put(('log', f"√Åp d·ª•ng th√†nh ph·∫ßn bi·ªÉu c·∫£m t√πy ch·ªânh: {use_d_keys_dict}"))
                more_kwargs["setup_kwargs"]["use_d_keys"] = use_d_keys_dict
            
            # Th√™m ctrl_info n·∫øu c√≥
            if ctrl_info:
                more_kwargs["run_kwargs"]["ctrl_info"] = ctrl_info
            
            # L∆∞u more_kwargs v√†o file pickle ƒë·ªÉ truy·ªÅn v√†o inference.py
            more_kwargs_path = temp_dir / "more_kwargs.pkl"
            with open(more_kwargs_path, 'wb') as f:
                pickle.dump(more_kwargs, f)
            
            # ∆Ø·ªõc t√≠nh s·ªë frame ƒë·ªÉ th√™m v√†o log v√† ti·∫øn tr√¨nh
            audio, sr = librosa.core.load(str(actual_audio_path), sr=16000)
            num_frames = int(len(audio) / 16000 * 25)
            msg_queue.put(('log', f"∆Ø·ªõc t√≠nh video s·∫Ω c√≥ kho·∫£ng {num_frames} frames"))
            
            # S·ª≠ d·ª•ng subprocess ƒë·ªÉ g·ªçi inference.py thay v√¨ import tr·ª±c ti·∫øp SDK
            cmd = [
                "python",
                "inference.py",
                "--data_root", model_path,
                "--cfg_pkl", "./checkpoints/cfg/v0.4_hubert_cfg_trt.pkl",
                "--audio_path", str(actual_audio_path),
                "--source_path", str(actual_mc_path),
                "--output_path", str(talking_path),
                "--more_kwargs", str(more_kwargs_path)
            ]
            
            msg_queue.put(('log', f"Ch·∫°y l·ªánh: {' '.join(cmd)}"))
            
            # Kh·ªüi ch·∫°y ti·∫øn tr√¨nh inference v·ªõi theo d√µi output
            process = subprocess.Popen(
                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1
            )
            
            # X·ª≠ l√Ω output
            frame_count, fps = 0, 0
            
            # Thi·∫øt l·∫≠p timeout
            start_time = time.time()
            max_wait_time = 3600  # 1 gi·ªù
            
            while process.poll() is None:
                # Ki·ªÉm tra timeout v√† h·ªßy
                if time.time() - start_time > max_wait_time or cancel_event.is_set():
                    process.terminate()
                    msg_queue.put(('error', "Qu√° th·ªùi gian x·ª≠ l√Ω" if time.time() - start_time > max_wait_time else "Qu√° tr√¨nh ƒë√£ b·ªã h·ªßy"))
                    return
                
                # ƒê·ªçc m·ªôt d√≤ng t·ª´ stdout
                if line := process.stdout.readline():
                    # L·ªçc ANSI escape sequences
                    clean = re.sub(r'\x1B(?:[@-Z\\-_]|\[[0-? ]*[ -/]*[@-~])', '', line.strip())
                    if not clean or "aligned" in clean:
                        continue
                    
                    # X·ª≠ l√Ω th√¥ng tin audio processing
                    if "dit:" in clean and (m := re.search(r'dit: (\d+)it . *? (\d+\.\d+)?it/s', clean)):
                        step, speed = int(m.group(1)), float(m.group(2) or 0)
                        progress_value = min(30 + step/10*10, 40)
                        msg_queue.put(('progress', int(progress_value)))
                        msg_queue.put(('status', f"ƒêang x·ª≠ l√Ω √¢m thanh ‚Üí chuy·ªÉn ƒë·ªông ({step}/10)"))
                        msg_queue.put(('metrics', {'Ti·∫øn ƒë·ªô √¢m thanh': f"{step*10}%", 'T·ªëc ƒë·ªô': f"{speed:.1f}it/s"}))
                        if step % 2 == 0:
                            msg_queue.put(('log', f"‚û°Ô∏è Audio processing: {step}/10 ({speed:.1f}it/s)"))
                    
                    # X·ª≠ l√Ω th√¥ng tin frame video
                    elif "writer:" in clean and (m := re.search(r'writer: (\d+)it . *? (\d+\.\d+)?it/s', clean)):
                        frame, speed = int(m.group(1)), float(m.group(2) or 0)
                        frame_count, fps = frame, speed
                        progress_value = min(40 + frame/400*20, 60)
                        msg_queue.put(('progress', int(progress_value)))
                        msg_queue.put(('status', f"ƒêang t·∫°o video (frame {frame})"))
                        msg_queue.put(('metrics', {'Frames': frame, 'FPS': f"{speed:.1f}"}))
                        if frame % 50 == 0 or frame <= 5:
                            msg_queue.put(('log', f"üé¨ Video: frame {frame} ({speed:.1f} fps)"))
                else:
                    # Tr√°nh ti√™u t·ªën CPU
                    time.sleep(0.1)
            
            # ƒê·ªçc stderr output ƒë·ªÉ ghi log
            stderr_output = process.stderr.read()
            
            # Ki·ªÉm tra file ƒë·∫ßu ra
            if process.returncode != 0 or not os.path.exists(talking_path):
                # Fallback: T·∫°o video tr·ª±c ti·∫øp b·∫±ng ffmpeg
                msg_queue.put(('log', f"L·ªói khi t·∫°o video khu√¥n m·∫∑t n√≥i: {stderr_output}"))
                msg_queue.put(('log', "D√πng ffmpeg tr·ª±c ti·∫øp ƒë·ªÉ t·∫°o video khu√¥n m·∫∑t n√≥i"))
                
                fallback_cmd = [
                    "ffmpeg", "-y", 
                    "-loop", "1" if Path(str(actual_mc_path)).suffix.lower() in ['.jpg', '.jpeg', '.png'] else "-i",
                    str(actual_mc_path),
                    "-i", str(actual_audio_path),
                    "-c:v", "libx264",
                    "-tune", "stillimage" if Path(str(actual_mc_path)).suffix.lower() in ['.jpg', '.jpeg', '.png'] else None,
                    "-c:a", "aac",
                    "-shortest", str(talking_path)
                ]
                # L·ªçc b·ªè c√°c tham s·ªë None
                fallback_cmd = [cmd for cmd in fallback_cmd if cmd is not None]
                
                msg_queue.put(('log', f"L·ªánh fallback: {' '.join(fallback_cmd)}"))
                result = subprocess.run(fallback_cmd, capture_output=True, text=True)
                
                if result.returncode != 0 or not os.path.exists(talking_path):
                    msg_queue.put(('error', f"L·ªói khi t·∫°o video v·ªõi ph∆∞∆°ng √°n d·ª± ph√≤ng: {result.stderr}"))
                    return
                
                msg_queue.put(('log', "ƒê√£ t·∫°o video b·∫±ng ph∆∞∆°ng √°n d·ª± ph√≤ng"))
            
            msg_queue.put(('log', f"‚úÖ ƒê√£ t·∫°o th√†nh c√¥ng video khu√¥n m·∫∑t n√≥i: {talking_path}"))
            msg_queue.put(('progress', 60))
        else:
            msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc t·∫°o video khu√¥n m·∫∑t n√≥i"))
            # N·∫øu b·ªè qua, s·ª≠ d·ª•ng MC g·ªëc l√†m video khu√¥n m·∫∑t
            if Path(str(actual_mc_path)).suffix.lower() in ['.mp4']:
                shutil.copy(str(actual_mc_path), str(talking_path))
            else:
                # N·∫øu l√† ·∫£nh, t·∫°o video tƒ©nh t·ª´ ·∫£nh
                ffmpeg_cmd = [
                    "ffmpeg", "-y", "-loop", "1", "-i", str(actual_mc_path), 
                    "-i", str(actual_audio_path), "-c:v", "libx264", 
                    "-tune", "stillimage", "-c:a", "aac", "-shortest", str(talking_path)
                ]
                subprocess.run(ffmpeg_cmd, capture_output=True)
        
        msg_queue.put(('progress', 60))
        
        # N·∫øu ƒëang ·ªü tab "T·∫°o Video Khu√¥n M·∫∑t AI", ho·∫∑c b·ªè qua b∆∞·ªõc gh√©p video,
        # th√¨ d√πng talking_path l√†m k·∫øt qu·∫£ cu·ªëi c√πng
        if actual_bg_path is None or not workflow_dict.get("video_overlay", True):
            # ƒê√¢y l√† tab "T·∫°o Video Khu√¥n M·∫∑t AI" ho·∫∑c b·ªè qua b∆∞·ªõc gh√©p video
            msg_queue.put(('log', "D√πng video khu√¥n m·∫∑t n√≥i l√†m k·∫øt qu·∫£ cu·ªëi c√πng"))
            shutil.copy(str(talking_path), str(final_output))
            
            # Ho√†n t·∫•t
            msg_queue.put(('progress', 100))
            msg_queue.put(('status', "‚úÖ Ho√†n th√†nh!"))
            msg_queue.put(('log', "X·ª≠ l√Ω video ho√†n t·∫•t!"))
            
            # Th√™m v√†o l·ªãch s·ª≠ v√† ho√†n th√†nh
            msg_queue.put(('complete', {
                'output_file': str(final_output),
                'file_size': os.path.getsize(final_output) / (1024*1024),
                'frame_count': frame_count,
                'fps': fps
            }))
            return
        
        # Ki·ªÉm tra n·∫øu qu√° tr√¨nh ti·∫øp theo c√≥ ƒë∆∞·ª£c th·ª±c hi·ªán hay kh√¥ng
        if cancel_event.is_set():
            msg_queue.put(('error', "Qu√° tr√¨nh ƒë√£ b·ªã h·ªßy b·ªüi ng∆∞·ªùi d√πng"))
            return
        
        # Gh√©p video MC v√† n·ªÅn
        if workflow_dict.get("video_overlay", True):
            msg_queue.put(('status', "üé¨ ƒêang gh√©p video..."))
            msg_queue.put(('log', "B·∫Øt ƒë·∫ßu gh√©p video..."))
            
            # Truy·ªÅn tr·ª±c ti·∫øp t√™n ti·∫øng Vi·ªát (kh√¥ng chuy·ªÉn sang ti·∫øng Anh)
            overlay_cmd = [
                "python", "video_overlay.py", 
                "-m", str(talking_path), 
                "-b", str(actual_bg_path), 
                "-o", str(output_file),
                "-p", position_val,  # Truy·ªÅn tr·ª±c ti·∫øp t√™n ti·∫øng Vi·ªát
                "-s", str(scale_val),
                "-q", quality_val
            ]
            
            msg_queue.put(('log', f"Ch·∫°y l·ªánh gh√©p video: {' '.join(overlay_cmd)}"))
            
            try:
                # Ch·∫°y l·ªánh v·ªõi timeout ƒë·ªÉ tr√°nh treo
                result = subprocess.run(
                    overlay_cmd, 
                    capture_output=True, 
                    text=True, 
                    timeout=1800  # 30 ph√∫t
                )
                
                # Log output ƒë·∫ßy ƒë·ªß ƒë·ªÉ debug
                if result.stdout.strip():
                    msg_queue.put(('log', f"Output: {result.stdout}"))
                if result.stderr.strip():
                    msg_queue.put(('log', f"Error: {result.stderr}"))
                
                if result.returncode != 0:
                    # Fallback: S·ª≠ d·ª•ng ffmpeg tr·ª±c ti·∫øp
                    msg_queue.put(('log', "D√πng ffmpeg tr·ª±c ti·∫øp ƒë·ªÉ gh√©p video"))
                    
                    # √Ånh x·∫° v·ªã tr√≠ cho ffmpeg
                    positions = {
                        "G√≥c tr√™n tr√°i": "10:10",
                        "G√≥c tr√™n ph·∫£i": "main_w-overlay_w-10:10",
                        "G√≥c d∆∞·ªõi tr√°i": "10:main_h-overlay_h-10",
                        "G√≥c d∆∞·ªõi ph·∫£i": "main_w-overlay_w-10:main_h-overlay_h-10",
                        "Ch√≠nh gi·ªØa": "(main_w-overlay_w)/2:(main_h-overlay_h)/2"
                    }
                    pos = positions.get(position_val, positions["G√≥c d∆∞·ªõi ph·∫£i"])
                    
                    fallback_cmd = [
                        "ffmpeg", "-y",
                        "-i", str(actual_bg_path),
                        "-i", str(talking_path),
                        "-filter_complex", f"[1:v]scale=iw*{scale_val}:-1[overlay];[0:v][overlay]overlay={pos}"
                    ]
                    
                    # Th√™m audio mapping
                    try:
                        # Ki·ªÉm tra stream audio
                        audio_check = subprocess.run(
                            ["ffprobe", "-v", "error", "-show_entries", "stream=codec_type", "-of", "json", str(talking_path)],
                            capture_output=True, text=True
                        )
                        
                        if "audio" in audio_check.stdout:
                            fallback_cmd.extend(["-map", "1:a", "-c:a", "aac", "-b:a", "192k"])
                        else:
                            audio_check_bg = subprocess.run(
                                ["ffprobe", "-v", "error", "-show_entries", "stream=codec_type", "-of", "json", str(actual_bg_path)],
                                capture_output=True, text=True
                            )
                            if "audio" in audio_check_bg.stdout:
                                fallback_cmd.extend(["-map", "0:a", "-c:a", "aac", "-b:a", "192k"])
                            else:
                                fallback_cmd.append("-an")
                    except Exception:
                        # M·∫∑c ƒë·ªãnh gi·ªØ audio t·ª´ background n·∫øu ki·ªÉm tra th·∫•t b·∫°i
                        fallback_cmd.extend(["-map", "0:a? ", "-c:a", "aac"])
                    
                    # Th√™m c√†i ƒë·∫∑t video v√† ƒë∆∞·ªùng d·∫´n output
                    fallback_cmd.extend([
                        "-c:v", "libx264",
                        "-preset", {"low": "ultrafast", "medium": "medium", "high": "slow"}.get(quality_val, "medium"),
                        "-crf", "23",
                        str(output_file)
                    ])
                    
                    msg_queue.put(('log', f"L·ªánh fallback: {' '.join(fallback_cmd)}"))
                    fallback_result = subprocess.run(fallback_cmd, capture_output=True, text=True, timeout=1800)
                    
                    if fallback_result.returncode != 0:
                        msg_queue.put(('error', f"L·ªói khi gh√©p video v·ªõi ph∆∞∆°ng √°n d·ª± ph√≤ng: {fallback_result.stderr}"))
                        return
                    
                    msg_queue.put(('log', "ƒê√£ gh√©p video b·∫±ng ph∆∞∆°ng √°n d·ª± ph√≤ng"))
            except subprocess.TimeoutExpired:
                msg_queue.put(('error', "Qu√° th·ªùi gian x·ª≠ l√Ω khi gh√©p video (30 ph√∫t)"))
                return
            except Exception as e:
                error_details = traceback.format_exc()
                msg_queue.put(('error', f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi gh√©p video: {str(e)}\n{error_details}"))
                return
            
            # Ki·ªÉm tra file ƒë·∫ßu ra
            if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:
                msg_queue.put(('error', f"File ƒë·∫ßu ra kh√¥ng h·ª£p l·ªá: {output_file}"))
                return
            
            msg_queue.put(('log', f"‚úÖ ƒê√£ gh√©p video th√†nh c√¥ng: {output_file}"))
            msg_queue.put(('progress', 80))
        else:
            msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc gh√©p video"))
            # N·∫øu b·ªè qua, s·ª≠ d·ª•ng video khu√¥n m·∫∑t l√†m k·∫øt qu·∫£
            shutil.copy(str(talking_path), str(output_file))
            msg_queue.put(('progress', 80))
        
        # Ki·ªÉm tra h·ªßy
        if cancel_event.is_set():
            msg_queue.put(('error', "Qu√° tr√¨nh ƒë√£ b·ªã h·ªßy b·ªüi ng∆∞·ªùi d√πng"))
            return
        
        # √Åp d·ª•ng ph·ª• ƒë·ªÅ theo style ƒë√£ ch·ªçn
        if workflow_dict.get("caption_application", True):
            msg_queue.put(('status', "üî§ ƒêang th√™m ph·ª• ƒë·ªÅ..."))
            msg_queue.put(('log', f"Th√™m ph·ª• ƒë·ªÅ ki·ªÉu: {caption_style_val}"))
            
            try:
                # Ki·ªÉm tra file SRT
                if not os.path.exists(srt_path):
                    msg_queue.put(('error', f"Kh√¥ng t√¨m th·∫•y file ph·ª• ƒë·ªÅ SRT: {srt_path}"))
                    return
                
                if caption_style_val == "Style 01 (t·ª´ng t·ª´)":
                    success, error = editor.apply_caption_style_01(
                        output_file,
                        srt_path,
                        final_output,
                        actual_audio_path,
                        fontsize_val
                    )
                else:  # Style 02 (gradient)
                    success, error = editor.apply_caption_style_02(
                        output_file,
                        srt_path,
                        final_output,
                        actual_audio_path,
                        fontsize_val,
                        caption_position_val,
                        caption_zoom_val,
                        zoom_size_val
                    )
                
                if not success:
                    msg_queue.put(('error', f"L·ªói khi th√™m ph·ª• ƒë·ªÅ: {error}"))
                    return
            except Exception as e:
                error_details = traceback.format_exc()
                msg_queue.put(('error', f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi th√™m ph·ª• ƒë·ªÅ: {str(e)}\n{error_details}"))
                return
        else:
            msg_queue.put(('log', "‚è© B·ªè qua b∆∞·ªõc th√™m ph·ª• ƒë·ªÅ"))
            # S·ª≠ d·ª•ng video kh√¥ng c√≥ ph·ª• ƒë·ªÅ l√†m k·∫øt qu·∫£ cu·ªëi c√πng
            shutil.copy(str(output_file), str(final_output))
        
        # Ho√†n t·∫•t
        msg_queue.put(('progress', 100))
        msg_queue.put(('status', "‚úÖ Ho√†n th√†nh!"))
        msg_queue.put(('log', "X·ª≠ l√Ω video ho√†n t·∫•t!"))
        
        # Ki·ªÉm tra file ƒë·∫ßu ra cu·ªëi c√πng
        if not os.path.exists(final_output) or os.path.getsize(final_output) == 0:
            msg_queue.put(('error', f"File ƒë·∫ßu ra cu·ªëi c√πng kh√¥ng h·ª£p l·ªá: {final_output}"))
            # Th·ª≠ copy file output n·∫øu c√≥
            if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
                shutil.copy(output_file, final_output)
                msg_queue.put(('log', f"ƒê√£ sao ch√©p video kh√¥ng c√≥ ph·ª• ƒë·ªÅ l√†m k·∫øt qu·∫£ cu·ªëi c√πng"))
            else:
                return
        
        # Th√™m v√†o l·ªãch s·ª≠ v√† ho√†n th√†nh
        msg_queue.put(('complete', {
            'output_file': str(final_output),
            'file_size': os.path.getsize(final_output) / (1024*1024),
            'frame_count': frame_count if 'frame_count' in locals() else 0,
            'fps': fps if 'fps' in locals() else 0
        }))
        
    except Exception as e:
        error_details = traceback.format_exc()
        msg_queue.put(('error', f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}\n{error_details}"))
    finally:
        # D·ªçn d·∫πp
        try:
            shutil.rmtree(temp_dir, ignore_errors=True)
        except Exception as e:
            msg_queue.put(('log', f"L·ªói khi d·ªçn d·∫πp: {str(e)}"))

def main():
    # ƒê·∫£m b·∫£o session_state ƒë∆∞·ª£c kh·ªüi t·∫°o
    init_session_state()
    
    st.set_page_config(page_title="Video AI Creator", page_icon="üé¨", layout="wide")
    
    # Hi·ªÉn th·ªã logo v√† ti√™u ƒë·ªÅ ·ª©ng d·ª•ng
    # col1, col2 = st.columns([1, 5])
    # with col1:
    #     st.image("/home/image_talking/aiclip_logo.png", width=100)
    # with col2:
    #     st.title("üé¨ Video AI Creator")
    #     st.caption("Powered by [aiclip.ai](https://aiclip.ai/)")
    
    # Kh·ªüi t·∫°o editor
    editor = VideoEditor(output_dir="./output")

    # === Sidebar cho c√†i ƒë·∫∑t ===
    with st.sidebar:
        st.title("‚öôÔ∏è C√†i ƒë·∫∑t")
        show_logs = st.checkbox("Hi·ªÉn th·ªã logs", value=False)  # M·∫∑c ƒë·ªãnh kh√¥ng hi·ªÉn th·ªã logs
        quality = st.select_slider(
            "Ch·∫•t l∆∞·ª£ng video",
            options=["low", "medium", "high"],
            value="medium"
        )
        st.divider()
        
        with st.expander("üí° M·∫πo s·ª≠ d·ª•ng", expanded=False):
            st.markdown("""
            **M·∫πo t·ªëi ∆∞u:**
            - MC n√™n c√≥ n·ªÅn ƒë·ªìng m√†u ho·∫∑c trong su·ªët
            - Video n·ªÅn n√™n c√≥ ƒë·ªãnh d·∫°ng 16:9
            - Audio n√™n r√µ r√†ng, kh√¥ng nhi·ªÖu
            
            **ƒê·ªãnh d·∫°ng h·ªó tr·ª£:**
            - MC: JPG, PNG, MP4
            - N·ªÅn: MP4
            - Audio: WAV, MP3
            """)

    # === Tabs ch√≠nh ===
    tabs = st.tabs(["T·∫°o Video MC", "T·∫°o Video Khu√¥n M·∫∑t AI", "Quy Tr√¨nh", "L·ªãch S·ª≠", "H∆∞·ªõng D·∫´n"])

    # === Tab 0: T·∫°o Video MC ===
    with tabs[0]:
        # Chia th√†nh 2 c·ªôt
        col1, col2 = st.columns([3, 2])
        
        with col1:
            # Subtabs cho input
            input_tabs = st.tabs(["Input Files", "MC Settings", "Caption Settings"])
            
            # === Tab Input Files ===
            with input_tabs[0]:
                st.subheader("T·∫£i l√™n v√† c√†i ƒë·∫∑t")
                
                # MC uploader
                mc_file = st.file_uploader("T·∫£i l√™n ·∫¢nh/Video MC", type=["png", "jpg", "jpeg", "mp4"])
                if mc_file:
                    # Hi·ªÉn th·ªã preview cho file ƒë√£ upload
                    if Path(mc_file.name).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                        st.image(mc_file, use_container_width=True, caption="MC Preview")
                    else:
                        st.video(mc_file)
                else:
                    mc_path = st.selectbox(
                        "Ho·∫∑c ch·ªçn file m·∫´u:",
                        options=[""] + [str(p) for p in Path("./example").glob("*.[jp][pn]g")] + [str(p) for p in Path("./example").glob("*mc*.mp4")],
                        format_func=lambda x: Path(x).name if x else "Ch·ªçn file m·∫´u..."
                    )
                    if mc_path:
                        if Path(mc_path).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                            st.image(mc_path, use_container_width=True, caption="MC Preview")
                        else:
                            st.video(mc_path)
                
                # BG uploader
                bg_file = st.file_uploader("T·∫£i l√™n Video N·ªÅn", type=["mp4"])
                if bg_file:
                    # Hi·ªÉn th·ªã preview cho video n·ªÅn ƒë√£ upload
                    st.video(bg_file)
                else:
                    bg_path = st.selectbox(
                        "Ho·∫∑c ch·ªçn video n·ªÅn m·∫´u:",
                        options=[""] + [str(p) for p in Path("./example").glob("*bg*.mp4")],
                        format_func=lambda x: Path(x).name if x else "Ch·ªçn video n·ªÅn m·∫´u...",
                        key="bg_sample"
                    )
                    if bg_path:
                        st.video(bg_path)
                
                # Audio source
                audio_source = st.radio("Ngu·ªìn audio:", ["Upload file", "T·∫°o t·ª´ vƒÉn b·∫£n"], horizontal=True)
                
                if audio_source == "Upload file":
                    audio_file = st.file_uploader("T·∫£i l√™n Audio tho·∫°i", type=["wav", "mp3"])
                    if audio_file:
                        # Hi·ªÉn th·ªã preview cho audio ƒë√£ upload
                        st.audio(audio_file)
                    else:
                        audio_path = st.selectbox(
                            "Ho·∫∑c ch·ªçn audio m·∫´u:",
                            options=[""] + [str(p) for p in Path("./example").glob("*.wav")] + [str(p) for p in Path("./example").glob("*.mp3")],
                            format_func=lambda x: Path(x).name if x else "Ch·ªçn audio m·∫´u..."
                        )
                        if audio_path:
                            st.audio(audio_path)
                    text_prompt = None
                else:  # T·∫°o t·ª´ vƒÉn b·∫£n
                    audio_file = None
                    audio_path = None
                    text_prompt = st.text_area("Nh·∫≠p vƒÉn b·∫£n tho·∫°i:", height=150)
                
                # TTS settings - CH·ªà HI·ªÇN TH·ªä KHI CH·ªåN "T·∫†O T·ª™ VƒÇN B·∫¢N"
                if audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n":
                    with st.expander("C√†i ƒë·∫∑t TTS", expanded=True):
                        tts_service = st.selectbox(
                            "D·ªãch v·ª• TTS:",
                            options=["Edge TTS", "OpenAI TTS", "GPT-4o-mini-TTS"],
                            index=2,  # M·∫∑c ƒë·ªãnh ch·ªçn GPT-4o-mini-TTS
                            key="tts_service"
                        )
                        
                        # Hi·ªÉn th·ªã c√°c t√πy ch·ªçn gi·ªçng n√≥i d·ª±a tr√™n d·ªãch v·ª•
                        if tts_service == "Edge TTS":
                            voice_options = ["vi-VN-NamMinhNeural", "vi-VN-HoaiMyNeural"]
                            
                            tts_voice = st.selectbox(
                                "Gi·ªçng ƒë·ªçc:",
                                options=voice_options,
                                index=0,
                                key="tts_voice"
                            )
                            
                            # Hi·ªÉn th·ªã ƒëi·ªÅu ch·ªânh t·ªëc ƒë·ªô cho Edge TTS
                            tts_speed = st.slider(
                                "T·ªëc ƒë·ªô ƒë·ªçc:",
                                min_value=0.8,
                                max_value=1.5,
                                value=1.2,
                                step=0.1,
                                key="tts_speed"
                            )
                            
                            # ƒê·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh cho tts_instructions
                            tts_instructions = ""
                            
                        elif tts_service == "OpenAI TTS":
                            voice_options = ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
                            
                            tts_voice = st.selectbox(
                                "Gi·ªçng ƒë·ªçc:",
                                options=voice_options,
                                index=0,
                                key="tts_voice"
                            )
                            
                            # Hi·ªÉn th·ªã ƒëi·ªÅu ch·ªânh t·ªëc ƒë·ªô cho OpenAI TTS
                            tts_speed = st.slider(
                                "T·ªëc ƒë·ªô ƒë·ªçc:",
                                min_value=0.8,
                                max_value=1.5,
                                value=1.2,
                                step=0.1,
                                key="tts_speed"
                            )
                            
                            # ƒê·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh cho tts_instructions
                            tts_instructions = ""
                            
                        else:  # GPT-4o-mini-TTS
                            voice_options = ["Ash", "Ballad", "Coral", "Echo", "Fable", "Onyx", "Nova", "Sage", "Shimmer", "Verse"]
                            
                            st.write("**üîä Ch·ªçn gi·ªçng n√≥i v√† nghe th·ª≠:**")
                            
                            tts_voice = st.selectbox(
                                "Gi·ªçng ƒë·ªçc:",
                                options=voice_options,
                                index=voice_options.index("Shimmer") if "Shimmer" in voice_options else 0,
                                key="tts_voice"
                            )
                            
                            # Hi·ªÉn th·ªã m√¥ t·∫£ ng·∫Øn g·ªçn c·ªßa gi·ªçng ƒë·ªçc ƒë√£ ch·ªçn
                            st.caption(f"**{tts_voice}**: {VOICE_DESCRIPTIONS.get(tts_voice, '')}")
                            
                            # Th√™m tr∆∞·ªùng h∆∞·ªõng d·∫´n gi·ªçng n√≥i cho GPT-4o-mini-TTS
                            tts_instructions = st.text_area(
                                "H∆∞·ªõng d·∫´n v·ªÅ gi·ªçng ƒëi·ªáu:",
                                value="""Tone: T·ª± nhi√™n, tr√¥i ch·∫£y, chuy√™n nghi·ªáp
Emotion: Nhi·ªát t√¨nh, t·ª± tin
Delivery: R√µ r√†ng, nh·ªãp ƒë·ªô v·ª´a ph·∫£i, nh·∫•n m·∫°nh t·ª´ kh√≥a quan tr·ªçng""",
                                height=100,
                                key="tts_instructions",
                                help="M√¥ t·∫£ t√¥ng gi·ªçng, c·∫£m x√∫c v√† c√°ch truy·ªÅn ƒë·∫°t mong mu·ªën"
                            )
                        
                            # T·∫°o m·∫´u vƒÉn b·∫£n ƒë·ªÉ nghe th·ª≠
                            if text_prompt:
                                sample_text = text_prompt[:200] + "..." if len(text_prompt) > 200 else text_prompt
                            else:
                                sample_text = "Xin ch√†o! ƒê√¢y l√† m·∫´u th·ª≠ gi·ªçng n√≥i t·ª´ GPT-4o. B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh vƒÉn b·∫£n n√†y ƒë·ªÉ nghe th·ª≠ tr∆∞·ªõc khi t·∫°o video."
                            
                            preview_text = st.text_area(
                                "VƒÉn b·∫£n m·∫´u ƒë·ªÉ nghe th·ª≠:",
                                value=sample_text,
                                height=80,
                                key="preview_text"
                            )
                            
                            preview_message = st.empty()
                            preview_audio = st.empty()
                            
                            # N√∫t nghe th·ª≠
                            if st.button("üîä Nghe th·ª≠ gi·ªçng n√≥i", use_container_width=True, key="tts_preview"):
                                if not preview_text.strip():
                                    preview_message.warning("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n m·∫´u ƒë·ªÉ nghe th·ª≠")
                                else:
                                    # S·ª≠ d·ª•ng asyncio
                                    audio_bytes = asyncio.run(preview_audio_tts(
                                        preview_text,
                                        tts_instructions,
                                        tts_voice,
                                        preview_message
                                    ))
                                    
                                    if audio_bytes:
                                        preview_message.success("‚úÖ T·∫°o m·∫´u gi·ªçng n√≥i th√†nh c√¥ng!")
                                        preview_audio.audio(audio_bytes, format="audio/mp3")
                        
                        # Hi·ªÉn th·ªã c√°c th√¥ng tin tham kh·∫£o v·ªÅ gi·ªçng n√≥i v√† h∆∞·ªõng d·∫´n (b√™n ngo√†i expander)
                        if tts_service == "GPT-4o-mini-TTS":
                            st.divider()
                            st.subheader("üîä Tham kh·∫£o v·ªÅ gi·ªçng n√≥i GPT-4o-mini-TTS")
                            
                            voice_info_col, examples_col = st.columns(2)
                            
                            with voice_info_col:
                                st.markdown("**ƒê·∫∑c ƒëi·ªÉm c·ªßa c√°c gi·ªçng n√≥i:**")
                                st.markdown("""
                                - **Ash**: Gi·ªçng nam tr∆∞·ªüng th√†nh, h∆°i tr·∫ßm, ph√π h·ª£p cho phim t√†i li·ªáu
                                - **Ballad**: Gi·ªçng n·ªØ m·ªÅm m·∫°i, ·∫•m √°p, ph√π h·ª£p cho n·ªôi dung t∆∞ v·∫•n
                                - **Coral**: Gi·ªçng n·ªØ tr·∫ª, r√µ r√†ng, t·ª± tin, ph√π h·ª£p cho n·ªôi dung gi√°o d·ª•c
                                - **Echo**: Gi·ªçng nam tr·∫ª, nƒÉng ƒë·ªông, ph√π h·ª£p cho qu·∫£ng c√°o
                                - **Fable**: Gi·ªçng nam uy t√≠n, ph√π h·ª£p cho th√¥ng b√°o ch√≠nh th·ª©c
                                - **Onyx**: Gi·ªçng nam tr·∫ßm, sang tr·ªçng, ph√π h·ª£p cho thuy·∫øt tr√¨nh
                                - **Nova**: Gi·ªçng n·ªØ chuy√™n nghi·ªáp, ph√π h·ª£p cho tin t·ª©c
                                - **Sage**: Gi·ªçng n·ªØ t·ª´ng tr·∫£i, ·∫•m √°p, ph√π h·ª£p cho podcast
                                - **Shimmer**: Gi·ªçng n·ªØ t∆∞∆°i s√°ng, nƒÉng ƒë·ªông, ph√π h·ª£p cho gi·∫£i tr√≠
                                - **Verse**: Gi·ªçng nam t·ª± nhi√™n, c√¢n b·∫±ng, ph√π h·ª£p cho ƒëa d·∫°ng n·ªôi dung
                                """)
                            
                            with examples_col:
                                st.markdown("**V√≠ d·ª• v·ªÅ h∆∞·ªõng d·∫´n gi·ªçng n√≥i:**")
                                st.markdown("""
                                **Gi·ªçng di·ªÖn thuy·∫øt:**
                                ```
                                Tone: ƒêƒ©nh ƒë·∫°c, trang tr·ªçng, ƒë·∫ßy t·ª± tin
                                Emotion: Nhi·ªát huy·∫øt, quy·∫øt ƒëo√°n
                                Delivery: Nh·ªãp ƒë·ªô v·ª´a ph·∫£i v·ªõi c√°c ng·∫Øt qu√£ng, nh·∫•n m·∫°nh t·ª´ kh√≥a quan tr·ªçng
                                ```
                                
                                **Gi·ªçng t∆∞ v·∫•n:**
                                ```
                                Tone: ·∫§m √°p, th√¢n thi·ªán, ƒë√°ng tin c·∫≠y
                                Emotion: Th·∫•u hi·ªÉu, quan t√¢m
                                Delivery: Nh·∫π nh√†ng, r√µ r√†ng, t·∫°o c·∫£m gi√°c an t√¢m
                                ```
                                
                                **Gi·ªçng qu·∫£ng c√°o:**
                                ```
                                Tone: S√¥i n·ªïi, cu·ªën h√∫t, nƒÉng ƒë·ªông
                                Emotion: Ph·∫•n kh√≠ch, h√†o h·ª©ng
                                Delivery: Nhanh, ƒë·∫ßy nƒÉng l∆∞·ª£ng, v·ªõi c∆∞·ªùng ƒë·ªô tƒÉng d·∫ßn
                                ```
                                """)
            
            # === Tab MC Settings ===
            with input_tabs[1]:
                st.subheader("T√πy ch·ªânh MC")
                
                # V·ªã tr√≠ v√† k√≠ch th∆∞·ªõc
                p_col, s_col = st.columns(2)
                with p_col:
                    position = st.selectbox(
                        "V·ªã tr√≠ MC",
                        ["G√≥c tr√™n tr√°i", "G√≥c tr√™n ph·∫£i", "G√≥c d∆∞·ªõi tr√°i", "G√≥c d∆∞·ªõi ph·∫£i", "Ch√≠nh gi·ªØa"],
                        index=3
                    )
                
                with s_col:
                    # Auto scale checkbox
                    auto_scale = st.checkbox(
                        "T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc",
                        value=st.session_state.auto_scale
                    )
                    st.session_state.auto_scale = auto_scale
                    
                    # T√≠nh to√°n scale
                    scale = 0.25  # M·∫∑c ƒë·ªãnh
                    if auto_scale and bg_file:
                        try:
                            # L∆∞u file bg t·∫°m v√† l·∫•y k√≠ch th∆∞·ªõc
                            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(bg_file.name).suffix) as temp:
                                temp.write(bg_file.getbuffer())
                                bg_temp_path = temp.name
                            
                            width, height = get_video_resolution(bg_temp_path)
                            
                            os.unlink(bg_temp_path)
                            
                            # T√≠nh scale t·ª± ƒë·ªông
                            scale = calculate_auto_scale(mc_file if mc_file else mc_path, width, height)
                            st.write(f"K√≠ch th∆∞·ªõc t·ª± ƒë·ªông: {scale:.2f}")
                        except Exception:
                            scale = 0.25
                    elif auto_scale and bg_path:
                        try:
                            width, height = get_video_resolution(bg_path)
                            scale = calculate_auto_scale(mc_file if mc_file else mc_path, width, height)
                            st.write(f"K√≠ch th∆∞·ªõc t·ª± ƒë·ªông: {scale:.2f}")
                        except Exception:
                            scale = 0.25
                    else:
                        scale = st.slider("K√≠ch th∆∞·ªõc", 0.1, 0.5, 0.25, 0.05)
                
                # Th√™m ph·∫ßn t√πy ch·ªânh kh·∫©u h√¨nh
                with st.expander("üó£Ô∏è T√πy ch·ªânh kh·∫©u h√¨nh", expanded=False):
                    # ƒêi·ªÅu ch·ªânh m·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i
                    vad_alpha = st.slider(
                        "M·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i:",
                        min_value=0.0,
                        max_value=1.0,
                        value=1.0,
                        step=0.05,
                        help="Gi√° tr·ªã th·∫•p h∆°n s·∫Ω l√†m gi·∫£m chuy·ªÉn ƒë·ªông m√¥i, gi√° tr·ªã cao h∆°n s·∫Ω tƒÉng chuy·ªÉn ƒë·ªông"
                    )
                    
                    # T√πy ch·ªçn n√¢ng cao
                    mouth_advanced = st.checkbox("T√πy ch·ªçn n√¢ng cao cho kh·∫©u h√¨nh", value=False)
                    if mouth_advanced:
                        # Ch·ªçn c√°c th√†nh ph·∫ßn bi·ªÉu c·∫£m
                        exp_components = st.multiselect(
                            "Th√†nh ph·∫ßn bi·ªÉu c·∫£m:",
                            options=["exp", "pitch", "yaw", "roll", "t"],
                            default=["exp", "pitch", "yaw", "roll", "t"],
                            help="Ch·ªçn c√°c th√†nh ph·∫ßn bi·ªÉu c·∫£m ƒë·ªÉ s·ª≠ d·ª•ng t·ª´ m√¥ h√¨nh"
                        )
                        
                        # ƒêi·ªÅu ch·ªânh t·ª∑ l·ªá cho c√°c th√†nh ph·∫ßn
                        exp_scale = st.slider(
                            "T·ª∑ l·ªá bi·ªÉu c·∫£m mi·ªáng (exp):",
                            min_value=0.5,
                            max_value=1.5,
                            value=1.0,
                            step=0.1,
                            help="ƒêi·ªÅu ch·ªânh t·ª∑ l·ªá bi·ªÉu c·∫£m mi·ªáng"
                        )
                        
                        pose_scale = st.slider(
                            "T·ª∑ l·ªá chuy·ªÉn ƒë·ªông ƒë·∫ßu (pitch, yaw, roll):",
                            min_value=0.5,
                            max_value=1.5,
                            value=1.0,
                            step=0.1,
                            help="ƒêi·ªÅu ch·ªânh t·ª∑ l·ªá chuy·ªÉn ƒë·ªông ƒë·∫ßu"
                        )
                        
                        # ƒêi·ªÅu ch·ªânh offset bi·ªÉu c·∫£m m√¥i
                        delta_exp_enabled = st.checkbox("Th√™m offset bi·ªÉu c·∫£m m√¥i", value=False)
                        if delta_exp_enabled:
                            delta_exp_value = st.slider(
                                "Gi√° tr·ªã offset:",
                                min_value=-0.2,
                                max_value=0.2,
                                value=0.0,
                                step=0.01
                            )
            
            # === Tab Caption Settings ===
            with input_tabs[2]:
                st.subheader("T√πy ch·ªânh ph·ª• ƒë·ªÅ")
                
                # Style ph·ª• ƒë·ªÅ
                caption_style = st.radio(
                    "Ki·ªÉu ph·ª• ƒë·ªÅ:",
                    ["Style 01 (t·ª´ng t·ª´)", "Style 02 (gradient)"],
                    horizontal=True,
                    index=0
                )
                
                # Auto fontsize
                auto_fontsize = st.checkbox(
                    "T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc ph·ª• ƒë·ªÅ",
                    value=st.session_state.auto_fontsize
                )
                st.session_state.auto_fontsize = auto_fontsize
                
                # T√≠nh to√°n fontsize
                fontsize = 48  # M·∫∑c ƒë·ªãnh
                if auto_fontsize and bg_file:
                    try:
                        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(bg_file.name).suffix) as temp:
                            temp.write(bg_file.getbuffer())
                            bg_temp_path = temp.name
                        
                        width, height = get_video_resolution(bg_temp_path)
                        
                        os.unlink(bg_temp_path)
                        
                        fontsize = calculate_auto_fontsize(width, height)
                        st.write(f"K√≠ch th∆∞·ªõc ph·ª• ƒë·ªÅ t·ª± ƒë·ªông: {fontsize}")
                    except Exception:
                        fontsize = 48
                elif auto_fontsize and bg_path:
                    try:
                        width, height = get_video_resolution(bg_path)
                        fontsize = calculate_auto_fontsize(width, height)
                        st.write(f"K√≠ch th∆∞·ªõc ph·ª• ƒë·ªÅ t·ª± ƒë·ªông: {fontsize}")
                    except Exception:
                        fontsize = 48
                else:
                    fontsize = st.slider("K√≠ch th∆∞·ªõc ph·ª• ƒë·ªÅ:", 24, 100, 48, 2)
                
                # C√†i ƒë·∫∑t cho Style 02
                caption_position, caption_zoom, zoom_size = "center", False, 0.01
                if caption_style == "Style 02 (gradient)":
                    caption_position = st.selectbox("V·ªã tr√≠ ph·ª• ƒë·ªÅ:", ["center", "top", "bottom"], index=0)
                    caption_zoom = st.checkbox("Hi·ªáu ·ª©ng zoom ph·ª• ƒë·ªÅ", value=True)
                    zoom_size = st.slider("ƒê·ªô l·ªõn hi·ªáu ·ª©ng zoom:", 0.005, 0.05, 0.01, 0.005) if caption_zoom else 0.01
            
            # N√∫t "T·∫°o Video"
            submitted = st.button(
                "üöÄ T·∫°o Video",
                use_container_width=True,
                type="primary",
                disabled=st.session_state.processing
            )
        
        # === C·ªôt hi·ªÉn th·ªã ti·∫øn tr√¨nh v√† k·∫øt qu·∫£ ===
        with col2:
            # T·∫°o c√°c placeholder cho UI tr·∫°ng th√°i
            elapsed_time_container = st.empty()
            status_container = st.empty()
            progress_container = st.empty()
            metrics_container = st.container()
            cancel_container = st.empty()
            log_container = st.container()
            
            if st.session_state.processing:
                status_container.subheader("‚è≥ ƒêang x·ª≠ l√Ω...")
                
                # Hi·ªÉn th·ªã th·ªùi gian x·ª≠ l√Ω
                if st.session_state.process_start_time:
                    elapsed = time.time() - st.session_state.process_start_time
                    elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
                
                progress = progress_container.progress(0)
                
                # N√∫t h·ªßy x·ª≠ l√Ω
                cancel_button = cancel_container.button("üõë H·ªßy x·ª≠ l√Ω", use_container_width=True)
                
                # Hi·ªÉn th·ªã logs
                if show_logs:
                    log_container.markdown("**Logs:**")
                    log_content = log_container.code("\n".join(st.session_state.logs[-20:]))
            
            elif st.session_state.complete and st.session_state.output_file:
                status_container.subheader("‚úÖ ƒê√£ ho√†n th√†nh!")
                
                output_file = st.session_state.output_file
                if Path(output_file).exists():
                    metrics_container.video(output_file)
                    
                    file_stats = Path(output_file).stat()
                    
                    # Hi·ªÉn th·ªã th√¥ng tin video
                    cols = metrics_container.columns(2)
                    cols[0].metric("K√≠ch th∆∞·ªõc", f"{file_stats.st_size / (1024*1024):.1f} MB")
                    cols[1].metric("Th·ªùi gian t·∫°o", datetime.fromtimestamp(file_stats.st_mtime).strftime("%H:%M:%S"))
                    
                    with open(output_file, "rb") as file:
                        cancel_container.download_button(
                            "üíæ T·∫£i xu·ªëng video",
                            file,
                            file_name=Path(output_file).name,
                            mime="video/mp4",
                            use_container_width=True
                        )
            else:
                status_container.subheader("Tr·∫°ng th√°i")
                metrics_container.info("Nh·∫•n n√∫t 'T·∫°o Video' ƒë·ªÉ b·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
            
            # Preview ch·ªâ hi·ªÉn th·ªã n·∫øu kh√¥ng ƒëang trong qu√° tr√¨nh x·ª≠ l√Ω ho·∫∑c ho√†n th√†nh
            if mc_file or (mc_path if 'mc_path' in locals() else None):
                preview_container = log_container.container()
                preview_container.subheader("MC Preview")
                if mc_file:
                    if Path(mc_file.name).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                        preview_container.image(mc_file, use_container_width=True)
                    else:
                        preview_container.video(mc_file)
                elif 'mc_path' in locals() and mc_path:
                    if Path(mc_path).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                        preview_container.image(mc_path, use_container_width=True)
                    else:
                        preview_container.video(mc_path)

    # === Tab 1: T·∫°o Video Khu√¥n M·∫∑t AI ===
    with tabs[1]:
        st.subheader("üé≠ T·∫°o Video Khu√¥n M·∫∑t N√≥i v·ªõi AI")
        st.write("Chuy·ªÉn ƒë·ªïi ·∫£nh ho·∫∑c video MC tƒ©nh th√†nh video v·ªõi kh·∫£ nƒÉng n√≥i theo audio")
        
        # Chia th√†nh 2 c·ªôt
        ai_col1, ai_col2 = st.columns([3, 2])
        
        with ai_col1:
            # Input files section
            st.subheader("T·∫£i l√™n files ƒë·∫ßu v√†o")
            
            # MC uploader
            ai_mc_file = st.file_uploader("T·∫£i l√™n ·∫¢nh/Video MC", type=["png", "jpg", "jpeg", "mp4"], key="ai_mc_file")
            if ai_mc_file:
                if Path(ai_mc_file.name).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    st.image(ai_mc_file, use_container_width=True, caption="MC Preview")
                else:
                    st.video(ai_mc_file)
            else:
                ai_mc_path = st.selectbox(
                    "Ho·∫∑c ch·ªçn file m·∫´u:",
                    options=[""] + [str(p) for p in Path("./example").glob("*.[jp][pn]g")] + [str(p) for p in Path("./example").glob("*mc*.mp4")],
                    format_func=lambda x: Path(x).name if x else "Ch·ªçn file m·∫´u...",
                    key="ai_mc_sample"
                )
                if ai_mc_path:
                    if Path(ai_mc_path).suffix.lower() in ['.jpg', '.jpeg', '.png']:
                        st.image(ai_mc_path, use_container_width=True, caption="MC Preview")
                    else:
                        st.video(ai_mc_path)
            
            # Audio source
            ai_audio_source = st.radio("Ngu·ªìn audio:", ["Upload file", "T·∫°o t·ª´ vƒÉn b·∫£n"], horizontal=True, key="ai_audio_source")
            
            if ai_audio_source == "Upload file":
                ai_audio_file = st.file_uploader("T·∫£i l√™n Audio tho·∫°i", type=["wav", "mp3"], key="ai_audio_file")
                if ai_audio_file:
                    st.audio(ai_audio_file)
                else:
                    ai_audio_path = st.selectbox(
                        "Ho·∫∑c ch·ªçn audio m·∫´u:",
                        options=[""] + [str(p) for p in Path("./example").glob("*.wav")] + [str(p) for p in Path("./example").glob("*.mp3")],
                        format_func=lambda x: Path(x).name if x else "Ch·ªçn audio m·∫´u...",
                        key="ai_audio_sample"
                    )
                    if ai_audio_path:
                        st.audio(ai_audio_path)
                ai_text_prompt = None
            else:  # T·∫°o t·ª´ vƒÉn b·∫£n
                ai_audio_file = None
                ai_audio_path = None
                ai_text_prompt = st.text_area("Nh·∫≠p vƒÉn b·∫£n tho·∫°i:", height=150, key="ai_text_prompt")
            
            # TTS settings
            if ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n":
                with st.expander("C√†i ƒë·∫∑t TTS", expanded=True):
                    ai_tts_service = st.selectbox(
                        "D·ªãch v·ª• TTS:",
                        options=["Edge TTS", "OpenAI TTS", "GPT-4o-mini-TTS"],
                        index=2,  # M·∫∑c ƒë·ªãnh ch·ªçn GPT-4o-mini-TTS
                        key="ai_tts_service"
                    )
                    
                    # Hi·ªÉn th·ªã c√°c t√πy ch·ªçn gi·ªçng n√≥i d·ª±a tr√™n d·ªãch v·ª•
                    if ai_tts_service == "Edge TTS":
                        ai_voice_options = ["vi-VN-NamMinhNeural", "vi-VN-HoaiMyNeural"]
                        
                        ai_tts_voice = st.selectbox(
                            "Gi·ªçng ƒë·ªçc:",
                            options=ai_voice_options,
                            index=0,
                            key="ai_tts_voice"
                        )
                        
                        # Hi·ªÉn th·ªã ƒëi·ªÅu ch·ªânh t·ªëc ƒë·ªô cho Edge TTS
                        ai_tts_speed = st.slider(
                            "T·ªëc ƒë·ªô ƒë·ªçc:",
                            min_value=0.8,
                            max_value=1.5,
                            value=1.2,
                            step=0.1,
                            key="ai_tts_speed"
                        )
                        
                        # ƒê·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh cho ai_tts_instructions
                        ai_tts_instructions = ""
                        
                    elif ai_tts_service == "OpenAI TTS":
                        ai_voice_options = ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
                        
                        ai_tts_voice = st.selectbox(
                            "Gi·ªçng ƒë·ªçc:",
                            options=ai_voice_options,
                            index=0,
                            key="ai_tts_voice"
                        )
                        
                        # Hi·ªÉn th·ªã ƒëi·ªÅu ch·ªânh t·ªëc ƒë·ªô cho OpenAI TTS
                        ai_tts_speed = st.slider(
                            "T·ªëc ƒë·ªô ƒë·ªçc:",
                            min_value=0.8,
                            max_value=1.5,
                            value=1.2,
                            step=0.1,
                            key="ai_tts_speed"
                        )
                        
                        # ƒê·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh cho ai_tts_instructions
                        ai_tts_instructions = ""
                        
                    else:  # GPT-4o-mini-TTS
                        ai_voice_options = ["Ash", "Ballad", "Coral", "Echo", "Fable", "Onyx", "Nova", "Sage", "Shimmer", "Verse"]
                        
                        st.write("**üîä Ch·ªçn gi·ªçng n√≥i v√† nghe th·ª≠:**")
                        
                        ai_tts_voice = st.selectbox(
                            "Gi·ªçng ƒë·ªçc:",
                            options=ai_voice_options,
                            index=ai_voice_options.index("Shimmer") if "Shimmer" in ai_voice_options else 0,
                            key="ai_tts_voice"
                        )
                        
                        # Hi·ªÉn th·ªã m√¥ t·∫£ ng·∫Øn g·ªçn c·ªßa gi·ªçng ƒë·ªçc ƒë√£ ch·ªçn
                        st.caption(f"**{ai_tts_voice}**: {VOICE_DESCRIPTIONS.get(ai_tts_voice, '')}")
                        
                        # Th√™m tr∆∞·ªùng h∆∞·ªõng d·∫´n gi·ªçng n√≥i
                        ai_tts_instructions = st.text_area(
                            "H∆∞·ªõng d·∫´n v·ªÅ gi·ªçng ƒëi·ªáu:",
                            value="""Tone: T·ª± nhi√™n, tr√¥i ch·∫£y, chuy√™n nghi·ªáp
Emotion: Nhi·ªát t√¨nh, t·ª± tin
Delivery: R√µ r√†ng, nh·ªãp ƒë·ªô v·ª´a ph·∫£i, nh·∫•n m·∫°nh t·ª´ kh√≥a quan tr·ªçng""",
                            height=100,
                            key="ai_tts_instructions"
                        )
                        
                        # T·∫°o m·∫´u vƒÉn b·∫£n ƒë·ªÉ nghe th·ª≠
                        if ai_text_prompt:
                            ai_sample_text = ai_text_prompt[:200] + "..." if len(ai_text_prompt) > 200 else ai_text_prompt
                        else:
                            ai_sample_text = "Xin ch√†o! ƒê√¢y l√† m·∫´u th·ª≠ gi·ªçng n√≥i t·ª´ GPT-4o. B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh vƒÉn b·∫£n n√†y ƒë·ªÉ nghe th·ª≠ tr∆∞·ªõc khi t·∫°o video."
                        
                        ai_preview_text = st.text_area(
                            "VƒÉn b·∫£n m·∫´u ƒë·ªÉ nghe th·ª≠:",
                            value=ai_sample_text,
                            height=80,
                            key="ai_preview_text"
                        )
                        
                        ai_preview_message = st.empty()
                        ai_preview_audio = st.empty()
                        
                        # N√∫t nghe th·ª≠
                        if st.button("üîä Nghe th·ª≠ gi·ªçng n√≥i", use_container_width=True, key="ai_tts_preview"):
                            if not ai_preview_text.strip():
                                ai_preview_message.warning("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n m·∫´u ƒë·ªÉ nghe th·ª≠")
                            else:
                                # S·ª≠ d·ª•ng asyncio
                                audio_bytes = asyncio.run(preview_audio_tts(
                                    ai_preview_text,
                                    ai_tts_instructions,
                                    ai_tts_voice,
                                    ai_preview_message
                                ))
                                
                                if audio_bytes:
                                    ai_preview_message.success("‚úÖ T·∫°o m·∫´u gi·ªçng n√≥i th√†nh c√¥ng!")
                                    ai_preview_audio.audio(audio_bytes, format="audio/mp3")
                        
                        # Hi·ªÉn th·ªã c√°c th√¥ng tin tham kh·∫£o v·ªÅ gi·ªçng n√≥i v√† h∆∞·ªõng d·∫´n (b√™n ngo√†i expander)
                        if ai_tts_service == "GPT-4o-mini-TTS":
                            st.divider()
                            st.subheader("üîä Tham kh·∫£o v·ªÅ gi·ªçng n√≥i GPT-4o-mini-TTS")
                            
                            ai_voice_info_col, ai_examples_col = st.columns(2)
                            
                            with ai_voice_info_col:
                                st.markdown("**ƒê·∫∑c ƒëi·ªÉm c·ªßa c√°c gi·ªçng n√≥i:**")
                                st.markdown("""
                                - **Ash**: Gi·ªçng nam tr∆∞·ªüng th√†nh, h∆°i tr·∫ßm, ph√π h·ª£p cho phim t√†i li·ªáu
                                - **Ballad**: Gi·ªçng n·ªØ m·ªÅm m·∫°i, ·∫•m √°p, ph√π h·ª£p cho n·ªôi dung t∆∞ v·∫•n
                                - **Coral**: Gi·ªçng n·ªØ tr·∫ª, r√µ r√†ng, t·ª± tin, ph√π h·ª£p cho n·ªôi dung gi√°o d·ª•c
                                - **Echo**: Gi·ªçng nam tr·∫ª, nƒÉng ƒë·ªông, ph√π h·ª£p cho qu·∫£ng c√°o
                                - **Fable**: Gi·ªçng nam uy t√≠n, ph√π h·ª£p cho th√¥ng b√°o ch√≠nh th·ª©c
                                - **Onyx**: Gi·ªçng nam tr·∫ßm, sang tr·ªçng, ph√π h·ª£p cho thuy·∫øt tr√¨nh
                                - **Nova**: Gi·ªçng n·ªØ chuy√™n nghi·ªáp, ph√π h·ª£p cho tin t·ª©c
                                - **Sage**: Gi·ªçng n·ªØ t·ª´ng tr·∫£i, ·∫•m √°p, ph√π h·ª£p cho podcast
                                - **Shimmer**: Gi·ªçng n·ªØ t∆∞∆°i s√°ng, nƒÉng ƒë·ªông, ph√π h·ª£p cho gi·∫£i tr√≠
                                - **Verse**: Gi·ªçng nam t·ª± nhi√™n, c√¢n b·∫±ng, ph√π h·ª£p cho ƒëa d·∫°ng n·ªôi dung
                                """)
                            
                            with ai_examples_col:
                                st.markdown("**V√≠ d·ª• v·ªÅ h∆∞·ªõng d·∫´n gi·ªçng n√≥i:**")
                                st.markdown("""
                                **Gi·ªçng di·ªÖn thuy·∫øt:**
                                ```
                                Tone: ƒêƒ©nh ƒë·∫°c, trang tr·ªçng, ƒë·∫ßy t·ª± tin
                                Emotion: Nhi·ªát huy·∫øt, quy·∫øt ƒëo√°n
                                Delivery: Nh·ªãp ƒë·ªô v·ª´a ph·∫£i v·ªõi c√°c ng·∫Øt qu√£ng, nh·∫•n m·∫°nh t·ª´ kh√≥a quan tr·ªçng
                                ```
                                
                                **Gi·ªçng t∆∞ v·∫•n:**
                                ```
                                Tone: ·∫§m √°p, th√¢n thi·ªán, ƒë√°ng tin c·∫≠y
                                Emotion: Th·∫•u hi·ªÉu, quan t√¢m
                                Delivery: Nh·∫π nh√†ng, r√µ r√†ng, t·∫°o c·∫£m gi√°c an t√¢m
                                ```
                                
                                **Gi·ªçng qu·∫£ng c√°o:**
                                ```
                                Tone: S√¥i n·ªïi, cu·ªën h√∫t, nƒÉng ƒë·ªông
                                Emotion: Ph·∫•n kh√≠ch, h√†o h·ª©ng
                                Delivery: Nhanh, ƒë·∫ßy nƒÉng l∆∞·ª£ng, v·ªõi c∆∞·ªùng ƒë·ªô tƒÉng d·∫ßn
                                ```
                                """)
            
            # C√†i ƒë·∫∑t AI model
            with st.expander("C√†i ƒë·∫∑t m√¥ h√¨nh AI", expanded=False):
                ai_model = st.selectbox(
                    "M√¥ h√¨nh AI:",
                    options=["M√¥ h√¨nh m·∫∑c ƒë·ªãnh", "M√¥ h√¨nh t·ªëi ∆∞u h√≥a"],
                    help="Ch·ªçn m√¥ h√¨nh AI ƒë·ªÉ t·∫°o video khu√¥n m·∫∑t n√≥i",
                    index=0,
                    key="ai_model"
                )
                
                ai_quality = st.select_slider(
                    "Ch·∫•t l∆∞·ª£ng video AI:",
                    options=["low", "medium", "high"],
                    value="medium",
                    key="ai_quality"
                )
                
                ai_advanced = st.checkbox("C√†i ƒë·∫∑t n√¢ng cao", value=False, key="ai_advanced")
                if ai_advanced:
                    ai_inference_steps = st.slider(
                        "S·ªë b∆∞·ªõc inference:",
                        min_value=5,
                        max_value=20,
                        value=10,
                        step=1,
                        key="ai_inference_steps"
                    )
            
            # T√πy ch·ªânh kh·∫©u h√¨nh AI (ri√™ng bi·ªát, kh√¥ng l·ªìng trong expander kh√°c)
            with st.expander("üó£Ô∏è T√πy ch·ªânh kh·∫©u h√¨nh", expanded=False):
                # ƒêi·ªÅu ch·ªânh m·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i
                ai_vad_alpha = st.slider(
                    "M·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i:",
                    min_value=0.0,
                    max_value=1.0,
                    value=1.0,
                    step=0.05,
                    key="ai_vad_alpha",
                    help="Gi√° tr·ªã th·∫•p h∆°n s·∫Ω l√†m gi·∫£m chuy·ªÉn ƒë·ªông m√¥i, gi√° tr·ªã cao h∆°n s·∫Ω tƒÉng chuy·ªÉn ƒë·ªông"
                )
                
                # T√πy ch·ªçn n√¢ng cao
                ai_mouth_advanced = st.checkbox("T√πy ch·ªçn n√¢ng cao cho kh·∫©u h√¨nh", value=False, key="ai_mouth_advanced")
                if ai_mouth_advanced:
                    # Ch·ªçn c√°c th√†nh ph·∫ßn bi·ªÉu c·∫£m
                    ai_exp_components = st.multiselect(
                        "Th√†nh ph·∫ßn bi·ªÉu c·∫£m:",
                        options=["exp", "pitch", "yaw", "roll", "t"],
                        default=["exp", "pitch", "yaw", "roll", "t"],
                        key="ai_exp_components",
                        help="Ch·ªçn c√°c th√†nh ph·∫ßn bi·ªÉu c·∫£m ƒë·ªÉ s·ª≠ d·ª•ng t·ª´ m√¥ h√¨nh"
                    )
                    
                    # ƒêi·ªÅu ch·ªânh t·ª∑ l·ªá cho c√°c th√†nh ph·∫ßn
                    ai_exp_scale = st.slider(
                        "T·ª∑ l·ªá bi·ªÉu c·∫£m mi·ªáng (exp):",
                        min_value=0.5,
                        max_value=1.5,
                        value=1.0,
                        step=0.1,
                        key="ai_exp_scale",
                        help="ƒêi·ªÅu ch·ªânh t·ª∑ l·ªá bi·ªÉu c·∫£m mi·ªáng"
                    )
                    
                    ai_pose_scale = st.slider(
                        "T·ª∑ l·ªá chuy·ªÉn ƒë·ªông ƒë·∫ßu (pitch, yaw, roll):",
                        min_value=0.5,
                        max_value=1.5,
                        value=1.0,
                        step=0.1,
                        key="ai_pose_scale",
                        help="ƒêi·ªÅu ch·ªânh t·ª∑ l·ªá chuy·ªÉn ƒë·ªông ƒë·∫ßu"
                    )
                    
                    # ƒêi·ªÅu ch·ªânh offset bi·ªÉu c·∫£m m√¥i
                    ai_delta_exp_enabled = st.checkbox("Th√™m offset bi·ªÉu c·∫£m m√¥i", value=False, key="ai_delta_exp_enabled")
                    if ai_delta_exp_enabled:
                        ai_delta_exp_value = st.slider(
                            "Gi√° tr·ªã offset:",
                            min_value=-0.2,
                            max_value=0.2,
                            value=0.0,
                            step=0.01,
                            key="ai_delta_exp_value"
                        )
            
            # N√∫t "T·∫°o Video Khu√¥n M·∫∑t N√≥i"
            ai_submitted = st.button(
                "üöÄ T·∫°o Video Khu√¥n M·∫∑t N√≥i",
                use_container_width=True,
                type="primary",
                key="ai_create_button"
            )
        
        # C·ªôt hi·ªÉn th·ªã ti·∫øn tr√¨nh v√† k·∫øt qu·∫£
        with ai_col2:
            # T·∫°o c√°c placeholder cho UI tr·∫°ng th√°i
            ai_elapsed_time_container = st.empty()
            ai_status_container = st.empty()
            ai_progress_container = st.empty()
            ai_metrics_container = st.container()
            ai_cancel_container = st.empty()
            ai_result_container = st.container()
            
            if st.session_state.processing:
                ai_status_container.subheader("‚è≥ ƒêang x·ª≠ l√Ω...")
                
                # Hi·ªÉn th·ªã th·ªùi gian x·ª≠ l√Ω
                if st.session_state.process_start_time:
                    elapsed = time.time() - st.session_state.process_start_time
                    ai_elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
                
                progress = ai_progress_container.progress(0)
                
                # N√∫t h·ªßy x·ª≠ l√Ω
                cancel_button = ai_cancel_container.button("üõë H·ªßy x·ª≠ l√Ω", key="ai_cancel_processing", use_container_width=True)
            elif st.session_state.complete and st.session_state.output_file:
                ai_status_container.subheader("‚úÖ ƒê√£ ho√†n th√†nh!")
                
                output_file = st.session_state.output_file
                if Path(output_file).exists():
                    ai_metrics_container.video(output_file)
                    
                    file_stats = Path(output_file).stat()
                    
                    # Hi·ªÉn th·ªã th√¥ng tin video
                    cols = ai_metrics_container.columns(2)
                    cols[0].metric("K√≠ch th∆∞·ªõc", f"{file_stats.st_size / (1024*1024):.1f} MB")
                    cols[1].metric("Th·ªùi gian t·∫°o", datetime.fromtimestamp(file_stats.st_mtime).strftime("%H:%M:%S"))
                    
                    with open(output_file, "rb") as file:
                        ai_cancel_container.download_button(
                            "üíæ T·∫£i xu·ªëng video",
                            file,
                            file_name=Path(output_file).name,
                            mime="video/mp4",
                            use_container_width=True,
                            key="ai_download_button"
                        )
            else:
                # Hi·ªÉn th·ªã tr·∫°ng th√°i ban ƒë·∫ßu
                ai_status_container.subheader("Tr·∫°ng th√°i")
                ai_metrics_container.info("Nh·∫•n n√∫t 'T·∫°o Video Khu√¥n M·∫∑t N√≥i' ƒë·ªÉ b·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
        
        # X·ª≠ l√Ω khi nh·∫•n n√∫t t·∫°o video
        if ai_submitted and not st.session_state.processing:
            # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n files
            mc_path_final = ai_mc_file if ai_mc_file else ai_mc_path if 'ai_mc_path' in locals() and ai_mc_path else None
            audio_path_final = ai_audio_file if ai_audio_file else ai_audio_path if 'ai_audio_path' in locals() and ai_audio_path else None
            
            if (mc_path_final and (audio_path_final or ai_text_prompt)):
                # Chu·∫©n b·ªã workflow ch·ªâ cho talking head generation
                ai_workflow_steps = {k: False for k in WORKFLOW_STEPS}
                ai_workflow_steps["prepare_files"] = True
                ai_workflow_steps["tts_generation"] = ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n"
                ai_workflow_steps["talking_head_generation"] = True
                
                # C√†i ƒë·∫∑t session state v√† UI
                st.session_state.processing = True
                st.session_state.process_start_time = time.time()
                st.session_state.logs = ["B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o video khu√¥n m·∫∑t n√≥i..."]
                
                # C·∫≠p nh·∫≠t UI tr·∫°ng th√°i ban ƒë·∫ßu
                ai_status_container.subheader("‚è≥ ƒêang x·ª≠ l√Ω...")
                elapsed = time.time() - st.session_state.process_start_time
                ai_elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
                progress = ai_progress_container.progress(0)
                cancel_button = ai_cancel_container.button("üõë H·ªßy x·ª≠ l√Ω", key="ai_cancel_processing", use_container_width=True)
                
                # Chu·∫©n b·ªã tempdir v√† ƒë∆∞·ªùng d·∫´n
                temp_dir = Path(tempfile.mkdtemp())
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                
                # Chu·∫©n b·ªã h√†ng ƒë·ª£i giao ti·∫øp
                msg_queue = queue.Queue()
                cancel_event = threading.Event()
                
                # Chu·∫©n b·ªã containers cho handler
                ui_containers = {
                    'status': ai_status_container,
                    'progress': progress,
                    'log_content': None,  # Kh√¥ng hi·ªÉn th·ªã log tr√™n tab n√†y
                    'metrics': ai_metrics_container
                }
                
                # Chu·∫©n b·ªã c√°c tham s·ªë TTS
                tts_service_val = ai_tts_service if ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" else "Edge TTS"
                tts_voice_val = ai_tts_voice if ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" else "vi-VN-NamMinhNeural"
                tts_speed_val = ai_tts_speed if ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" and ai_tts_service != "GPT-4o-mini-TTS" else 1.2
                tts_instructions_val = ai_tts_instructions if ai_audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" and ai_tts_service == "GPT-4o-mini-TTS" else ""
                
                # Kh·ªüi ch·∫°y thread x·ª≠ l√Ω v·ªõi workflow ch·ªâ d√†nh cho AI talking head
                thread = threading.Thread(
                    target=process_video,
                    args=(
                        ai_workflow_steps,
                        mc_path_final,
                        None,
                        audio_path_final,
                        ai_text_prompt,
                        temp_dir,
                        msg_queue,
                        cancel_event,
                        editor,
                        timestamp,
                        tts_service_val,
                        tts_voice_val,
                        tts_speed_val,
                    ),
                    kwargs={
                        'tts_instructions_val': tts_instructions_val,
                        'ai_model_val': ai_model,  # Truy·ªÅn l·ª±a ch·ªçn m√¥ h√¨nh AI
                        'quality_val': ai_quality,
                        # Th√™m c√°c tham s·ªë kh·∫©u h√¨nh
                        'vad_alpha': ai_vad_alpha if 'ai_vad_alpha' in locals() else 1.0,
                        'exp_components': ai_exp_components if 'ai_exp_components' in locals() and ai_mouth_advanced else None,
                        'exp_scale': ai_exp_scale if 'ai_exp_scale' in locals() and ai_mouth_advanced else 1.0,
                        'pose_scale': ai_pose_scale if 'ai_pose_scale' in locals() and ai_mouth_advanced else 1.0,
                        'delta_exp_enabled': ai_delta_exp_enabled if 'ai_delta_exp_enabled' in locals() and ai_mouth_advanced else False,
                        'delta_exp_value': ai_delta_exp_value if 'ai_delta_exp_value' in locals() and ai_delta_exp_enabled and ai_mouth_advanced else 0.0,
                    }
                )
                thread.daemon = True
                thread.start()
                
                # UI theo d√µi ti·∫øn tr√¨nh (t∆∞∆°ng t·ª± nh∆∞ tab ch√≠nh)
                try:
                    while thread.is_alive() or not msg_queue.empty():
                        # C·∫≠p nh·∫≠t th·ªùi gian x·ª≠ l√Ω
                        if st.session_state.process_start_time:
                            elapsed = time.time() - st.session_state.process_start_time
                            ai_elapsed_time_container.caption(
                                f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}"
                            )
                        
                        # Ki·ªÉm tra n·∫øu ƒë√£ nh·∫•n n√∫t h·ªßy
                        if cancel_button:
                            cancel_event.set()
                            st.session_state.processing = False
                            st.warning("ƒê√£ h·ªßy qu√° tr√¨nh x·ª≠ l√Ω")
                            break
                        
                        # X·ª≠ l√Ω th√¥ng ƒëi·ªáp
                        try:
                            msg_type, content = msg_queue.get(timeout=0.1)
                            handle_message(msg_type, content, ui_containers, False)  # Kh√¥ng hi·ªÉn th·ªã logs
                            msg_queue.task_done()
                            
                            if msg_type == 'complete':
                                time.sleep(0.5)  # ƒê·ª£i UI c·∫≠p nh·∫≠t
                                st.rerun()
                        except queue.Empty:
                            time.sleep(0.1)
                        
                        # Th√™m ƒë·ªô tr·ªÖ ƒë·ªÉ tr√°nh qu√° t·∫£i UI
                        time.sleep(0.05)
                except Exception as e:
                    st.error(f"L·ªói UI: {str(e)}")
                    st.session_state.processing = False
            else:
                st.error("Vui l√≤ng t·∫£i l√™n c·∫£ MC v√† audio (ho·∫∑c nh·∫≠p vƒÉn b·∫£n)")

    # === Tab 2: Quy Tr√¨nh ===
    with tabs[2]:
        st.subheader("‚öôÔ∏è C·∫•u H√¨nh Quy Tr√¨nh")
        
        st.write("Ch·ªçn c√°c b∆∞·ªõc x·ª≠ l√Ω c·∫ßn th·ª±c hi·ªán. C√°c b∆∞·ªõc kh√¥ng ƒë∆∞·ª£c ch·ªçn s·∫Ω b·ªã b·ªè qua trong qu√° tr√¨nh x·ª≠ l√Ω.")
        
        # Chia th√†nh 2 c·ªôt ƒë·ªÉ hi·ªÉn th·ªã checkbox
        left_col, right_col = st.columns(2)
        
        # ƒê·∫£m b·∫£o workflow_steps c√≥ trong session state
        workflow_steps_dict = {}
        
        # Hi·ªÉn th·ªã c√°c checkbox trong hai c·ªôt ƒë·ªÉ giao di·ªán c√¢n ƒë·ªëi
        steps = list(WORKFLOW_STEPS.items())
        mid_idx = len(steps) // 2 + len(steps) % 2
        
        # C·ªôt tr√°i
        with left_col:
            for step_id, step_name in steps[:mid_idx]:
                # L·∫•y gi√° tr·ªã hi·ªán t·∫°i t·ª´ session state m·ªôt c√°ch an to√†n
                current_value = True
                if 'workflow_steps' in st.session_state:
                    if isinstance(st.session_state.workflow_steps, dict):
                        current_value = st.session_state.workflow_steps.get(step_id, True)
                
                workflow_steps_dict[step_id] = st.checkbox(
                    step_name,
                    value=current_value,
                    key=f"workflow_{step_id}"
                )
        
        # C·ªôt ph·∫£i
        with right_col:
            for step_id, step_name in steps[mid_idx:]:
                # L·∫•y gi√° tr·ªã hi·ªán t·∫°i t·ª´ session state m·ªôt c√°ch an to√†n
                current_value = True
                if 'workflow_steps' in st.session_state:
                    if isinstance(st.session_state.workflow_steps, dict):
                        current_value = st.session_state.workflow_steps.get(step_id, True)
                
                workflow_steps_dict[step_id] = st.checkbox(
                    step_name,
                    value=current_value,
                    key=f"workflow_{step_id}"
                )
        
        # C·∫≠p nh·∫≠t workflow_steps trong session state
        st.session_state.workflow_steps = workflow_steps_dict
        
        # Th√™m m√¥ t·∫£ chi ti·∫øt
        with st.expander("‚ÑπÔ∏è Chi ti·∫øt c√°c b∆∞·ªõc x·ª≠ l√Ω", expanded=False):
            st.markdown("""
            **M√¥ t·∫£ chi ti·∫øt t·ª´ng b∆∞·ªõc:**
            
            **Chu·∫©n b·ªã files**: Chu·∫©n b·ªã v√† sao ch√©p c√°c file ƒë·∫ßu v√†o ƒë·ªÉ x·ª≠ l√Ω
            
            **T·∫°o √¢m thanh t·ª´ vƒÉn b·∫£n**: S·ª≠ d·ª•ng c√¥ng ngh·ªá TTS ƒë·ªÉ t·∫°o √¢m thanh t·ª´ vƒÉn b·∫£n nh·∫≠p v√†o
            
            **T·∫°o ph·ª• ƒë·ªÅ t·ª´ audio**: Ph√¢n t√≠ch audio v√† t·∫°o file ph·ª• ƒë·ªÅ (.srt)
            
            **T·∫°o video khu√¥n m·∫∑t n√≥i**: S·ª≠ d·ª•ng AI ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng n√≥i cho MC d·ª±a tr√™n audio
            
            **Gh√©p video MC v√† n·ªÅn**: Gh√©p video MC ƒë√£ t·∫°o v√†o video n·ªÅn v·ªõi v·ªã tr√≠ ƒë√£ ch·ªçn
            
            **Th√™m ph·ª• ƒë·ªÅ v√†o video**: √Åp d·ª•ng ph·ª• ƒë·ªÅ v√†o video cu·ªëi c√πng v·ªõi hi·ªáu ·ª©ng ƒë√£ ch·ªçn
            """)
        
        # N√∫t reset t·∫•t c·∫£ c√°c b∆∞·ªõc
        if st.button("‚Ü©Ô∏è Kh√¥i ph·ª•c t·∫•t c·∫£ b∆∞·ªõc", use_container_width=True):
            st.session_state.workflow_steps = {k: True for k in WORKFLOW_STEPS}
            st.rerun()
        
        # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ c√°c ƒëi·ªÅu ki·ªán ph·ª• thu·ªôc
        st.info("**L∆∞u √Ω:** M·ªôt s·ªë b∆∞·ªõc ph·ª• thu·ªôc v√†o c√°c b∆∞·ªõc tr∆∞·ªõc ƒë√≥. Khi b·ªè qua m·ªôt b∆∞·ªõc, h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông d√πng d·ªØ li·ªáu m·∫´u ho·∫∑c k·∫øt qu·∫£ c√≥ s·∫µn.")

    # === Tab 3: L·ªãch S·ª≠ ===
    with tabs[3]:
        st.subheader("L·ªãch s·ª≠ video ƒë√£ t·∫°o")
        
        # C·∫≠p nh·∫≠t l·ªãch s·ª≠ t·ª´ th∆∞ m·ª•c
        update_history_from_folder()
        
        if not st.session_state.history:
            st.info("Ch∆∞a c√≥ video n√†o ƒë∆∞·ª£c t·∫°o.")
        else:
            # T√πy ch·ªçn s·∫Øp x·∫øp v√† t√¨m ki·∫øm
            col1, col2 = st.columns(2)
            with col1:
                sort_option = st.selectbox(
                    "S·∫Øp x·∫øp theo:",
                    ["Th·ªùi gian t·∫°o (m·ªõi nh·∫•t)", "Th·ªùi gian t·∫°o (c≈© nh·∫•t)", "K√≠ch th∆∞·ªõc (l·ªõn nh·∫•t)", "K√≠ch th∆∞·ªõc (nh·ªè nh·∫•t)"],
                    index=0
                )
                
            with col2:
                search_term = st.text_input("T√¨m ki·∫øm:", placeholder="Nh·∫≠p t√™n file...")
            
            # S·∫Øp x·∫øp v√† l·ªçc l·ªãch s·ª≠
            history = sorted(
                st.session_state.history,
                key=lambda x: x.get('created', datetime.now()) if sort_option.startswith("Th·ªùi gian") else x.get('size', 0),
                reverse=sort_option in ["Th·ªùi gian t·∫°o (m·ªõi nh·∫•t)", "K√≠ch th∆∞·ªõc (l·ªõn nh·∫•t)"]
            )
            
            # L·ªçc theo t·ª´ kh√≥a
            if search_term:
                history = [item for item in history if search_term.lower() in Path(item.get('path', '')).name.lower()]
            
            # Hi·ªÉn th·ªã danh s√°ch
            for i, item in enumerate(history):
                file_path = Path(item['path'])
                if not file_path.exists():
                    continue
                
                with st.expander(f"{file_path.name} ({item['created'].strftime('%Y-%m-%d %H:%M:%S')})", expanded=i==0):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.video(str(file_path))
                    
                    with col2:
                        st.write(f"K√≠ch th∆∞·ªõc: {item['size']:.1f} MB")
                        st.write(f"Th·ªùi gian t·∫°o: {item['created'].strftime('%Y-%m-%d %H:%M:%S')}")
                    
                    buttons_col1, buttons_col2 = st.columns(2)
                    with buttons_col1:
                        with open(file_path, "rb") as f:
                            st.download_button(
                                "üíæ T·∫£i xu·ªëng",
                                f,
                                file_name=file_path.name,
                                mime="video/mp4",
                                key=f"download_{file_path.name}",
                                use_container_width=True
                            )
                    
                    with buttons_col2:
                        if st.button("üóëÔ∏è X√≥a video", key=f"delete_{file_path.name}", use_container_width=True):
                            try:
                                file_path.unlink()
                                st.session_state.history = [h for h in st.session_state.history if h['path'] != str(file_path)]
                                st.success(f"ƒê√£ x√≥a {file_path.name}")
                                st.rerun()
                            except Exception as e:
                                st.error(f"Kh√¥ng th·ªÉ x√≥a file: {str(e)}")

        # === Tab 4: H∆∞·ªõng D·∫´n ===
    with tabs[4]:
        st.subheader("H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng")
        
        # C√°c ph·∫ßn h∆∞·ªõng d·∫´n
        with st.expander("üöÄ B·∫Øt ƒë·∫ßu nhanh", expanded=True):
            st.markdown("""
            **C√°c b∆∞·ªõc c∆° b·∫£n ƒë·ªÉ t·∫°o video MC:**
            
            - 1. **T·∫£i l√™n ho·∫∑c ch·ªçn file MC**
              - H√¨nh ·∫£nh ho·∫∑c video c√≥ nh√¢n v·∫≠t n√≥i
            - 2. **T·∫£i l√™n ho·∫∑c ch·ªçn video n·ªÅn**
              - Video n·ªÅn cho MC
            - 3. **Ch·ªçn ngu·ªìn audio**
              - T·∫£i l√™n file audio ho·∫∑c t·∫°o m·ªõi t·ª´ vƒÉn b·∫£n
            - 4. **ƒêi·ªÅu ch·ªânh c√†i ƒë·∫∑t MC**
              - V·ªã tr√≠ v√† k√≠ch th∆∞·ªõc MC tr√™n video
            - 5. **ƒêi·ªÅu ch·ªânh c√†i ƒë·∫∑t ph·ª• ƒë·ªÅ**
              - Ch·ªçn ki·ªÉu v√† k√≠ch th∆∞·ªõc ph·ª• ƒë·ªÅ
            - 6. **Nh·∫•n n√∫t "T·∫°o Video"**
              - Ch·ªù x·ª≠ l√Ω v√† t·∫£i xu·ªëng k·∫øt qu·∫£
            """)
        
        with st.expander("‚öôÔ∏è C√°c t√≠nh nƒÉng n√¢ng cao", expanded=False):
            st.markdown("""
            **T√≠nh nƒÉng t·ª± ƒë·ªông h√≥a:**
            - T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc MC d·ª±a tr√™n t·ª∑ l·ªá video n·ªÅn
            - T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc ph·ª• ƒë·ªÅ ph√π h·ª£p v·ªõi ƒë·ªô ph√¢n gi·∫£i
            
            **Ki·ªÉu ph·ª• ƒë·ªÅ:**
            - Style 01: Hi·ªÉn th·ªã t·ª´ng t·ª´ m·ªôt v·ªõi m√†u s·∫Øc thay ƒë·ªïi
            - Style 02: Ph·ª• ƒë·ªÅ c√≥ hi·ªáu ·ª©ng m√†u gradient v√† zoom nh·∫π
            
            **C·∫•u h√¨nh quy tr√¨nh:**
            - Ch·ªçn c√°c b∆∞·ªõc c·ª• th·ªÉ ƒë·ªÉ t√πy ch·ªânh quy tr√¨nh x·ª≠ l√Ω
            - B·ªè qua c√°c b∆∞·ªõc kh√¥ng c·∫ßn thi·∫øt ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian
            
            **Video Khu√¥n M·∫∑t AI:**
            - S·ª≠ d·ª•ng tab chuy√™n bi·ªát ƒë·ªÉ t·∫°o video khu√¥n m·∫∑t n√≥i m√† kh√¥ng c·∫ßn video n·ªÅn
            - Ch·ªçn gi·ªØa m√¥ h√¨nh m·∫∑c ƒë·ªãnh v√† m√¥ h√¨nh t·ªëi ∆∞u h√≥a ƒë·ªÉ ƒë·∫°t k·∫øt qu·∫£ t·ªët nh·∫•t
            
            **GPT-4o-mini-TTS:**
            - T·∫°o √¢m thanh v·ªõi c·∫£m x√∫c v√† gi·ªçng ƒëi·ªáu phong ph√∫
            - Ch·ªçn t·ª´ 10 gi·ªçng ƒë·ªçc kh√°c nhau (Ash, Ballad, Coral, Echo, Fable, Onyx, Nova, Sage, Shimmer, Verse)
            - T√πy ch·ªânh h∆∞·ªõng d·∫´n v·ªÅ t√¥ng gi·ªçng, c·∫£m x√∫c v√† c√°ch truy·ªÅn ƒë·∫°t
            - Nghe th·ª≠ tr∆∞·ªõc khi t·∫°o video ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng
            
            **ƒêi·ªÅu ch·ªânh kh·∫©u h√¨nh mi·ªáng:**
            - ƒêi·ªÅu ch·ªânh m·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông m√¥i theo √¢m thanh
            - T√πy ch·ªânh th√†nh ph·∫ßn bi·ªÉu c·∫£m v√† t·ª∑ l·ªá √°p d·ª•ng
            - Th√™m offset bi·ªÉu c·∫£m m√¥i ƒë·ªÉ ƒëi·ªÅu ch·ªânh h√¨nh d√°ng mi·ªáng
            """)
        
        with st.expander("üîç X·ª≠ l√Ω s·ª± c·ªë", expanded=False):
            st.markdown("""
            **V·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p:**
            1. **X·ª≠ l√Ω ch·∫≠m:** Gi·∫£m k√≠ch th∆∞·ªõc/ƒë·ªô ph√¢n gi·∫£i file ƒë·∫ßu v√†o
            2. **L·ªói khi x·ª≠ l√Ω video:** Ki·ªÉm tra ƒë·ªãnh d·∫°ng file
            3. **Ph·ª• ƒë·ªÅ kh√¥ng ƒë·ªìng b·ªô:** Ki·ªÉm tra audio c√≥ r√µ r√†ng kh√¥ng
            
            **N·∫øu g·∫∑p l·ªói:**
            - Xem logs ƒë·ªÉ bi·∫øt chi ti·∫øt
            - Th·ª≠ s·ª≠ d·ª•ng file m·∫´u ƒë·ªÉ x√°c ƒë·ªãnh v·∫•n ƒë·ªÅ
            """)

    # === X·ª≠ l√Ω khi submit ===
    if submitted and not st.session_state.processing:
        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c√°c files
        mc_path_final = mc_file if mc_file else mc_path if 'mc_path' in locals() and mc_path else None
        bg_path_final = bg_file if bg_file else bg_path if 'bg_path' in locals() and bg_path else None
        audio_path_final = audio_file if audio_file else audio_path if 'audio_path' in locals() and audio_path else None
        
        # L·∫•y workflow_steps t·ª´ session_state
        if 'workflow_steps' not in st.session_state:
            workflow_steps = {k: True for k in WORKFLOW_STEPS}
        else:
            workflow_steps = st.session_state.workflow_steps.copy() if isinstance(st.session_state.workflow_steps, dict) else {k: True for k in WORKFLOW_STEPS}
        
        if ((mc_path_final and bg_path_final) and (audio_path_final or text_prompt)):
            st.session_state.processing = True
            st.session_state.process_start_time = time.time()
            st.session_state.logs = ["B·∫Øt ƒë·∫ßu qu√° tr√¨nh x·ª≠ l√Ω..."]
            
            # C·∫≠p nh·∫≠t UI tr·∫°ng th√°i ban ƒë·∫ßu
            status_container.subheader("‚è≥ ƒêang x·ª≠ l√Ω...")
            elapsed = time.time() - st.session_state.process_start_time
            elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
            progress = progress_container.progress(0)
            cancel_button = cancel_container.button("üõë H·ªßy x·ª≠ l√Ω", key="cancel_processing", use_container_width=True)
            if show_logs:
                log_container.markdown("**Logs:**")
                log_content = log_container.code("ƒêang b·∫Øt ƒë·∫ßu...")
            
            # T·∫°o tempdir v√† ƒë∆∞·ªùng d·∫´n
            temp_dir = Path(tempfile.mkdtemp())
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Chu·∫©n b·ªã h√†ng ƒë·ª£i giao ti·∫øp
            msg_queue = queue.Queue()
            
            # S·ª≠ d·ª•ng threading.Event thay v√¨ truy c·∫≠p st.session_state t·ª´ lu·ªìng ph·ª•
            cancel_event = threading.Event()
            
            # Chu·∫©n b·ªã containers cho handler
            ui_containers = {
                'status': status_container,
                'progress': progress,
                'log_content': log_content if show_logs else None,
                'metrics': metrics_container
            }
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ truy·ªÅn cho thread
            # Tr√≠ch xu·∫•t c√°c gi√° tr·ªã c·∫ßn thi·∫øt t·ª´ UI ƒë·ªÉ truy·ªÅn v√†o thread
            tts_service_val = tts_service if audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" else "Edge TTS"
            tts_voice_val = tts_voice if audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" else "vi-VN-NamMinhNeural"
            tts_speed_val = tts_speed if audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" and tts_service != "GPT-4o-mini-TTS" else 1.2
            tts_instructions_val = tts_instructions if audio_source == "T·∫°o t·ª´ vƒÉn b·∫£n" and tts_service == "GPT-4o-mini-TTS" else ""
            caption_style_val = caption_style
            fontsize_val = fontsize
            caption_position_val = caption_position
            caption_zoom_val = caption_zoom
            zoom_size_val = zoom_size
            quality_val = quality
            
            # Kh·ªüi ch·∫°y thread x·ª≠ l√Ω v·ªõi tham s·ªë l√† dictionary workflow_steps
            thread = threading.Thread(
                target=process_video,
                args=(
                    workflow_steps,
                    mc_path_final,
                    bg_path_final,
                    audio_path_final,
                    text_prompt,
                    temp_dir,
                    msg_queue,
                    cancel_event,
                    editor,
                    timestamp,
                    tts_service_val,
                    tts_voice_val,
                    tts_speed_val,
                ),
                kwargs={
                    'tts_instructions_val': tts_instructions_val,
                    'position_val': position,
                    'scale_val': scale,
                    'caption_style_val': caption_style_val,
                    'fontsize_val': fontsize_val,
                    'caption_position_val': caption_position_val,
                    'caption_zoom_val': caption_zoom_val,
                    'zoom_size_val': zoom_size_val,
                    'quality_val': quality_val,
                    'ai_model_val': "M√¥ h√¨nh m·∫∑c ƒë·ªãnh",  # M·∫∑c ƒë·ªãnh cho tab ch√≠nh
                    # Th√™m c√°c tham s·ªë kh·∫©u h√¨nh
                    'vad_alpha': vad_alpha if 'vad_alpha' in locals() else 1.0,
                    'exp_components': exp_components if 'exp_components' in locals() and mouth_advanced else None,
                    'exp_scale': exp_scale if 'exp_scale' in locals() and mouth_advanced else 1.0,
                    'pose_scale': pose_scale if 'pose_scale' in locals() and mouth_advanced else 1.0,
                    'delta_exp_enabled': delta_exp_enabled if 'delta_exp_enabled' in locals() and mouth_advanced else False,
                    'delta_exp_value': delta_exp_value if 'delta_exp_value' in locals() and delta_exp_enabled and mouth_advanced else 0.0,
                }
            )
            thread.daemon = True
            thread.start()
            
            # UI theo d√µi ti·∫øn tr√¨nh
            try:
                while thread.is_alive() or not msg_queue.empty():
                    # C·∫≠p nh·∫≠t th·ªùi gian x·ª≠ l√Ω
                    if st.session_state.process_start_time:
                        elapsed = time.time() - st.session_state.process_start_time
                        elapsed_time_container.caption(f"Th·ªùi gian x·ª≠ l√Ω: {int(elapsed // 60):02d}:{int(elapsed % 60):02d}")
                    
                    # Ki·ªÉm tra n·∫øu ƒë√£ nh·∫•n n√∫t h·ªßy
                    if cancel_button:
                        cancel_event.set()
                        st.session_state.processing = False
                        st.warning("ƒê√£ h·ªßy qu√° tr√¨nh x·ª≠ l√Ω")
                        break
                    
                    # X·ª≠ l√Ω th√¥ng ƒëi·ªáp
                    try:
                        msg_type, content = msg_queue.get(timeout=0.1)
                        handle_message(msg_type, content, ui_containers, show_logs)
                        msg_queue.task_done()
                        
                        if msg_type == 'complete':
                            time.sleep(0.5)  # ƒê·ª£i UI c·∫≠p nh·∫≠t tr∆∞·ªõc khi rerun
                            st.rerun()
                    except queue.Empty:
                        time.sleep(0.1)
                    
                    # Th√™m ƒë·ªô tr·ªÖ ƒë·ªÉ tr√°nh qu√° t·∫£i UI
                    time.sleep(0.05)
                
                # N·∫øu thread v·∫´n ch·∫°y nh∆∞ng v√≤ng l·∫∑p k·∫øt th√∫c
                if thread.is_alive():
                    st.warning("Ti·∫øn tr√¨nh x·ª≠ l√Ω v·∫´n ƒëang ch·∫°y n·ªÅn...")
                
            except Exception as e:
                error_details = traceback.format_exc()
                st.error(f"L·ªói UI: {str(e)}\n{error_details}")
                st.session_state.processing = False
        else:
            st.error("Vui l√≤ng ch·ªçn ƒë·∫ßy ƒë·ªß: MC, video n·ªÅn, v√† audio (ho·∫∑c nh·∫≠p vƒÉn b·∫£n)")

    # Th√™m footer v·ªõi th√¥ng tin aiclip.ai
    # st.markdown("---")
    # st.markdown(
    #     "<div style='text-align: center;'>"
    #     "Copyright ¬© 2025 "
    #     "<a href='https://aiclip.ai' target='_blank'>aiclip.ai</a>"
    #     "</div>",
    #     unsafe_allow_html=True
    # )

if __name__ == "__main__":
    main()
